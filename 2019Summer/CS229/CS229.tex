\documentclass{report}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1truein]{geometry}
\usepackage{tkz-graph}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{setspace}
\linespread{1.15}
\usepackage[margin=1truein]{geometry}

\newcommand{\upi}[0]{^{(i)}}
\newcommand{\bfX}[0]{\textbf{X}}

\counterwithin{equation}{section}
\counterwithin{figure}{section}

\pagestyle{fancy}
\lhead{CS229: Machine Learning}

\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}

\title{CS229: Machine Learning}
\date{\today}
\author{Tianyu Du}
\begin{document}
    \maketitle
    \tableofcontents
    \chapter{Preliminary}
    \section{Lecture Notes Jun. 24 2019}
    \subsection{Review of Linear Algebra}
    
    \begin{remark}
        In this course, vectors are treated as \emph{column matrices}.
    \end{remark}
    
    \begin{definition}
        Given $A \in M_{n \times n}(\R)$, the trace of $A$ is defined as 
        \begin{equation}
            \tx{tr}(A) := \sum_{i=1}^n A_{i,i}
        \end{equation}
    \end{definition}

    \begin{definition}
        Given $x, y \in \R^n$, the \textbf{inner product} is defined as 
        \begin{equation}
            \inner{x}{y} := x^T y = \sum_{i=1}^n x_i\ y_i
        \end{equation}
    \end{definition}

    \begin{definition}
        Given $x \in \R^b, y \in \R^p$, the \textbf{outer product} is defined as 
        \begin{equation}
            x \otimes y := x y^T = A \in M_{b \times p}(\R)
        \end{equation}
        in which
        \begin{equation}
            A_{i, j} := x_i\ y_j
        \end{equation}
        the constructed matrix $A$ is a \textbf{rank 1 matrix}.
    \end{definition}
    
    \begin{remark}
        Given two rank 1 matrices $A_1$ and $A_2$, then $A_1 + A_2$ is a rank 2 matrix.
    \end{remark}
    
    \begin{remark}
        Note that the outer product operation is not commutative.
    \end{remark}
    
    \begin{definition}
        Let $v, b \in \R^n$, the \textbf{projection matrix} of $v$ is defined as $\frac{v v^T}{v^T v} \equiv \frac{v \otimes v}{\inner{v}{v}}$. Then $\frac{v \otimes v}{\inner{v}{v}} b$ is the projection of $b$ on $v$.
        \begin{align}
            \frac{v \otimes v}{\inner{v}{v}} b &= \left[\frac{v}{\inner{v}{v}} \right] \left [\frac{v}{\inner{v}{v}} \right]^T b \\
            &= \tilde{v} \underbrace{\tilde{v}^T b}_{\tx{magnitude}}
        \end{align}
    \end{definition}
    
    \begin{proposition}
        Let $A \in M_{m \times n}(\R)$, the projection of vector $b \in \R^m$ onto the \emph{column space} of $A$ is given by the generalized projection matrix
        \begin{equation}
            A (A^T A)^{-1} A^T b
        \end{equation}
    \end{proposition}
    
    
    \section{Lecture Notes Jun. 28 2019}
        \begin{example}[Maximum Likelihood Estimation for Multivariate Gaussian Distribution]
            \begin{lemma}
                Let $x \in \R^n$ and $A \in \R^{n \times n}$, then
                \begin{equation}
                    \nabla_A x^T A x = x x^T
                \end{equation}
                \begin{proof}
                    \begin{align}
                        x^T A x &= 
                        \begin{pmatrix}
                            \sum_{i=1}^n x_i A_{i, 1} \\
                            \vdots \\
                            \sum_{i=1}^n x_i A_{i, n}
                        \end{pmatrix} x
                        = \sum_{j=1}^n \sum_{i=1}^n x_i A_{i, j} x_j \\
                        \implies \nabla_A x^T A x_{i, j} &= \pd{\sum_{j=1}^n \sum_{i=1}^n x_i A_{i, j} x_j}{A_{i, j}} = x_i x_j \\
                        \implies \nabla_A x^T A x &= x x^T
                    \end{align}
                \end{proof}
            \end{lemma}
            
            \begin{lemma}
                Let $x \in \R^n$, and $A \in \R^{n \times n}$, then
                \begin{equation}
                    \nabla_x x^T A x = 2 x^T A
                \end{equation}
            \end{lemma}
            
            \begin{lemma}
                Let $A \in \R^{n \times n}$ such that $A$ is non-singular, then
                \begin{equation}
                    \nabla_A \ln (|A|) = A^{-1}
                \end{equation}
            \end{lemma}
            
            \begin{proof}[Derive the MLE for Gaussian.]
                Let $\left( x^{(i)} \right)_{i=1}^n$ denote the set of training instances. Assuming they are independently and identically distributed ($i.i.d.$) following $\mc{N}(\mu, \Sigma)$, the joint likelihood can be written as
                \begin{equation}
                    \mc{L}(\mu, \Sigma; x^{(i)}) = \prod_{i \in [n]} \frac{1}{(2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}}}
                    \exp \left (
                        - \frac{1}{2} (x^{(i)} - \mu)^T \Sigma^{-1} (x^{(i)} - \mu)
                    \right )
                \end{equation}
                Then the MLE becomes the maximizer of the log-likelihood
                \begin{align}
                    (\hat{\mu}, \hat{\Sigma}) &:= \argmax_{\mu, \Sigma} \ell(\mu, \Sigma; x^{(i)}) \\
                    &= \argmax_{\mu, \Sigma} \sum_{i \in [n]} \left \{ 
                    \ln \left (
                        \frac{1}{(2\pi)^{\frac{n}{2}}} \right)
                    - \frac{1}{2} \ln (|\Sigma|)
                    - \frac{1}{2} (x^{(i)} - \mu)^T \Sigma^{-1} (x^{(i)} - \mu)
                    \right \}
                \end{align}
                Then the first order condition for $\hat{\mu}$ is 
                \begin{align}
                    &\nabla_\mu \ell(\mu, \Sigma, x^{(i)})|_{\mu = \hat{\mu}} = 0 \\
                    \implies& \sum_{i \in [n]} (x^{(i)} - \hat{\mu})^T \Sigma^{-1} = 0 \\
                    \implies& \Sigma^{-1} n \hat{\mu} = \Sigma^{-1} \sum_{i \in [n]} x^{(i)} \\
                    \implies& \hat{\mu} = \frac{1}{n} \sum_{i \in [n]} x^{(i)}
                \end{align}
                
                For $\hat{\Sigma}$, define $S := \Sigma^{-1}$, note that $\nabla_{S} \ell = 0 \iff \nabla_{\Sigma^{-1}} \ell = 0$
                \begin{align}
                    &\nabla_S = 0 \\
                    \implies& \nabla_S \sum_{i \in [n]} \left \{\frac{1}{2}\ln(|S|) - \frac{1}{2} (x^{(i)} - \mu)^T S (x^{(i)} - \mu) \right \} = 0\\
                    \implies& \sum_{i \in [n]} \left\{ S^{-1} - (x^{(i)} - \mu) (x^{(i)} - \mu)^T \right \} = 0 \\
                    \implies& S^{-1} = \sum_{i \in [n]} (x^{(i)} - \mu) (x^{(i)} - \mu)^T \\
                    \implies& \hat{\Sigma} = \frac{1}{n} \sum_{i \in [n]} (x^{(i)} - \mu) (x^{(i)} - \mu)^T \approx \expect{(x^{(i)} - \expect{x^{(i)}})^2} \equiv \var{x^{(i)}}
                \end{align}
            \end{proof}
        \end{example}
    
    \chapter{Supervised Learning}
    \section{Lecture Notes Jul. 1 2019}
        \paragraph{Supervised Learning Problem} Learn a mapping from input space to output space $f: X \to Y$. Given a data set $(x_i, y_i)_i$, in regression problem, the algorithm is trying to learn a \emph{hypothesis} $h(x) \approx f$.
        
        \paragraph{Terminologies}
        \begin{enumerate}[(i)]
            \item $n$: number of $(x, y)$ examples;
            \item $d$: $x \in \R^d$;
            \item $x^{(i)}$: input;
            \item $y^{(i)}$: label or output;
            \item $(x^{(i)}, y^{(i)})$: $i^{th}$ example.
        \end{enumerate}
        \subsection{Linear Regression}
        \paragraph{Model}
        \begin{align}
            h_\theta(x) &= \theta_0 + \sum_{i=1}^d \theta_i x_i,\ \theta \in \R^{d+1} \\
            &= \inner{\theta}{x},\ x := (1, x_1, x_2, \dots, x_d) \in \R^{d+1}
        \end{align}
        
        \paragraph{Cost/Loss Function}
        \begin{align}
            J(\theta) &:= \frac{1}{2} \sum_{i=1}^n \left(
                h_\theta(x^{(i)}) - y^{(i)}
            \right )^2 \\
            \hat{\theta} &:= \argmin_{\theta \in \R^{d+1}} J(\theta) \subset \R^{d+1} =: \Theta\\
            &= \argmin_{\theta \in \Theta} \frac{1}{2} \sum_{i=1}^n \left(h_\theta(x^{(i)}) - y^{(i)}\right)^2
        \end{align}
        
        \paragraph{(Full) Gradient Descent}
        \begin{align}
            \theta^{(t+1)} := \theta^{(t)} - \alpha \nabla_\theta J(\theta^{(t)})
        \end{align}
        where $\alpha$ denotes the \textbf{learning rate}. With appropriate learning rate
        \begin{align}
            J(\theta^{(t + 1)}) < J(\theta^{(t)})
        \end{align}
        And the updating step is repeated till it \ul{converges}. Theoretically, the \emph{convergence} means the convergence of series $(\theta^{(t)})_i$.\\
        \paragraph{Practical ways to check convergence}
        \begin{enumerate}[(i)]
            \item $\norm{\theta^{(t+1)} - \theta^{(t)}} < \varepsilon$;
            \item $\abs{J(\theta^{(t+1)}) - J(\theta^{(t)})} < \varepsilon$;
            \item $\norm{\nabla_\theta J(\theta^{(t)})} < \varepsilon$.
        \end{enumerate}
        
        \paragraph{Gradient Descent on Linear Regression}
        \begin{enumerate}
            \item Initialize $\theta^{(0)}$;
            \item Repeat till convergence:
            \begin{align}
                \theta^{(t+1)} &:= \theta^{(t)} - \alpha \nabla_\theta J(\theta^{(t)}) \\
                &= \theta^{(t)} - \alpha \nabla_\theta \frac{1}{2} \sum_{i=1}^n \left( \inner{\theta^{(t)}}{x^{(i)}} - y^{(i)} \right )^2 \\
                &= \theta^{(t)} - \alpha \sum_{i=1}^n \left( \inner{\theta^{(t)}}{x^{(i)}} - y^{(i)} \right ) x^{(i)}
            \end{align}
        \end{enumerate}
        
        \paragraph{Stochastic Gradient Descent (SGD)} Gradient descent based on \ul{one randomly selected} training sample $k$, and use a \ul{proxy loss function}:
        \begin{align}
            \tilde{J}(\theta) := \frac{1}{2} \left( \inner{\theta^{(t)}}{x^{(k)}} - y^{(k)} \right )^2
        \end{align}
        and update
        \begin{align}
            \theta^{(t+1)} := \theta^{(t)} - \alpha \left( \inner{\theta^{(t)}}{x^{(k)}} - y^{(k)} \right ) x^{(k)}
        \end{align}
        where the $\theta^{(t)}$ will finally in a region near the global minimum, the size of region is characterized by the learning rate $\alpha$.
        \begin{remark}
            SGD takes more iterations to converge than full gradient descent, but the computational cost of each step is much less than full GD.
        \end{remark}
        \paragraph{Analytical Solution to OLS}
        \begin{align}
            J(\theta) = \frac{1}{2} \sum_{i=1}^n \left(\theta^T x^{(i)} - y^{(i)}\right)
        \end{align}
        A \textbf{design matrix}, $\textbf{X}$, is a $n \times d$ matrix, in which each row is an input vector. $y \in \R^{n \times 1}$ denotes the collection of labels. Then the loss function can be expressed as
        \begin{align}
            J(\theta) = \frac{1}{2} \norm{\textbf{X}\theta - y}^2 = \frac{1}{2} [\textbf{X}\theta - y]^T [\textbf{X}\theta - y]
        \end{align}
        The OLS estimator can be obtained by solving
        \begin{align}
            \nabla_\theta J(\theta) &= 0 \\
            \iff \nabla_\theta \frac{1}{2} [\textbf{X}\theta - y]^T [\textbf{X}\theta - y] &= 0 \\
            % \iff \nabla_\theta \frac{1}{2} \left(\textbf{X}\theta - y^T \textbf{X}\theta - (\textbf{X}\theta)^T y + y^T y\right) &= 0 \\
            \iff \frac{1}{2} 2 [\textbf{X}\theta - y] \textbf{X} &= 0 \\
            \iff [\textbf{X}\theta - y] \textbf{X}^T &= 0  \\
            \iff \textbf{X} \textbf{X}^T \theta - \textbf{X}^T y &= 0 \quad (\tx{normal equation}) \\
            \implies \hat{\theta} &= (\textbf{X}^T \textbf{X})^{-1} \textbf{X} y
        \end{align}
        \paragraph{Probabilistic Interpretation} Assuming the underlying data generating process follows
        \begin{align}
            y^{(i)} &= \theta^T x^{(i)} + \varepsilon^{(i)} \\
            \varepsilon^{(i)} &\sim \mc{N}(0, \sigma^2) \\
            \implies y^{(i)}|x^{(i)} &\sim \mc{N}(\theta^T x^{(i)}, \sigma^2) \\
            \implies P(y^{(i)}|x^{(i)}; \theta) &= \phi(y^{(i)}; x^{(i)}, \theta) \\
            &= \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(
                -\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2 \sigma^2}
            \right)
        \end{align}
        \paragraph{Maximum Likelihood Estimation} Assuming $\varepsilon\upi \sim i.i.d $, then the log-likelihood can be expressed as 
        \begin{align}
            \ell(\theta; x\upi, y\upi) &= \prod_i\\
            &= \sum_i \\
            &= \sum_i \ln \left[ \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(
                -\frac{(y^{(i)} - \theta^T x^{(i)})^2}{2 \sigma^2}
            \right) \right] \\
            &= \sum_i K - \frac{1}{2 \sigma^2}(y^{(i)} - \theta^T x^{(i)})^2 \\
            &= \tilde{K} - \frac{1}{\sigma^2} \red{
                \frac{1}{2} \sum_{i=1}^n (y^{(i)} - \theta^T x^{(i)})^2
            }
        \end{align}
        
        \paragraph{Conclusion} The ordinary least square estimator coincides with the maximum likelihood estimator. If we assume the noise in the model follows Gaussian distribution, then maximizing the likelihood and minimizing loss function are equivalent:
        \begin{align}
            \argmax_{\theta \in \Theta} \ell(\theta; x\upi, y\upi) = \argmin_{\theta \in \Theta} J(\theta)
        \end{align}
        
        \paragraph{Another Interpretation of Linear Regression} Suppose $d \ll n$. Assume $\textbf{X}$ is full-ranked, that is, rank(\textbf{X})=$d$. The projection matrix is $\bfX (\bfX^T \bfX)\bfX^{-1}$. So linear regression is projecting ...
\end{document}