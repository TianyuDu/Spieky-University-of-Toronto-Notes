\documentclass[11pt]{article}

\title{ECO475H1 S\\ Applied Econometrics II}

\author{Tianyu Du}
\date{\today}

\usepackage{spikey}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1truein]{geometry}

\counterwithin{equation}{section}
\counterwithin{figure}{section}

\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\bm}[1]{\boldsymbol{#1}}

\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}

\begin{document}
	\maketitle
	\doclicenseThis
	\texttt{Github Page} \url{https://github.com/TianyuDu/Spikey_UofT_Notes}\\
	\texttt{Note Page} \url{TianyuDu.com/notes}
	
	\tableofcontents
	\newpage
	
	\section{Topic 1 Binary Outcome Model}
		\subsection{Lecture 1. Jan. 10 2019}
			\subsubsection{Model}
				\paragraph{Interpretation} Binary outcome models can be interpreted as rational individuals are making binary decisions by comparing the utilities resulting from each decision.
				\begin{equation}
					\begin{cases}
						u_1(\ve{x}, \varepsilon_1) &\tx{ utility from decision 1} \\
						u_0(\ve{x}, \varepsilon_0) &\tx{ utility from decision 0}
					\end{cases}
				\end{equation}
				and rationality of individuals suggests
				\begin{equation}
					y = \begin{cases}
						1 &\tx{ if } u_1(\ve{x}_i, \varepsilon_1) \red{\geq} u_0(\ve{x}_i, \varepsilon_0) \\
						0 &\tx{ otherwise}
					\end{cases}
				\end{equation}
				\begin{assumption}
					By convention, the edge case that $u_0 = u_1$ is modelled and classified to be in the first case, but $\prob{u_1 - u_0 = 0|\ve{x}} =0$ for continuous utilities, so the edge does not really matter.
				\end{assumption}
				
				\begin{assumption}
					The difference in utilities can be captured by a linear function, and we define the \textbf{latent variable} $y^*$ as
					\begin{align}
						y^*(\ve{x}) &\equiv u_1(\ve{x}, \varepsilon_1) - u_0(\ve{x}, \varepsilon_0) \\
						&= \ve{x}' \beta + \varepsilon
					\end{align}
				\end{assumption}
				
				\begin{remark}[About shapes of variables]
					\begin{gather}
						\ve{x},\ \beta \in \mathbb{M}_{k \times 1};\ y,\ d \in \R
					\end{gather}
				\end{remark}
				\paragraph{Reduced Model} Then the primary model (1.2) is reduced to
				\begin{equation}
					y = \id{y^*(\ve{x}) \geq 0}
				\end{equation}
			
			\subsubsection{Data Structure}
				\par For each individual, the observation contains two \emph{primary variables}, $(\ve{x}_i, y_i)$. So the dataset would look like
					\begin{gather}
						\{(\ve{x}_i, y_i)\}_{i=1}^N
					\end{gather}
			
			\subsubsection{Linear Probability Model}
				\paragraph{Problems} \quad
				\begin{enumerate}
					\item Out-of-range prediction;
					\item Homogeneous marginal effect;
					\item (Potentially) heteroskedasticity.	
				\end{enumerate}
			\subsubsection{Model with Binomial Distribution}
				\paragraph{Procedures} \quad
					\begin{enumerate}
						\item Construct distribution and density functions.
						\item Construct likelihood from density function, assuming iid of observations.
						\item (MLE) estimates model parameters.
						\item Predictions, both individual and average effect.
					\end{enumerate}
				\paragraph{Conditional Density}
					\begin{gather}
						f_Y(y|\ve{x}) = \begin{cases}
							P(\ve{x}) &\tx{ if } y = 1 \\
							1 - P(\ve{x}) &\tx{ if } y = 0
						\end{cases} \\
						= P(\ve{x})^{d} [1 - P(\ve{x})]^{1-d} 
					\end{gather}
					
				\begin{assumption}[Distributions of $\varepsilon$]
					Generally, there are two assumptions regarding to the distribution of $\varepsilon$,
					\begin{enumerate}[(i)]
						\item \textbf{Standard Normal} $\varepsilon \sim \mc{N}(0, 1)$\footnote{\hl{In the normal distribution case, we cannot estimate the variance of $\hat{\beta}_{MLE}$ laster if we do not assume the variance to be 1.}}.
						\item \textbf{Gumbel Distribution} $F_\varepsilon(x) = \frac{e^x}{1 + e^x}$.
					\end{enumerate}
					Note that, in either case, $\varepsilon$ is symmetrically distributed, which means $\varepsilon$ and $-\varepsilon$ have the identical density and distribution.
				\end{assumption}
				
					\begin{gather}
						P(\ve{x}) \equiv \prob{y^* \geq 0|\ve{x}} = \prob{-\varepsilon \leq \ve{x}' \beta | \ve{x}} \equiv F_{\varepsilon}(\ve{x}'\beta) \\
						\implies f_Y(y|\ve{x}) = (F_{\varepsilon}(\ve{x}'\beta))^y (1 - F_{\varepsilon}(\ve{x}'\beta))^{1-y}
					\end{gather}
					
				\paragraph{Likelihood} assuming observations are iid, the joint density, conditioned on model parameter $\beta$, of the \ul{collection of observations} $\{(y_i)\}_{i=1}^N$ is
					\begin{gather}
						f_{\{(Y_i)\}_{i=1}^N}(\{(y_i)\}_{i=1}^N|\beta, \{(\ve{x}_i)\}_{i=1}^N) = \prod_{i=1}^N f_Y(y_i|\ve{x}_i, \beta) \\
						= \prod_{i=1}^N F_\varepsilon(\ve{x}_i' \beta)^{y_i} (1 - F_\varepsilon(\ve{x}_i' \beta))^{1 - y_i}
					\end{gather}
					and the likelihood of collection of observations $\{(\ve{x}_i, y_i, d_i)\}_{i=1}^N$ is given parameter $\beta$ is 
					\begin{gather}
						\mc{L}(\beta|\{(\ve{x}_i, y_i, d_i)\}_{i=1}^N) = \prod_{i=1}^N F_\varepsilon(\ve{x}_i' \beta)^{y_i} (1 - F_\varepsilon(\ve{x}_i' \beta))^{1 - y_i} \\
						\implies \ln \mc{L} = \sum_{i=1}^N y_i \ln F_\varepsilon(\ve{x}_i' \beta) + (1 - y_i) \ln (1 - F_\varepsilon(\ve{x}_i' \beta))
					\end{gather}
					
				\paragraph{Maximum Likelihood Estimation} First order conditions
					\begin{gather}
						\pd{\ln \mc{L}}{\beta} = \sum_{i=1}^N y_i \frac{F'_\varepsilon(\ve{x}_i' \beta)}{F_\varepsilon(\ve{x}_i' \beta)} \ve{x}'_i 
						- (1 - y_i) \frac{F'_\varepsilon(\ve{x}_i' \beta)}{1 - F_\varepsilon(\ve{x}_i' \beta)}\ve{x}'_i \\
						= \sum_{i=1}^N \frac{y_i F'_\varepsilon(\ve{x}_i' \beta) - F'_\varepsilon(\ve{x}_i' \beta)F_\varepsilon(\ve{x}_i' \beta)}{F_\varepsilon(\ve{x}_i' \beta)(1-F_\varepsilon(\ve{x}_i' \beta))} \ve{x}'_i = 0
					\end{gather}
					
				\paragraph{Prediction - Marginal Effect} With binary outcome model, according to mean of Binomial distribution, 
					\begin{gather}
						\expect{y|\ve{x}} = P(\ve{x}) \equiv F_\varepsilon(\ve{x}' \hat{\beta}_{MLE})
					\end{gather}
					Then the marginal effect is
					\begin{gather}
						\pd{\expect{y|\ve{x}}}{x_k} = F'_\varepsilon(\ve{x}' \hat{\beta}_{MLE}) \hat{\beta}_{j, MLE}
					\end{gather}
					In contrast to the linear probability model, in which the marginal effect is assumed to be homogeneous. In BOM prediction, the $F'_\varepsilon(\ve{x}' \hat{\beta}_{MLE})$ term in the marginal effect acts as a source of \textbf{heterogeneity} marginal effect.
				
				\begin{remark}
					Given $f_\varepsilon \geq 0$, $\hat{\beta}_{MLE}$ reports the \textbf{sign} of marginal effect, but it \hl{provides no quantitative implication}.
				\end{remark}
				
				\paragraph{Prediction - Average Marginal Effect}
					There are two methods to estimate the average marginal effect, these two methods generate different estimations unless the density function of $\varepsilon$ is linear.
					\begin{enumerate}[(i)]
						\item $\overline{ME}_j = \frac{1}{N} \sum_{i=1}^N F'_\varepsilon(\ve{x}_i \hat{\beta}) \hat{\beta}_j$
						\item $\overline{ME}_j = F'_\varepsilon(\overline{\ve{x}}_i \hat{\beta}) \hat{\beta}_j$
					\end{enumerate}
			
	\section{Lecture 3. Jan. 24 2019}
		\subsection{Two Side Censoring MLE}
		\par Consider the latent dependent variable
		\begin{equation}
			Y^* = \textbf{x}' \bm{\beta} + \epsilon
		\end{equation}
		where $\epsilon_i \sim \mc{N}(0, \red{\sigma^2})$\footnote{In general, we can assume the error variance to be $\sigma^2$ when the dependent variable is \emph{quantitive}, but with \emph{qualitative} dependent variables, we assume $\varepsilon \sim \mc{N}(0, 1)$ since we don't have sufficient information to estimate the error variance.}. \\
		Therefore, given fixed $\textbf{x}$,
		\begin{equation}
			Y^* \sim \mc{N}(\textbf{x}' \bm{\beta}, \sigma^2)
		\end{equation}
		Define parameter set 
		\begin{equation}
			\bm{\theta} \equiv (\bm{\beta}, \sigma)
		\end{equation}
		The observable variable is 
		\begin{gather}
			Y = \begin{cases}
				U &\tx{if } Y^* \geq U \\
				Y^* &\tx{if } Y^* \in (L, U) \\
				L &\tx{if } Y^* \leq L
			\end{cases}
		\end{gather}
		Let $f_Y(y|\textbf{x}, \bm{\beta}): [L, U] \to [0, 1]$ be the probability measure of $Y$. \\
		Let $y \in [L, U]$, 
		\begin{gather}
			f_Y(y|\textbf{x}, \bm{\beta}) = \begin{cases}
				\mathbb{P}(Y^* \geq U|\textbf{x}, \bm{\beta}) &\tx{if } y \geq U \\
				f_{Y^*}(y|\textbf{x}, \bm{\beta}) &\tx{if } y \in (L, U) \\
				\mathbb{P}(Y^* \leq L|\textbf{x}, \bm{\beta}) &\tx{if } y \leq L
			\end{cases} \\
			= \begin{cases}
				1 - F_{Y^*}(U|\textbf{x}, \bm{\beta}) &\tx{if } y \geq U \\
				f_{Y^*}(y|\textbf{x}, \bm{\beta}) &\tx{if } y \in (L, U) \\
				F_{Y^*}(L|\textbf{x}, \bm{\beta}) &\tx{if } y \leq L
			\end{cases}
		\end{gather}
		Define indicator $(d_1(y), d_2(y), d_3(y))$ as 
		\begin{gather}
			d_1(y) \equiv \mc{I}(y \geq U) \\
			d_2(y) \equiv \mc{I}(y \in (L, U)) \\
			d_3(y) \equiv \mc{I}(y \leq L)
		\end{gather}
		Then the probability measure of $Y$ can be expressed as
		\begin{gather}
			f_Y(y|\textbf{x}, \bm{\beta}) 
			= (1 - F_{Y^*}(U|\textbf{x}, \bm{\beta}))^{d_1} 
			\times f_{Y^*}(y|\textbf{x}, \bm{\beta}) ^ {d_2} 
			\times F_{Y^*}(L|\textbf{x}, \bm{\beta})^{d_3}
		\end{gather}
		Suppose samples are i.i.d., the joint density is
		\begin{gather}
			f_{Y_1, \dots, Y_N}(y_1, \dots, y_N|\textbf{X}, \bm{\beta}) = \prod_{i=1}^N f_Y (y_i|\textbf{x}_i, \bm{\beta})
		\end{gather}
		The log-likelihood is
		\begin{gather}
			\mc{L}_N (\bm{\theta}|\textbf{X}) = \sum_{i=1}^N \Big\{
			d_{1,i} \times \ln(1 - F_{Y^*}(U|\textbf{x}_i, \bm{\beta})) + d_{2,i} \times \ln(f_{Y^*}(y|\textbf{x}_i, \bm{\beta})) + d_{3,i} \times \ln(F_{Y^*}(L|\textbf{x}_i, \bm{\beta}))
			\Big\}
		\end{gather}
		Finally, solving
		\begin{equation}
			\hat{\bm{\theta}}_{MLE}
			= (\hat{\bm{\beta}}_{MLE}, \hat{\sigma}_{MLE}) = \argmax_{\bm{\theta} \in \Theta} \mc{L}_N (\bm{\theta})
		\end{equation}
		
		\subsection{Two Side Truncated MLE}
		\par Suppose the observations are truncated with lower and upper bounds $L$ and $U$. \\
		Let the latent dependent variable be 
		\begin{equation}
			Y^* = \textbf{x}' \bm{\beta} + \epsilon
		\end{equation}
		and 
		\begin{equation}
			\epsilon \sim \mc{N}(0, \sigma^2)
		\end{equation}
		which implies, for given $\textbf{x}$,
		\begin{equation}
			Y^* \sim \mc{N}(\textbf{x}' \bm{\beta}, \sigma^2)
		\end{equation}
		Define parameter set 
		\begin{equation}
			\bm{\theta} \equiv \{\bm{\beta}, \sigma\}	
		\end{equation}
		Observable random variable $Y$ is 
		\begin{equation}
			Y = \begin{cases}
				Y^* &\tx{if } Y^* \in (L, U) \\
				-- &\tx{if } Y^* \notin (L, U)
			\end{cases}
		\end{equation}
		Constructing the distribution for $Y$, note that $F_Y$ is only defined on $y \in (L, U)$,
		\begin{gather}
			F_Y(y|\textbf{x}, \bm{\theta}) = \mathbb{P}(Y<y|\textbf{x}, \bm{\theta})\\
			= \frac{\mathbb{P}(Y^* < y \land Y^* \in (L, U) | \textbf{x}, \bm{\theta})}{\mathbb{P}(Y^* \in (L, U)|\textbf{x}, \bm{\theta})} \\
			= \frac{\mathbb{P}(Y^* \in (L, y)|\textbf{x}, \bm{\theta})}{\mathbb{P}(Y^* \in (L,U)|\textbf{x}, \bm{\theta})} \\
			= \frac{F_{Y^*}(y|\textbf{x}, \bm{\theta}) - F_{Y^*}(L|\textbf{x}, \bm{\theta})}{F_{Y^*}(U|\textbf{x}, \bm{\theta}) - F_{Y^*}(L|\textbf{x}, \bm{\theta})}
		\end{gather}
		Then construct the density of $Y$ 
		\begin{gather}
			f_Y(y|\textbf{x}, \bm{\theta}) = \pd{F_Y(y|\textbf{x}, \bm{\theta})}{y} \\
			= \frac{f_{Y^*}(y|\textbf{x}, \bm{\theta})}{F_{Y^*}(U|\textbf{x}, \bm{\theta}) - F_{Y^*}(L|\textbf{x}, \bm{\theta})}
		\end{gather}
		The sample log-likelihood is
		\begin{gather}
			\mc{L}_N(\bm{\theta}) = \sum_{i=1}^N \ln(f_{Y^*}(y_i|\textbf{x}_i, \theta)) - \ln(F_{Y^*}(U|\textbf{x}_i, \bm{\theta}) - F_{Y^*}(L|\textbf{x}_i, \bm{\theta}))
		\end{gather}
		and the estimator is given by
		\begin{gather}
			\hat{\bm{\theta}}_{MLE} = \{
			\hat{\bm{\beta}}_{MLE}, \hat{\sigma}_{MLE}
			\} = \argmax_{\bm{\theta} \in \Theta} \mc{L}_N(\bm{\theta})
		\end{gather}
		
	\section{Lecture 4. Jan. 31 2019}
		\subsection{Tobit and Sample Selection}
			\paragraph{Model} the \emph{observable} variables in Tobit model with sample selection are determined by both \orange{\textbf{outcome equation}} and \blue{\textbf{selection equation}}.
			\begin{gather}
				y_i = \begin{cases}
					\orange{\ve{x}_i' \beta + \epsilon_i} &\tx{ if } \blue{\ve{w}_i' \gamma + v_i} > 0 \\
					$\xmark$ &\tx{ otherwise}
				\end{cases}
			\end{gather}
			where \textbf{unmeasurable errors} are assumed to follow joint normal distribution,
			\begin{gather}
				\begin{pmatrix}
					\epsilon_i \\ v_i	
				\end{pmatrix}
				\sim \mc{N}(\ve{0},
				\begin{pmatrix}
					\sigma_{\epsilon}^2 & \rho \sigma^2 \\
					\rho \sigma^2 & 1
				\end{pmatrix})
			\end{gather}
			
			\begin{lemma}
				If $(\epsilon, v)$ follows joint normal distribution, then there exists $e \perp v$ and $e \sim \mc{N}(0,1)$ such that
				\begin{gather}
					\frac{\epsilon}{\sigma_{\epsilon}} = \rho v + e
				\end{gather}
			\end{lemma}
			\paragraph{Expectation} Define $\tilde{\ve{x}}_i \equiv [\ve{x}_i, \ve{w}_i]$, then the expected \emph{observed} dependent variable is \footnote{For each variable, the $i$ subscript is omitted in the derivation}
			\begin{gather}
				\expect{y | \ve{w}_i' \gamma + v_i > 0, \tilde{\ve{x}}} \\
				= \expect{\ve{x}'\beta + \epsilon | \ve{w}_i' \gamma + v_i > 0, \tilde{\ve{x}}} \\
				= \ve{x}'\beta + \expect{\epsilon | \ve{w}_i' \gamma + v_i > 0, \tilde{\ve{x}}} \\
				= \ve{x}'\beta + \expect{\rho v \sigma_\epsilon + e \sigma_\epsilon | \ve{w}_i' \gamma + v_i > 0, \tilde{\ve{x}}} \\
				= \ve{x}'\beta + \rho \sigma_\epsilon \expect{v | \ve{w}_i' \gamma + v_i > 0, \tilde{\ve{x}}} + \sigma_\epsilon \expect{e | \ve{w}_i' \gamma + v_i > 0, \tilde{\ve{x}}} \\
				= \ve{x}'\beta + \rho \sigma_\epsilon \expect{v | \ve{w}_i' \gamma + v_i > 0, \tilde{\ve{x}}}
			\end{gather}
			\begin{remark}
				If $\rho = 0$ in equation (2.9), there is no sample selection problem and we can use OLS to estimate the outcome equation.
			\end{remark}
			\begin{lemma}
				If $X \sim \mc{N}(\mu, \sigma^2)$ then
				\begin{gather}
					\expect{X|X>\alpha} = \mu + \sigma \frac{\phi(\frac{x - \mu}{\sigma})}{1 - \Phi(\frac{x-\mu}{\sigma})}
				\end{gather}
			\end{lemma}
			(continue)
			\begin{gather}
				\dots = \ve{x}'\beta + \rho \sigma_\epsilon \expect{v | v > - \ve{w}' \gamma, \tilde{\ve{x}}} \\
				= \ve{x}'\beta + \rho \sigma_\epsilon \frac{\phi(-\ve{w}'\gamma)}{1 - \Phi(-\ve{w}'\gamma)} \\
				= \ve{x}' \beta + \rho \sigma_\epsilon \frac{\phi(\ve{w}'\gamma)}{\Phi(\ve{w}'\gamma)} \\
				= \ve{x}' \beta + \rho \sigma_\epsilon \lambda(\ve{w}'\gamma)
			\end{gather}
			where $\lambda(x)$ is the \textbf{inverse Mill's ratio} of standard normal at $x$.
			\paragraph{Marginal Effect} Consider the case 
			\begin{equation}
				\exists x_k \in \ve{x} \cap \ve{w}
			\end{equation}
			for instance, $x_k$ can be \emph{wage taxation}. The marginal effect of $x_k$ is
			\begin{gather}
				\pd{\expect{y|\ve{w}'\gamma + v > 0, \tilde{\ve{x}}}}{x_k} = \pd{\ve{x}' \beta + \rho \sigma_\epsilon \lambda(\ve{w}'\gamma)}{x_k} \\
				= \beta_k + \rho \sigma_\epsilon \lambda'(\ve{w}'\gamma) \gamma_k \\
			\end{gather}
			where $\beta_k$ measures the \textbf{direct effect} and $\lambda'(\ve{w}'\gamma) \gamma_k$ measures the \textbf{indirect effect} of $x_k$.
		\subsection{Heckman Estimation (Two-Step Procedure)}
			\paragraph{Step 1} Run a \emph{probit} estimation on the selection equation.\\
			MLE gives
			\begin{enumerate}[(i)]
				\item An estimation $\hat{\gamma}_{MLE}$ captures the \emph{indirect effect} of regressors in $\ve{w}$ on $y$ through the selection equation.
			\end{enumerate}
			And compute
			\begin{gather}
				\hat{\lambda}(\ve{w}'\hat{\gamma}_{MLE}) \equiv \frac{\phi(\ve{w}'\hat{\gamma}_{MLE})}{\Phi(\ve{w}'\hat{\gamma}_{MLE})}
			\end{gather}
			\paragraph{Step 2} Run OLS
			\begin{gather}
				y = \ve{x}' \beta + \rho \sigma_\epsilon \hat{\lambda} + \eta \tx{ where } \expect{\eta | \ve{x}, \hat{\lambda}} = 0
			\end{gather}
			OLS gives
			\begin{enumerate}[(i)]
				\item An estimation $\hat{\beta}_{OLS}$ measures the \emph{direct effect} of regressors in $\ve{x}$ on $y$ through the outcome equation.
				\item An estimation of $\widehat{\rho \sigma_\epsilon}$, given $\sigma_\epsilon > 0$, we can estimate the \emph{sign} of $\rho$.
			\end{enumerate}
			
			\paragraph{Special Case (i)} Consider the special case where
			\begin{gather}
				\ve{w} = \ve{x} \\
				\lambda(x) \tx{ is linear}
			\end{gather}
			then (2.14) and regression (2.20) can be written as 
			\begin{gather}
				y = \ve{x}' \beta + \rho \sigma_\epsilon \ve{x}' \lambda(\gamma) + \eta \\
				= \ve{x}' [\beta + \rho \sigma_\epsilon \lambda(\gamma)] + \eta
			\end{gather}
			where $\beta + \rho \sigma_\epsilon \lambda(\gamma)$ represents the \emph{mixed and non-separable} effect. \\
			\paragraph{Special Case (ii)} If
			\begin{gather}
				\ve{w} = [\ve{x}, z] \\
				\lambda(x) \tx{ is linear} \\
			\end{gather}
			Let the coefficients of $\ve{w}$ be $[\gamma, \theta]$, then
			\begin{gather}
				\lambda(\ve{w}[\gamma, \theta]) = \lambda(\ve{x} \gamma) + \lambda(z \theta) \\
				= \ve{x} \lambda(\gamma) + z \lambda(\theta)
			\end{gather}
			Then the regression can be rewritten as
			\begin{gather}
				y = \ve{x}' [\beta + \rho \sigma_\epsilon \lambda(\gamma)] + \rho \sigma_\epsilon z \lambda(\theta) + \eta
			\end{gather}
			\begin{remark}
				Therefore, if $\lambda$ is linear, we need at least one exclusion variable to identify the direct and indirect effects. If $\lambda$ is non-linear, it's \emph{probably} fine.
			\end{remark}
			
	\section{Binary Outcome with Continuous Endogenous Regressors:\\Control Function Approach}
		\subsection{Model}
			\paragraph{}In ordinary binary outcome models, like Probit models, we assumed all regressors are \emph{exogenous} ($Cov(x, \varepsilon) = 0$). But in many cases, we have some of the explanatory variables are endogenous. In this section, we are going to consider the case where the endogenous regressors are \hl{continuous}.
			\textbf{Outcome Equation}
			\begin{gather}
				y = \mathbb{I}\{\ve{x}_y \theta + \ve{w} \gamma + \varepsilon > 0\}
			\end{gather}
			where 
			\begin{enumerate}[(i)]
				\item $\ve{x}_y$: exogenous observable characteristics.
				\item $\ve{w}$: endogenous observable regressors, which are continuous.
			\end{enumerate}
			Similarly to the IV approach, we use another "auxiliary equation" to estimate $\ve{w}$:
			\begin{equation}
				\ve{w} = \ve{x}_w \eta + \sigma_w v
			\end{equation}
			where the error terms in (3.1) and (3.2) follows
			\begin{equation}
				\begin{pmatrix} \varepsilon \\ v \end{pmatrix}
				\sim \mc{N}
				\Big(
				\ve{0},\ 
				\begin{bmatrix}1 & \rho \\ 
				\rho & 1\end{bmatrix}
				\Big)
			\end{equation}
			Define $\tilde{\ve{x}} \equiv [\ve{x}_y, \ve{x}_w]$ as the set of usable regressors.
		\subsection{Maximum Likelihood Estimator}
			\paragraph{}To estimate the model using MLE, we need to construct the likelihood function. By Bayesian Theorem,  
			\begin{gather}
				f(y|\ve{w}, \tilde{\ve{x}}) = \frac{f(y, \ve{w}|\tilde{\ve{x}})}{f(\ve{w}|\tilde{\ve{x}})} \\
				\iff f(y, \ve{w} | \tilde{\ve{x}}) = f(y|\ve{w}, \tilde{\ve{x}}) f(\ve{w}|\tilde{\ve{x}})
			\end{gather}
			By equation (3.2)
			\begin{gather}
				w|_{\tilde{\ve{x}}} \sim \mc{N} \Big(\ve{x}_w \eta, \sigma_w^2 \Big) \\
				\implies f(w|\tilde{\ve{x}}) = \frac{1}{\sqrt{2\pi}\sigma_w}e^{\frac{-(w - \ve{x}_w \eta)^2}{2\sigma_w^2}}
			\end{gather}
			and to compute $f(y|w, \ve{x}_w \eta)$, since $y$ is binary, we are going to compute $\prob{y=1|w, \tilde{\ve{x}}}$ first.
			\begin{gather}
				\prob{y=1|w, \tilde{\ve{x}}} = \prob{-\varepsilon < \ve{x}_y \theta + w \gamma | w, \tilde{\ve{x}}} \\
				= \prob{-\varepsilon < \ve{x}_y \theta + w \gamma | \red{v}, \tilde{\ve{x}}}
			\end{gather}
			\begin{lemma}
				Given joint normal variables $(\varepsilon, v)$ conditioned on $\tilde{\ve{x}}$ following
				\begin{equation}
					\begin{pmatrix} \varepsilon \\ v \end{pmatrix}\vert_{\tilde{\ve{x}}}
					\sim \mc{N}
					\Big(
					\ve{0},\ 
					\begin{bmatrix}1 & \rho \\ 
					\rho & 1\end{bmatrix}
					\Big)
				\end{equation}
				then 
				\begin{equation}
					\varepsilon|_{v, \tilde{\ve{x}}} \sim \mc{N}(\rho v,\ 1 - \rho^2)
				\end{equation}
				which implies
				\begin{gather}
					\frac{\varepsilon - \rho v}{\sqrt{1 - \rho^2}} \sim \mc{N}(0,\ 1) \\
					\implies \frac{- \varepsilon + \rho v}{\sqrt{1 - \rho^2}} \sim \mc{N}(0,\ 1)
				\end{gather}
				by the symmetry of standard normal distribution.
			\end{lemma}
			\par Therefore,
			\begin{gather}
				\prob{-\varepsilon < \ve{x}_y \theta + w \gamma | v, \tilde{\ve{x}}} \\
				= \prob{-\varepsilon + \red{\rho v} < \ve{x}_y \theta + w \gamma + \red{\rho v}|v, \tilde{\ve{x}}} \\
				= \prob{\frac{-\varepsilon + \rho v}{\red{\sqrt{1 - \rho^2}}} < \frac{\ve{x}_y \theta + w \gamma + \rho v}{\red{\sqrt{1 - \rho^2}}} |v, \tilde{\ve{x}}} \\
				= \Phi(\frac{\ve{x}_y \theta + w \gamma + \rho v}{\sqrt{1 - \rho^2}})
			\end{gather}
			
		\subsection{Control Function}
			\paragraph{Step 1} Run OLS on $w = \ve{x}_w \eta + \sigma_w v$, Obtain estimations $\hat{\eta}_{OLS}$, $\hat{\sigma_w}_{OLS}$.
			\paragraph{Step 2} Obtain estimation of $v$ using the error terms and standard deviation in OLS results.
				\begin{equation}
					\hat{v} = \frac{w - \ve{x}_w \hat{\eta}_{OLS}}{\hat{\sigma}_{OLS}}
				\end{equation}
			\paragraph{Step 3} Plug in $\hat{v}$ and run \textbf{probit} model in (3.17),
				\begin{gather}
					\Phi(\frac{\ve{x}_y \theta + w \gamma + \rho v}{\sqrt{1 - \rho^2}}) \\
					= \Phi(\frac{\ve{x}_y \theta}{\sqrt{1 - \rho^2}} + \frac{w \gamma}{\sqrt{1 - \rho^2}} + \frac{\rho v}{\sqrt{1 - \rho^2}})
				\end{gather}
				Define
				\begin{gather}
					\theta^* \equiv \frac{\theta}{\sqrt{1 - \rho^2}} \\
					\gamma^* \equiv \frac{\gamma}{\sqrt{1 - \rho^2}} \\
					\alpha^* \equiv \frac{\alpha}{\sqrt{1 - \rho^2}}
				\end{gather}
				So the probit model can be written as
				\begin{equation}
					y = \mathbb{I}\{-\tilde{u} < \ve{x}_y \theta^* + w \gamma^* + v \alpha^*\}
				\end{equation}
				where $\tilde{u} \sim \mc{N}(0,1)$.\\
				\par Once we have an estimation on $\alpha^*$, $\rho$ can be calculated with
				\begin{equation}
					\rho = \pm \sqrt{\frac{\alpha^*}{1 + \alpha^{*2}}}
				\end{equation}
\end{document}













