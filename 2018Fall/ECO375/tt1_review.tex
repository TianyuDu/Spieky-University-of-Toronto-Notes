\documentclass[]{article}
\title{ECO375: Review Notes \\ \small Applied Econometrics I}
\author{Tianyu Du}
\date{\today}

\usepackage{spikey}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{chngcntr}

\begin{document}
	\maketitle
	\tableofcontents
	\section{Slide 4: Simple \& Multiple Regression - Estimation}
	\subsection{Regression Model}
	\begin{assumption}
	    Assuming the population follows
	    \[
	        y = \beta_0 + \beta_1 x + u
	    \]
	    and assume that \emph{$x$ causes y}.
	\end{assumption}
	
	\subsection{OLS}
	\begin{gather*}
	    \min_{\vec{\beta}} \sum_{i} (y_i - \hat{y}_i)^2 \\
	    \tx{With FOC:} \\
	    \sum_{i}(y_i - \hat{y}_i) = 0 \\
	    \sum_{i}x_{ij}(y_i - \hat{y}_i) = 0,\ \forall j
	\end{gather*}
	
	\begin{remark}
	    Both $\hat{\beta}_0$ and $\hat{\beta}_j$ are functions of \emph{random variables} and therefore themselves \emph{random} with \emph{sampling distribution}. And the estimated coefficients are random up to random sample chosen.
	\end{remark}
	
	\begin{property}
	    Properties of OLS estimators
	    \begin{itemize}
	        \item \textbf{Unbiased} $\mathbb{E}[\hat{\beta} | X] = \beta$
	        \item \textbf{Consistent} $\hat{\beta} \to \beta$ as $n \to \infty$
	        \item \textbf{Efficient/Good} min variance.
	    \end{itemize}
	\end{property}
	
	\begin{definition}
	    The \textbf{Simple Coefficient of Determination}
	    \[
	        R^2 = \frac{SSE}{SST}
	    \]
	    and $SS\underline{Total} = SS\underline{Explained} + SS\underline{Residual}$
	    \[
	        \sum_i {(y_i - \overline{y})^2} = \sum_i {(\hat{y}_i - \overline{y})^2} + \sum_i {(y_i - \hat{y}_i)^2}
	    \]
	\end{definition}
	
	\begin{proposition}[Logarithms] Interpretation with logarithmic transformation.
	    \begin{itemize}
	        \item $\ln{y} = \alpha + \beta \ln{y} + u$: \ul{$x$ increases by $1\%$, $y$ increases by $\beta \%$}.
	        \item $\ln{y} = \alpha + \beta x + u$: \ul{$x$ increases by 1 unit, $y$ increases by $100 \beta \%$}.
	        \item $y = \alpha + \beta \ln{x} + u$: \ul{$x$ increases by $1\%$, $y$ increases by $0.01\beta$ unit.}
	    \end{itemize}
	\end{proposition}
	
	\begin{assumption}
	    Simple regression model assumptions
	    \begin{enumerate}
	        \item Model is \ul{linear} in parameter.
	        \item \ul{Random samples} $\{(x_i, y_i)\}_{i=1}^n$.
	        \item Sample outcomes $\{x_i\}_{i=1}^n$ are not the same.
	        \item $\mathbb{E}(u|x)=0$ conditional on random sample $x$.
	        \item Error is \ul{homoskedastic}. $Var(u|x) = \sigma^2$ for all $x$.
	    \end{enumerate}
	\end{assumption}
	\paragraph{Benefits of MLR compared with SLR}
	    \begin{itemize}
	        \item More accurate causal effect estimation.
	        \item More flexible function forms.
	        \item Could explicitly include more predictors so $\mathbb{E}(u|X) = 0$ is easier to be satisfied.
	        \item MLR4 is less restrictive than SLR4.
	    \end{itemize}
	   
    \begin{property}
        MLR OLS residual satisfies
        \begin{gather*}
            \sum_i {\hat{u_i}} = 0\\
            \sum_i {x_{ji} \hat{u_i}} = 0,\ \forall i \in \{1, 2, \dots, k\}
        \end{gather*}
    \end{property}
    
    \begin{property}
        MLR OLS estimators $\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_k$ pass through the average point.
        \[
            \overline{y} = \hat{\beta}_0 + \hat{\beta}_1 \overline{x}_1 + \dots + \hat{\beta}_k \overline{x}_k
        \]
    \end{property}
    \begin{proof}
    \end{proof}
    
    \subsection{Partialling Out}
    \subsubsection{Steps}
        \begin{enumerate}
            \item Regress $x_1$ on $x_2, x_3, \dots, x_K$ and calculate the residual $\widetilde{r}_1$. 
            \item Regress $y$ on $\widetilde{r}_1$ with simple regression and find the estimated coefficient $\hat{\lambda}_1$.
            \item Then the multiple regression coefficient estimator $\hat{\beta}_1$ is
            \[
                \hat{\beta}_1 = \hat{\lambda}_1 = \frac{\sum_{i}{y_i \widetilde{r}_{1i}}}{\sum_i {(\widetilde{r}_{1i})^2}}
            \]
        \end{enumerate}
        \begin{proof}
        \end{proof}
        
    \subsubsection{Interpretation}
    \par This OLS estimator only uses the \ul{unique variance} of one independent variable. And the parts of variation correlated with other independent variables is partialled out.
    
    \begin{assumption}Multiple Regression Assumptions
        \begin{enumerate}
            \item (MLR1) The model is \ul{linear} in parameters. 
            \item (MLR2) \ul{Random sample} from population $\{(x_{1i}, \dots x_{ki}, y_i\}_{i=1}^n$.
            \item (MLR3) No perfect \ul{multicollinearity}.
            \item (MLR4) \ul{Zero expected error} conditional on population slice given by $X$.
            \[
                \mathbb{E}(u|X) = \mathbb{E}(u|x_1, x_2, \dots, x_k) = 0
            \]
            \item (MLR5) \ul{Homoskedastic error} conditional on population slice given by $X$.
            \[
                Var(u|X) = \sigma^2
            \]
            \item (MLR6, \emph{strict assumption}) \ul{Normally distributed error}
            \[
                u \sim \mc{N}(0, \sigma^2)
            \]
        \end{enumerate}
    \end{assumption}
    
    \subsection{Omitted Variable Bias}
    \par Suppose population follows the \emph{real model} 
    \begin{equation}
        y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_k x_{ki} + u_i
    \end{equation}
    Consider the \emph{alternative model}, and \ul{$x_k$ is omitted}, which is assumed to be relevant.
    \begin{equation}
        y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_{k-1} x_{(k-1)i} + r_i
    \end{equation}
    and use the partialling-out result on the second regression we have
    \[
        \tilde{\beta}_1 = \frac{\sum_i \tilde{r}_{1i} y_i}{( \tilde{r}_{1i})^2}
    \]
    where $\tilde{r_{1i}} = x_{1i} - \tilde{\alpha}_0 - \tilde{\alpha}_2 x_{2i} - \dots - \tilde{\alpha}_{k-1}x_{(k-1)i}$
    
    
    \begin{equation}
        \tilde{\beta}_1 = \hat{\beta}_1 + \hat{\beta}_k \frac{\sum (\tilde{r}_{1i} x_{ki})}{\sum (\tilde{r}_{1i})^2}
    \end{equation}
    and take the expectation 
    \begin{gather*}
        \mathbb{E}(\tilde{\beta}_1 | X) = \beta_1 + \tilde{\delta}_1 \beta_k \\
        Bias(\tilde{\beta}_1) = \tilde{\delta}_1 \beta_k
    \end{gather*}
    \paragraph{Conclusion} the sign of bias depends on $cov(x_1, x_k)$ and $\beta_k$.
    
    \begin{proof}
    	\textcolor{red}{TODO}
    \end{proof}
    
    \section{Matrix Differentiation*}
    	\begin{equation}
    		\textbf{y} = \textbf{A} \textbf{x} \implies \pd{\textbf{y}}{\textbf{x}} = \textbf{A}
    	\end{equation}
    	Let $\alpha = \textbf{y}' \textbf{A} \textbf{x}$, notice that $\alpha \in \R$, then 
    	\begin{gather}
    		\pd{\alpha}{\textbf{x}} = \textbf{y}' \textbf{A} \\
    		\pd{	\alpha}{\textbf{y}} = \textbf{x}' \textbf{A}'
    	\end{gather}
    	Consider special case $\alpha = \textbf{x}' \textbf{A} \textbf{x}$, then 
    	\begin{equation}
    		\pd{\alpha}{\textbf{x}} = \textbf{x}' \textbf{A} + \textbf{x}' \textbf{A}'
    	\end{equation}
    	and if $\textbf{A}$ is symmetric, 
    	\begin{equation}
    		\pd{\alpha}{\textbf{x}} = 2 \textbf{x}' \textbf{A}
    	\end{equation}
    
    \section{Multiple Regression in Matrices}
    	\subsection{The Model}
    		\paragraph{Predictor}
    			\[
    				\textbf{X} \in \mathbb{M}_{n \times (k+1)}(\R)
    			\]
    			where $n$ is the number of observations and $k$ is the number of features.
    			\[
    				\textbf{X} = \begin{bmatrix}
    					1 & x_{11} & \dots & x_{1k} \\
    					1 & x_{21} & \dots & x_{2k} \\
    					\vdots \\
    					1 & x_{n1} & \dots & x_{nk} \\
    				\end{bmatrix}_{n \times (k+1)}
    			\]
    		\paragraph{Model}
    			\[
    				\textbf{y} = \textbf{X} \vec{\beta} + \textbf{u}
    			\]
    		\paragraph{First order condition for OLS}
    			\begin{gather*}
    				\textbf{X}' \hat{u} = \textbf{0} \in \R^{k+1} \\
    				\iff \textbf{X}' (\textbf{y} - \textbf{X} \hat{\beta}) = \textbf{0} \in \R^{k+1}
    			\end{gather*}
    		\paragraph{Estimator}	
    			\[
    				\hat{\beta} = (\textbf{X}' \textbf{X})^{-1} \textbf{X}' \textbf{y}
    			\]
    			\begin{proof}
    				From the first order condition for the OLS estimator
    				\begin{gather*}
    					\textbf{X}' (\textbf{y} - \textbf{X} \hat{\beta}) = \textbf{0} \\
    					\implies \textbf{X}' \textbf{y} - \textbf{X}' \textbf{X} \hat{\beta} = \textbf{0} \\
    					\implies \textbf{X}' \textbf{y} = \textbf{X}' \textbf{X} \hat{\beta} \\
    					\implies \hat{\beta} = (\textbf{X}' \textbf{X})^{-1} \textbf{X}' \textbf{y}
    				\end{gather*}
    				and note that $(\textbf{X}' \textbf{X})$ is guaranteed to be invertible by assumption \emph{no perfect multi-collinearity}.
    			\end{proof}
    		\paragraph{Sum Squared Residual}
    		\[
    			SSR(\hat{\beta}) = \hat{u}' \cdot \hat{u} = (\textbf{y} - \textbf{X} \hat{\beta})' \cdot (\textbf{y} - \textbf{X} \hat{\beta})
    		\]
    		
    		\subsection{Variance Matrix}
    			\par Consider 
    			\begin{gather*}
    				\vec{z}_t = [z_{1t}, z_{2t}, \dots z_{nt}]' \\
    				\vec{z}_s = [z_{1s}, z_{2s}, \dots z_{ns}]'
    			\end{gather*}
    			Notice that the variance and covariance are defined as
    			\begin{gather*}
    				Var(\vec{z}_t) = \expect{(\vec{z}_t - \expect{\vec{z}_t})^2} \\
    				Cov(\vec{z}_t, \vec{z}_s) = \expect{(\vec{z}_t - \expect{\vec{z}_t})(\vec{z}_s - \expect{\vec{z}_s})}
    			\end{gather*}
    			The \textbf{variance matrix} of $\textbf{z} = [z_1, z_2, \dots, z_n]$ is given by
    			\begin{gather*}
    				Var(\textbf{z}) = \begin{bmatrix}
    					Var(z_1) & Cov(z_1, z_2) & \dots & Cov(z_1, z_n) \\
    					Cov(z_2, z_1) & \dots \\
    					\vdots \\
    					Cov(z_n, z_1) & \dots & \dots & Var(z_n) \\
    				\end{bmatrix} \\
    				= 
    				\begin{bmatrix}
    					\expect{(z_1 - \overline{z}_1)^2} & \expect{(z_1 - \overline{z}_1)(z_2 - \overline{z}_2)} & \dots \\
    					\expect{(z_2 - \overline{z}_2)(z_1 - \overline{z}_1)} & \dots \\
    					\vdots \\
    					\expect{(z_n - \overline{z}_n)(z_1 - \overline{z}_1)} & \dots & \expect{(z_n - \overline{z}_n)^2} \\
    				\end{bmatrix} \\
    				= \textcolor{red}{
    				\expect{(\textbf{z} - \expect{\textbf{z}})_{n \times 1} \cdot (\textbf{z} - \expect{\textbf{z}})'_{1 \times n}} \in \mathbb{M}_{n \times n}
    				}
    			\end{gather*}
    			In the special case $\expect{\vec{z}} = \vec{0}$, variance is reduced to
    			\[
    				\textcolor{red}{
    					Var(\textbf{z}) = \expect{\textbf{z} \cdot \textbf{z}'}
    				}
    			\]
    			\paragraph{Residual} Since residual $u_i$ are \emph{i.i.d} with variance $\sigma^2$, the variance matrix of $\textbf{u}$ is
    			\[\textcolor{red}{
    				Var(\textbf{u}) = \expect{\textbf{u} \cdot \textbf{u}'} = \sigma^2 \textbf{I}_n
    			}\]
    			
    			\paragraph{Estimator} If $\hat{\beta}$ is unbiased, $\expect{\hat{\beta} | \textbf{X}} = \vec{\beta}$, then 
    			\[\textcolor{red}{
    				Var(\hat{\beta}|\textbf{X}) = \expect{(\hat{\beta} - \vec{\beta})\cdot(\hat{\beta} - \vec{\beta})' | \textbf{X}} \in \mathbb{M}_{(k+1) \times (k+1)}
    			}\]
    \section{Slide 7}
    	\subsection{Assumptions (MLRs) in Matrix Form}
    		\paragraph{E.1.} \emph{linear in parameter}
    		\[
    			\textbf{y} = \textbf{X}\vec{\beta} + \textbf{u}
    		\]
    		\paragraph{E.2.} \emph{no perfect multi-collinearity}
    		\[
    			rank(\textbf{X}) = k +1
    		\]
    		\paragraph{E.3.} Error has expected value of \textbf{0} conditional on \textbf{X}.
    		\[
    			\expect{\textbf{u}|\textbf{X}} = \textbf{0}
    		\]
    		\paragraph{E.4.} Error \textbf{u} is \emph{homoscedastic}.
    		\[
    			Var(\textbf{u}|\textbf{X}) = \sigma^2 \textbf{I}_n
    		\]
    		\paragraph{E.5.} \emph{Normally distributed} error \textbf{u}. Note that this assumption is relatively strong.
    		\[
    			\textbf{u} \sim \mc{N}(\textbf{0}, \sigma^2 \textbf{I}_n)
    		\]
    	\subsection{Properties of OLS Estimator}
    	\begin{theorem}
    		Given \emph{E.1. E.2. E.3.}, the OLS estimator $\hat{\beta}$ is an unbiased estimator for $\vec{\beta}$.
    		\[
    			\expect{\hat{\beta}|\textbf{X}} = \vec{\beta}
    		\]
    	\end{theorem}
    	\begin{proof}
    		\begin{gather*}
    			\hat{\beta} = (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{y} \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' (\textbf{X}\vec{\beta} + \textbf{u}) \\
    			= \vec{\beta} + (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{u} \\
    			\tx{Taking expectation conditional on \textbf{X} on both sides, }\\
    			\expect{\hat{\beta}|\textbf{X}} = \vec{\beta} + (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{0} 
    			= \vec{\beta}
    		\end{gather*}
    	\end{proof}
    	
    	\begin{lemma}
    		Suppose $\textbf{A} \in \mathbb{M}_{m\times n}$ and $\textbf{z} \in \mathbb{M}_{n\times 1}$ then 
    		\[
    			Var(\textbf{A} \textbf{z}) = \textbf{A} Var(\textbf{z}) \textbf{A}'
    		\]
    	\end{lemma}
    	\begin{theorem}
    		Given $E.1 \sim E.4$
    		\[
    			Var(\hat{\beta}|\textbf{X}) = (\textbf{X}' \textbf{X})^{-1} \sigma^2
    		\]
    	\end{theorem}
    	\begin{proof}
    		\begin{gather*}
    			Var(\hat{\beta}|\textbf{X}) = Var((\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{y} | \textbf{X}) \\ 
    			= Var((\textbf{X}'\textbf{X})^{-1} \textbf{X}'(\textbf{X}\vec{\beta} + \textbf{u})|\textbf{X}) \\
    			= Var(\textbf{X}'\textbf{X})^{-1} \textbf{X}'\textbf{u}|\textbf{X}) \\
    			\tx{By the lemma above, } \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' Var(\textbf{u}|\textbf{X}) [(\textbf{X}'\textbf{X})^{-1} \textbf{X}']' \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' Var(\textbf{u}|\textbf{X}) \textbf{X}'' (\textbf{X}'\textbf{X})^{-1} \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \sigma^2 \textbf{I}_n \textbf{X} (\textbf{X}'\textbf{X})^{-1} \\
    			= \sigma^2 (\textbf{X}'\textbf{X})^{-1}
    		\end{gather*}
    	\end{proof}
    	
    	\begin{theorem}[Gause-Markov]
    		Given $E.1. \sim E.4.$, the OLS estimator is the \ul{best linear unbiased estimator}(BLUE). \\(\emph{The best} here means the OLS has the least variance among all estimators.)
    	\end{theorem}
    	
    	\subsection{Variance Inflation}
    	\par Let $j \in \{1, 2, \dots, k\}$, then the variance of an individual estimator on particular feature $j$ is 
    	\[\textcolor{red}{
    		Var(\hat{\beta}_j) = \frac{\sigma^2}{(1-R_j^2)SST_j}
    	}\]
    	where
    	\begin{gather*}
    		SST_j = \sum_{i=1}^n{(x_{ij} - \overline{x}_j)^2}
    	\end{gather*}
    	and $R_j^2$ is the coefficient of determination while regressing $x_j$ on \ul{all other} features $x_i, \forall i \neq j$.
    	\begin{definition}
    		The \textbf{variance inflation} on estimator for feature $j$ is 
    		\[
    			VIF_j = \frac{1}{1-R_j^2}
    		\]
    	\end{definition}
    	\begin{remark}[Interpretation] the standard error of estimator on a particular variable ($\hat{\beta}_j$) is \emph{inflated} by it's($x_j$) relationship with other explanatory variables.
    	\end{remark}	

    	\paragraph{Solutions to high VIF}
    		\begin{enumerate}
    			\item Drop the explanatory variable.
    			\item Use ratio $\frac{x_i}{x_j}$ instead.
    			\item Ridge regression.
    		\end{enumerate}
    	\begin{remark}
    		VIF highlights the importantce of \textbf{not} including redundant predictors.
    	\end{remark}
    	
    \section{Slide 8: Multiple Regression-Inference}
    \paragraph{Hypothesis Testing} on multiple regression model 
    	\[
    		y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots \beta_k x_{ik} + u_i
    	\]
    	
    	\subsection{t-test for significance of individual predicator}
    		\paragraph{Test statistic} Given $MLR.1 \sim MLR.6$ (need $\textbf{u} \sim \mc{N}(\textbf{0}, \sigma^2 \textbf{I}_n)$),
    		\[
    			t = \frac{\hat{\beta}_j - b}{s.e.(\hat{\beta}_j)} \sim t_{n-k-1}
    		\]
    		where 
    		\begin{gather*}
    			H_0: \beta_j = b \\
    			H_1: \beta_j (\neq, >, <) b
    		\end{gather*}
    	
    	
    	\subsection{t-test for comparing 2 coefficients}
    	\paragraph{Test statistic}
    	\[
    		t = \frac{(\hat{\beta}_i - \hat{\beta}_j) - b}{s.e.(\hat{\beta}_i - \hat{\beta}_j)} \sim t_{n-k-1}
    	\]
    	where 
    		\begin{gather*}
    			H_0: \beta_i - \beta_j = b \\
    			H_1: \beta_i - \beta_j (\neq, >, <) b
    		\end{gather*}
    	notice
    	\begin{gather*}
    		s.e.(\hat{\beta}_i - \hat{\beta}_j) = \sqrt{Var(\hat{\beta}_i - \hat{\beta}_j)} \\
    		= \sqrt{Var(\hat{\beta}_i) + Var(\hat{\beta}_j) - 2Cov(\hat{\beta}_i, \hat{\beta}_j)}
    	\end{gather*}
    	
    	\subsection{Partial F-test for joint significance}
    	\begin{gather*}
    		H_0: \beta_i = \beta_j = \beta_k = \dots = 0 \\
    		H_1: \exists\ z \in \{i, j, k, \dots \}\ s.t.\ \beta_z \neq 0
    	\end{gather*}
    	Test significance by comparing the \emph{restricted} and \emph{unrestricted} models, see whether restricting the model by removing certain explanatory variables "significantly" hurts the fit of the model.
    	\[
    		df = (q, n-k-1)
    	\]
    	\paragraph{Test statistic}
    	\begin{gather*}
    		F = \frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)} \\
    		\tx{or} \\
    		F' = \frac{(R^2_{ur} - R^2_r)/q}{(1-R^2_{ur})/(n-k-1)}
    	\end{gather*}
    	
    	\subsection{Full F-test for the significance of the model}
    	\begin{gather*}
    		H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0 \\
    		H_1: \exists\ i \in \{1, 2, \dots ,3\}\ s.t.\ \beta_i \neq 0
    	\end{gather*}
    	\begin{remark}
    		$R^2$ version only and substitute $R^2_{r} = 0$, since $SSR_{r}$ is undefined.
    	\end{remark}
\end{document}





























