\documentclass[]{article}
\title{ECO375 Applied Econometrics I \\ \small Lecture Slide Notes}
\author{Tianyu Du}
\date{\today}

\usepackage{spikey}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{chngcntr}

\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}

\begin{document}
	\maketitle
	\doclicenseThis
	\textbf{Updated} version can be found on \url{www.tianyudu.com/notes}
	
	\tableofcontents
	\section{Slide 4: Simple \& Multiple Regression - Estimation}
	\subsection{Regression Model}
	\begin{assumption}
	    Assuming the population follows
	    \[
	        y = \beta_0 + \beta_1 x + u
	    \]
	    and assume that \emph{$x$ causes y}.
	\end{assumption}
	
	\subsection{OLS}
	\begin{gather*}
	    \min_{\vec{\beta}} \sum_{i} (y_i - \hat{y}_i)^2 \\
	    \tx{With FOC:} \\
	    \sum_{i}(y_i - \hat{y}_i) = 0 \\
	    \sum_{i}x_{ij}(y_i - \hat{y}_i) = 0,\ \forall j
	\end{gather*}
	
	\begin{remark}
	    Both $\hat{\beta}_0$ and $\hat{\beta}_j$ are functions of \emph{random variables} and therefore themselves \emph{random} with \emph{sampling distribution}. And the estimated coefficients are random up to random sample chosen.
	\end{remark}
	
	\begin{property}
	    Properties of OLS estimators
	    \begin{itemize}
	        \item \textbf{Unbiased} $\mathbb{E}[\hat{\beta} | X] = \beta$
	        \item \textbf{Consistent} $\hat{\beta} \to \beta$ as $n \to \infty$
	        \item \textbf{Efficient/Good} min variance.
	    \end{itemize}
	\end{property}
	
	\begin{definition}
	    The \textbf{Simple Coefficient of Determination}
	    \[
	        R^2 = \frac{SSE}{SST}
	    \]
	    and $SS\underline{Total} = SS\underline{Explained} + SS\underline{Residual}$
	    \[
	        \sum_i {(y_i - \overline{y})^2} = \sum_i {(\hat{y}_i - \overline{y})^2} + \sum_i {(y_i - \hat{y}_i)^2}
	    \]
	\end{definition}
	
	\begin{proposition}[Logarithms] Interpretation with logarithmic transformation.
	    \begin{itemize}
	        \item $\ln{y} = \alpha + \beta \ln{y} + u$: \ul{$x$ increases by $1\%$, $y$ increases by $\beta \%$}.
	        \item $\ln{y} = \alpha + \beta x + u$: \ul{$x$ increases by 1 unit, $y$ increases by $100 \beta \%$}.
	        \item $y = \alpha + \beta \ln{x} + u$: \ul{$x$ increases by $1\%$, $y$ increases by $0.01\beta$ unit.}
	    \end{itemize}
	\end{proposition}
	
	\begin{assumption}
	    Simple regression model assumptions
	    \begin{enumerate}
	        \item Model is \ul{linear} in parameter.
	        \item \ul{Random samples} $\{(x_i, y_i)\}_{i=1}^n$.
	        \item Sample outcomes $\{x_i\}_{i=1}^n$ are not the same.
	        \item $\mathbb{E}(u|x)=0$ conditional on random sample $x$.
	        \item Error is \ul{homoskedastic}. $Var(u|x) = \sigma^2$ for all $x$.
	    \end{enumerate}
	\end{assumption}
	\paragraph{Benefits of MLR compared with SLR}
	    \begin{itemize}
	        \item More accurate causal effect estimation.
	        \item More flexible function forms.
	        \item Could explicitly include more predictors so $\mathbb{E}(u|X) = 0$ is easier to be satisfied.
	        \item MLR4 is less restrictive than SLR4.
	    \end{itemize}
	   
    \begin{property}
        MLR OLS residual satisfies
        \begin{gather*}
            \sum_i {\hat{u_i}} = 0\\
            \sum_i {x_{ji} \hat{u_i}} = 0,\ \forall i \in \{1, 2, \dots, k\}
        \end{gather*}
    \end{property}
    
    \begin{property}
        MLR OLS estimators $\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_k$ pass through the average point.
        \[
            \overline{y} = \hat{\beta}_0 + \hat{\beta}_1 \overline{x}_1 + \dots + \hat{\beta}_k \overline{x}_k
        \]
    \end{property}
    \begin{proof}
    \end{proof}
    
    \subsection{Partialling Out}
    \subsubsection{Steps}
        \begin{enumerate}
            \item Regress $x_1$ on $x_2, x_3, \dots, x_K$ and calculate the residual $\widetilde{r}_1$. 
            \item Regress $y$ on $\widetilde{r}_1$ with simple regression and find the estimated coefficient $\hat{\lambda}_1$.
            \item Then the multiple regression coefficient estimator $\hat{\beta}_1$ is
            \[
                \hat{\beta}_1 = \hat{\lambda}_1 = \frac{\sum_{i}{y_i \widetilde{r}_{1i}}}{\sum_i {(\widetilde{r}_{1i})^2}}
            \]
        \end{enumerate}
        \begin{proof}
        \end{proof}
        
    \subsubsection{Interpretation}
    \par This OLS estimator only uses the \ul{unique variance} of one independent variable. And the parts of variation correlated with other independent variables is partialled out.
    
    \begin{assumption}Multiple Regression Assumptions
        \begin{enumerate}
            \item (MLR1) The model is \ul{linear} in parameters. 
            \item (MLR2) \ul{Random sample} from population $\{(x_{1i}, \dots x_{ki}, y_i\}_{i=1}^n$.
            \item (MLR3) No perfect \ul{multicollinearity}.
            \item (MLR4) \ul{Zero expected error} conditional on population slice given by $X$.
            \[
                \mathbb{E}(u|X) = \mathbb{E}(u|x_1, x_2, \dots, x_k) = 0
            \]
            \item (MLR5) \ul{Homoskedastic error} conditional on population slice given by $X$.
            \[
                Var(u|X) = \sigma^2
            \]
            \item (MLR6, \emph{strict assumption}) \ul{Normally distributed error}
            \[
                u \sim \mc{N}(0, \sigma^2)
            \]
        \end{enumerate}
    \end{assumption}
    
    \subsection{Omitted Variable Bias}
    \par Suppose population follows the \emph{real model} 
    \begin{equation}
        y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_k x_{ki} + u_i
    \end{equation}
    Consider the \emph{alternative model}, and \ul{$x_k$ is omitted}, which is assumed to be relevant.
    \begin{equation}
        y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_{k-1} x_{(k-1)i} + r_i
    \end{equation}
    and use the partialling-out result on the second regression we have
    \[
        \tilde{\beta}_1 = \frac{\sum_i \tilde{r}_{1i} y_i}{( \tilde{r}_{1i})^2}
    \]
    where $\tilde{r_{1i}} = x_{1i} - \tilde{\alpha}_0 - \tilde{\alpha}_2 x_{2i} - \dots - \tilde{\alpha}_{k-1}x_{(k-1)i}$
    
    
    \begin{equation}
        \tilde{\beta}_1 = \hat{\beta}_1 + \hat{\beta}_k \frac{\sum (\tilde{r}_{1i} x_{ki})}{\sum (\tilde{r}_{1i})^2}
    \end{equation}
    and take the expectation 
    \begin{gather*}
        \mathbb{E}(\tilde{\beta}_1 | X) = \beta_1 + \tilde{\delta}_1 \beta_k \\
        Bias(\tilde{\beta}_1) = \tilde{\delta}_1 \beta_k
    \end{gather*}
    \paragraph{Conclusion} the sign of bias depends on $cov(x_1, x_k)$ and $\beta_k$.
    
    \begin{proof}
    	\textcolor{red}{TODO}
    \end{proof}
    
    \section{Slide 5: Matrix Algebra for Regression Analysis}
    	\begin{equation}
    		\textbf{y} = \textbf{A} \textbf{x} \implies \pd{\textbf{y}}{\textbf{x}} = \textbf{A}
    	\end{equation}
    	Let $\alpha = \textbf{y}' \textbf{A} \textbf{x}$, notice that $\alpha \in \R$, then 
    	\begin{gather}
    		\pd{\alpha}{\textbf{x}} = \textbf{y}' \textbf{A} \\
    		\pd{	\alpha}{\textbf{y}} = \textbf{x}' \textbf{A}'
    	\end{gather}
    	Consider special case $\alpha = \textbf{x}' \textbf{A} \textbf{x}$, then 
    	\begin{equation}
    		\pd{\alpha}{\textbf{x}} = \textbf{x}' \textbf{A} + \textbf{x}' \textbf{A}'
    	\end{equation}
    	and if $\textbf{A}$ is symmetric, 
    	\begin{equation}
    		\pd{\alpha}{\textbf{x}} = 2 \textbf{x}' \textbf{A}
    	\end{equation}
    
    \section{Slide 6: Multiple Regression in Matrix Algebra}
    	\subsection{The Model}
    		\paragraph{Predictor}
    			\[
    				\textbf{X} \in \mathbb{M}_{n \times (k+1)}(\R)
    			\]
    			where $n$ is the number of observations and $k$ is the number of features.
    			\[
    				\textbf{X} = \begin{bmatrix}
    					1 & x_{11} & \dots & x_{1k} \\
    					1 & x_{21} & \dots & x_{2k} \\
    					\vdots \\
    					1 & x_{n1} & \dots & x_{nk} \\
    				\end{bmatrix}_{n \times (k+1)}
    			\]
    		\paragraph{Model}
    			\[
    				\textbf{y} = \textbf{X} \vec{\beta} + \textbf{u}
    			\]
    		\paragraph{First order condition for OLS}
    			\begin{gather*}
    				\textbf{X}' \hat{u} = \textbf{0} \in \R^{k+1} \\
    				\iff \textbf{X}' (\textbf{y} - \textbf{X} \hat{\beta}) = \textbf{0} \in \R^{k+1}
    			\end{gather*}
    		\paragraph{Estimator}	
    			\[
    				\hat{\beta} = (\textbf{X}' \textbf{X})^{-1} \textbf{X}' \textbf{y}
    			\]
    			\begin{proof}
    				From the first order condition for the OLS estimator
    				\begin{gather*}
    					\textbf{X}' (\textbf{y} - \textbf{X} \hat{\beta}) = \textbf{0} \\
    					\implies \textbf{X}' \textbf{y} - \textbf{X}' \textbf{X} \hat{\beta} = \textbf{0} \\
    					\implies \textbf{X}' \textbf{y} = \textbf{X}' \textbf{X} \hat{\beta} \\
    					\implies \hat{\beta} = (\textbf{X}' \textbf{X})^{-1} \textbf{X}' \textbf{y}
    				\end{gather*}
    				and note that $(\textbf{X}' \textbf{X})$ is guaranteed to be invertible by assumption \emph{no perfect multi-collinearity}.
    			\end{proof}
    		\paragraph{Sum Squared Residual}
    		\[
    			SSR(\hat{\beta}) = \hat{u}' \cdot \hat{u} = (\textbf{y} - \textbf{X} \hat{\beta})' \cdot (\textbf{y} - \textbf{X} \hat{\beta})
    		\]
    		
    		\subsection{Variance Matrix}
    			\par Consider 
    			\begin{gather*}
    				\vec{z}_t = [z_{1t}, z_{2t}, \dots z_{nt}]' \\
    				\vec{z}_s = [z_{1s}, z_{2s}, \dots z_{ns}]'
    			\end{gather*}
    			Notice that the variance and covariance are defined as
    			\begin{gather*}
    				Var(\vec{z}_t) = \expect{(\vec{z}_t - \expect{\vec{z}_t})^2} \\
    				Cov(\vec{z}_t, \vec{z}_s) = \expect{(\vec{z}_t - \expect{\vec{z}_t})(\vec{z}_s - \expect{\vec{z}_s})}
    			\end{gather*}
    			The \textbf{variance matrix} of $\textbf{z} = [z_1, z_2, \dots, z_n]$ is given by
    			\begin{gather*}
    				Var(\textbf{z}) = \begin{bmatrix}
    					Var(z_1) & Cov(z_1, z_2) & \dots & Cov(z_1, z_n) \\
    					Cov(z_2, z_1) & \dots \\
    					\vdots \\
    					Cov(z_n, z_1) & \dots & \dots & Var(z_n) \\
    				\end{bmatrix} \\
    				= 
    				\begin{bmatrix}
    					\expect{(z_1 - \overline{z}_1)^2} & \expect{(z_1 - \overline{z}_1)(z_2 - \overline{z}_2)} & \dots \\
    					\expect{(z_2 - \overline{z}_2)(z_1 - \overline{z}_1)} & \dots \\
    					\vdots \\
    					\expect{(z_n - \overline{z}_n)(z_1 - \overline{z}_1)} & \dots & \expect{(z_n - \overline{z}_n)^2} \\
    				\end{bmatrix} \\
    				= \textcolor{red}{
    				\expect{(\textbf{z} - \expect{\textbf{z}})_{n \times 1} \cdot (\textbf{z} - \expect{\textbf{z}})'_{1 \times n}} \in \mathbb{M}_{n \times n}
    				}
    			\end{gather*}
    			In the special case $\expect{\vec{z}} = \vec{0}$, variance is reduced to
    			\[
    				\textcolor{red}{
    					Var(\textbf{z}) = \expect{\textbf{z} \cdot \textbf{z}'}
    				}
    			\]
    			\paragraph{Residual} Since residual $u_i$ are \emph{i.i.d} with variance $\sigma^2$, the variance matrix of $\textbf{u}$ is
    			\[\textcolor{red}{
    				Var(\textbf{u}) = \expect{\textbf{u} \cdot \textbf{u}'} = \sigma^2 \textbf{I}_n
    			}\]
    			
    			\paragraph{Estimator} If $\hat{\beta}$ is unbiased, $\expect{\hat{\beta} | \textbf{X}} = \vec{\beta}$, then 
    			\[\textcolor{red}{
    				Var(\hat{\beta}|\textbf{X}) = \expect{(\hat{\beta} - \vec{\beta})\cdot(\hat{\beta} - \vec{\beta})' | \textbf{X}} \in \mathbb{M}_{(k+1) \times (k+1)}
    			}\]
    \section{Slide 7: Multiple Regression - Properties}
    	\subsection{Assumptions (MLRs) in Matrix Form}
    		\paragraph{E.1.} \emph{linear in parameter}
    		\[
    			\textbf{y} = \textbf{X}\vec{\beta} + \textbf{u}
    		\]
    		\paragraph{E.2.} \emph{no perfect multi-collinearity}
    		\[
    			rank(\textbf{X}) = k +1
    		\]
    		\paragraph{E.3.} Error has expected value of \textbf{0} conditional on \textbf{X}.
    		\[
    			\expect{\textbf{u}|\textbf{X}} = \textbf{0}
    		\]
    		\paragraph{E.4.} Error \textbf{u} is \emph{homoscedastic}.
    		\[
    			Var(\textbf{u}|\textbf{X}) = \sigma^2 \textbf{I}_n
    		\]
    		\paragraph{E.5.} \emph{Normally distributed} error \textbf{u}. Note that this assumption is relatively strong.
    		\[
    			\textbf{u} \sim \mc{N}(\textbf{0}, \sigma^2 \textbf{I}_n)
    		\]
    	\subsection{Properties of OLS Estimator}
    	\begin{theorem}
    		Given \emph{E.1. E.2. E.3.}, the OLS estimator $\hat{\beta}$ is an unbiased estimator for $\vec{\beta}$.
    		\[
    			\expect{\hat{\beta}|\textbf{X}} = \vec{\beta}
    		\]
    	\end{theorem}
    	\begin{proof}
    		\begin{gather*}
    			\hat{\beta} = (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{y} \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' (\textbf{X}\vec{\beta} + \textbf{u}) \\
    			= \vec{\beta} + (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{u} \\
    			\tx{Taking expectation conditional on \textbf{X} on both sides, }\\
    			\expect{\hat{\beta}|\textbf{X}} = \vec{\beta} + (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{0} 
    			= \vec{\beta}
    		\end{gather*}
    	\end{proof}
    	
    	\begin{lemma}
    		Suppose $\textbf{A} \in \mathbb{M}_{m\times n}$ and $\textbf{z} \in \mathbb{M}_{n\times 1}$ then 
    		\[
    			Var(\textbf{A} \textbf{z}) = \textbf{A} Var(\textbf{z}) \textbf{A}'
    		\]
    	\end{lemma}
    	\begin{theorem}
    		Given $E.1 \sim E.4$
    		\[
    			Var(\hat{\beta}|\textbf{X}) = (\textbf{X}' \textbf{X})^{-1} \sigma^2
    		\]
    	\end{theorem}
    	\begin{proof}
    		\begin{gather*}
    			Var(\hat{\beta}|\textbf{X}) = Var((\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{y} | \textbf{X}) \\ 
    			= Var((\textbf{X}'\textbf{X})^{-1} \textbf{X}'(\textbf{X}\vec{\beta} + \textbf{u})|\textbf{X}) \\
    			= Var(\textbf{X}'\textbf{X})^{-1} \textbf{X}'\textbf{u}|\textbf{X}) \\
    			\tx{By the lemma above, } \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' Var(\textbf{u}|\textbf{X}) [(\textbf{X}'\textbf{X})^{-1} \textbf{X}']' \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' Var(\textbf{u}|\textbf{X}) \textbf{X}'' (\textbf{X}'\textbf{X})^{-1} \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \sigma^2 \textbf{I}_n \textbf{X} (\textbf{X}'\textbf{X})^{-1} \\
    			= \sigma^2 (\textbf{X}'\textbf{X})^{-1}
    		\end{gather*}
    	\end{proof}
    	
    	\begin{theorem}[Gause-Markov]
    		Given $E.1. \sim E.4.$, the OLS estimator is the \ul{best linear unbiased estimator}(BLUE). \\(\emph{The best} here means the OLS has the least variance among all estimators.)
    	\end{theorem}
    	
    	\subsection{Variance Inflation}
    	\par Let $j \in \{1, 2, \dots, k\}$, then the variance of an individual estimator on particular feature $j$ is 
    	\[\textcolor{red}{
    		Var(\hat{\beta}_j) = \frac{\sigma^2}{(1-R_j^2)SST_j}
    	}\]
    	where
    	\begin{gather*}
    		SST_j = \sum_{i=1}^n{(x_{ij} - \overline{x}_j)^2}
    	\end{gather*}
    	and $R_j^2$ is the coefficient of determination while regressing $x_j$ on \ul{all other} features $x_i, \forall i \neq j$.
    	\begin{definition}
    		The \textbf{variance inflation} on estimator for feature $j$ is 
    		\[
    			VIF_j = \frac{1}{1-R_j^2}
    		\]
    	\end{definition}
    	\begin{remark}[Interpretation] the standard error of estimator on a particular variable ($\hat{\beta}_j$) is \emph{inflated} by it's($x_j$) relationship with other explanatory variables.
    	\end{remark}	

    	\paragraph{Solutions to high VIF}
    		\begin{enumerate}
    			\item Drop the explanatory variable.
    			\item Use ratio $\frac{x_i}{x_j}$ instead.
    			\item Ridge regression.
    		\end{enumerate}
    	\begin{remark}
    		VIF highlights the importantce of \textbf{not} including redundant predictors.
    	\end{remark}
    	
    \section{Slide 8: Multiple Regression - Inference}
    \paragraph{Hypothesis Testing} on multiple regression model 
    	\[
    		y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots \beta_k x_{ik} + u_i
    	\]
    	
    	\subsection{t-test for significance of individual predicator}
    		\paragraph{Test statistic} Given $MLR.1 \sim MLR.6$ (need $\textbf{u} \sim \mc{N}(\textbf{0}, \sigma^2 \textbf{I}_n)$),
    		\[
    			t = \frac{\hat{\beta}_j - b}{s.e.(\hat{\beta}_j)} \sim t_{n-k-1}
    		\]
    		where 
    		\begin{gather*}
    			H_0: \beta_j = b \\
    			H_1: \beta_j (\neq, >, <) b
    		\end{gather*}
    	
    	
    	\subsection{t-test for comparing 2 coefficients}
    	\paragraph{Test statistic}
    	\[
    		t = \frac{(\hat{\beta}_i - \hat{\beta}_j) - b}{s.e.(\hat{\beta}_i - \hat{\beta}_j)} \sim t_{n-k-1}
    	\]
    	where 
    		\begin{gather*}
    			H_0: \beta_i - \beta_j = b \\
    			H_1: \beta_i - \beta_j (\neq, >, <) b
    		\end{gather*}
    	notice
    	\begin{gather*}
    		s.e.(\hat{\beta}_i - \hat{\beta}_j) = \sqrt{Var(\hat{\beta}_i - \hat{\beta}_j)} \\
    		= \sqrt{Var(\hat{\beta}_i) + Var(\hat{\beta}_j) - 2Cov(\hat{\beta}_i, \hat{\beta}_j)}
    	\end{gather*}
    	
    	\subsection{Partial F-test for joint significance}
    	\begin{gather*}
    		H_0: \beta_i = \beta_j = \beta_k = \dots = 0 \\
    		H_1: \exists\ z \in \{i, j, k, \dots \}\ s.t.\ \beta_z \neq 0
    	\end{gather*}
    	Test significance by comparing the \emph{restricted} and \emph{unrestricted} models, see whether restricting the model by removing certain explanatory variables "significantly" hurts the fit of the model.
    	\[
    		df = (q, n-k-1)
    	\]
    	\paragraph{Test statistic}
    	\begin{gather*}
    		F = \frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)} \sim F_{(q, n-k-1)} \\
    		\tx{or} \\
    		F' = \frac{(R^2_{ur} - R^2_r)/q}{(1-R^2_{ur})/(n-k-1)} \sim F_{(q, n-k-1)}
    	\end{gather*}
    	
    	\subsection{Full F-test for the significance of the model}
    	\begin{gather*}
    		H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0 \\
    		H_1: \exists\ i \in \{1, 2, \dots ,3\}\ s.t.\ \beta_i \neq 0
    	\end{gather*}
    	\begin{remark}
    		$R^2$ version only and substitute $R^2_{r} = 0$, since $SSR_{r}$ is undefined.
    	\end{remark}
    	\paragraph{Test statistic}
    	\[
    		F = \frac{R^2_{ur}/k}{(1-R^2_{ur})/(n-k-1)} \sim F_{(k, n-k-1)}
    	\]
    	
    	\subsection{F-test for general restrictions}
    	\begin{remark}
    		Use the $SSR$ version of $Fstatistic$ only since the $SST$ for restricted and unrestricted models are different.
    	\end{remark}
    	\begin{remark}
    		We only reject or failed to reject $H_0$, we never accept $H_0$ in a hypothesis test.
    	\end{remark}
    	
    \section{Slide 9: Multiple Regression - Further Issues}
    	\subsection{Data Scaling}
    		\subsubsection{Mutiplier}
    		\begin{enumerate}
    			\item Enlarge $x_j$ by factor $a$: $\hat{\beta}_j$ shrinks by $a$.
    			\item Enlarge $y$ by factor $a$: \textbf{all} $\hat{\beta}_i$ enlarged by $a$.
    			\item \textcolor{red}{Test statistic $t = \frac{\hat{\beta}}{s.e.(\hat{\beta})} = \frac{a\hat{\beta}}{s.e.(a \hat{\beta})}$ is unaffected.}
    		\end{enumerate}
    	
    		\subsubsection{Standardization}
    			\paragraph{Standardized variable} For $j^{th}$ observation of explanatory variable $x$, 
    				\[
    					z_j = \frac{x_j - \overline{x}}{\sigma_{x}}
    				\]
    				which satisfies
    				\[
    					\expect{z_j} = 0,\ Var(z_j) = 1
    				\]
    			\paragraph{Properties} Consider model and find the estimator of regressing standardized $y$ on standardized $x$.
    				\[
    					y_i = \hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \dots + \hat{\beta}_k x_{ik} + \hat{u}_i
    				\]
    				Since OLS estimator passes through the mean,
    				\begin{gather*}
    					\overline{y} = \hat{\beta}_0 + \hat{\beta}_1 \overline{x}_1 + \dots \hat{\beta}_k \overline{x}_k \\
    					\implies (y_i - \overline{y}) = \hat{\beta}_1 (x_{i1} - \overline{x}_1) + \dots + \hat{\beta}_k (x_{ik} - \overline{x}_k) + \hat{u}_i \\
    					\implies \frac{y_i - \overline{y}}{\sigma_y} = 
    					\frac{\hat{\beta}_1 \sigma_{x_1}}{\sigma_y} \frac{x_{i1} - \overline{x}_1}{\sigma_{x_1}} + \dots + 
    					\frac{\hat{\beta}_k \sigma_{x_k}}{\sigma_y} \frac{x_{ik} - \overline{x}_k}{\sigma_{x_k}} + \frac{\hat{u}_i}{\sigma_y} \\
    					\implies b_j = \frac{\hat{\beta}_j \sigma_{x_j}}{\sigma_y}
    				\end{gather*}
    			\begin{remark}[Interpretation]
    				$x_j$ increases by 1 \textbf{std}, y increases by $b_j = \frac{\hat{\beta}_j \sigma_{x_j}}{\sigma_y}$ \textbf{std}, \emph{ceteris paribus}.
    			\end{remark}
    	\subsection{Logarithmic Function}
    		\par \textbf{Exact} interpretation of log transformation.
    			\[
    				\ln(y_i) = \hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \dots \hat{\beta}_k x_{ik} + \hat{u}_i
    			\]
    			\begin{proof}[Derive]
	    			\begin{gather*}
	    				\ln(y_2) - \ln(y_1) = \hat{\beta}_j \Delta x_j \\
	    				\implies \ln(\frac{y_2}{y_1}) = \hat{\beta}_j \Delta x_j \\
	    				\implies \frac{y_2}{y_1} = exp(\hat{\beta}_j \Delta x_j) \\
	    				\implies \frac{y_2 - y_1}{y_1} 
	    				= \frac{y_2}{y_1} - 1 \\
	    				\implies \%\Delta y= exp(\hat{\beta}_j \Delta x_j) - 1
	    			\end{gather*}
    			\end{proof}
    	\subsection{Quadratics and Polynomials}
    		\paragraph{Model}
    			\[
    				y_i = \sum_{p=0}^k {\beta_p x_{i}^p} + u_i
    			\]
    		\begin{remark}
    			Consider the \textbf{interpretation} and \textbf{turning points}.
    		\end{remark}
    	\subsection{Interaction Effects}
    		\par Consider model
    		\[
    			y = \beta_0 + \beta_1 x + \beta_2 z + \beta_3 xz + u
    		\]
    		then 
    		\[
    			\pd{y}{x} = \beta_1 + \beta_3 z
    		\]
    		\begin{enumerate}
    			\item The effects of change of $x$ on $y$ depends on $z$.
    			\item Interpretation: \emph{evaluate} $\pd{y}{x}$ at a $z$ point that we are interested in.
    			\item Use \emph{conventional testing} (t-test) to check if interaction term is significant.
    		\end{enumerate}
    	\subsection{Regression Selection and Adjusted R-square}
    		\par The adjusted R-square, $\overline{R^2}$, incorporates a \emph{penalty} for including more regressors (if insignificant).
    		\[
    			\overline{R^2} = 1 - \frac{(1-R^2)(n-1)}{n-k-1}
    		\]
    		\begin{remark}
    			$\overline{R^2}$ increases when adding new regressor(or a group of regressors) if and only if the $t$ value ($F$) for the individual regression(group of regressors) is more than 1.
    		\end{remark}
    	
    	\subsection{Causal Mechanism}
    	
    	\subsection{Confidence Interval for Prediction}
    		\par Consider a prediction 
    		\[
    			\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \dots \hat{\beta}_k x_k
    		\]
    		Evaluate at an arbitrary data point (not necessarily an observation in sample)
    		\[
    			\textbf{c} = (c_1, c_2, \dots ,c_k)
    		\]
    		Then the estimation of $y$ at \textbf{c} is 
    		\begin{gather*}
    			\theta_0 = \expect{y|x_1=c_1, x_2=c_2, \dots x_k=c_k} \\
    			= \beta_0 + \beta_1 c_1 + \beta_2 c_2 + \dots + \beta_k c_k \\
    			\implies \beta_0 = \theta_0 - \beta_1 c_1 - \beta_2 c_2 - \dots - \beta_k c_k
    		\end{gather*}
    		substitute back into the model
    		\[
    			y = \theta_0 + \beta_1 (x_1 - c_1) + \beta_2 (x_2 - c_2) + \dots + \beta_k x_k + u
    		\]
    		And the \ul{margin of error} of confidence interval of prediction of $y$ at \textbf{c} can be found by inspecting the \ul{intercept} on above regression.
    		\[
    			ME = t_{\frac{\alpha}{2}} \times s.e. (intercept)
    		\]
      		The \ul{center} of confidence interval can be found from 
    		\[
    			\hat{\theta}_0 = \hat{\beta}_0 + \hat{\beta}_1 c_1 + \dots + \hat{\beta}_k x_k
      		\]
      		The $\alpha$ confidence interval is given by
      		\[
      			\hat{\theta}_0 \pm ME
      		\]
      		
	\section{Slide 10: Multiple Regression - Qualitative Information}
		\subsection{Binary predictors}
			\begin{remark}
				With binary independent variables, $MLR.1 \sim MLR.6$ still holds, but the interpretations are different.
			\end{remark}
			
			\subsubsection{On Intercept}
			\[
				y = \delta_0 + \delta_1 male + \dots + u
			\]
			\begin{remark}
				To avoid perfect multi-collinearity, never include all categories.
			\end{remark}
			
			\subsubsection{On Slopes}
			\[
				y = \delta_0 + (\delta_1 + \delta_2 male)\times education + \dots + u
			\]
			
			\subsubsection{F-test(Chow test)}
			\par Test whether the \ul{true coefficients} in 2 linear regression models (e.g. for different gender groups) are equal. 
			\begin{enumerate}
				\item Restricted model ($SSR_r$) \[y = \beta_0 + \beta_1 x + u \]
				\item Unrestricted model ($SSR_{ur}$) \[y = (\beta_0 + \delta_0 indicator) + (\beta_1 + \delta_1 indicator) x + u\]
				\item Test whether the additional factors in coefficients ($\delta_0, \delta_1$) are significant. ($q=2$ in this case)
				\[
					F = \frac{(SSR_{r} - SSR_{ur})/q}{SSR_{ur}/(n-k-1)}
				\]
			\end{enumerate}
		\subsection{Linear Probability Model}
			\emph{Qualitative binary dependent variable}
			\[
				y = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k + u,\ y \in \{0, 1\}
			\]
			\paragraph{Interpretation} the model above predicts the probability of $y=1$.
				\begin{proof}
					\begin{gather*}
						\expect{y|\textbf{x}} = 0 \times Pr(y=0|\textbf{x}) + 1 \times Pr(y=1|\textbf{x}) \\
						= Pr(y=1|\textbf{x})
					\end{gather*}
				\end{proof}
			\begin{remark}
				$\beta_j = \pd{P(\textbf{x})}{x_j}$ is the \textbf{response probability}, and $\hat{P}(\textbf{x})$ is the \textbf{predicted probability} of $y$ to be 1.
			\end{remark}
			\begin{remark}[Out-of-range predictions]
				Notice the prediction is not necessarily with the range of $[0,1]$ for some extreme values of \textbf{x}.
			\end{remark}
		\subsection{Heterskedasticity of LPM}
			\begin{remark}
				For probability linear models, $MLR.5$(homoskedasticity) fails.
			\end{remark}
			\begin{proof}
				\begin{gather*}
					y_i = \beta_0 + \beta_1 x_{i1} + \dots \beta_k x_{ik} + u_i \\
					\tx{For binary $y$} \\
					\textcolor{red}{
						Var(u) = Var(y) = Pr(y=1) (1-Pr(y=1))
					} \\
					Var(u|\textbf{x}) = Var(y - \beta_0 - \beta_1 x_1 - \beta_2 x_2 - \dots - \beta_k x_k | \textbf{x}) \\
					= Var(y | \textbf{x}) \\
					= Pr(y=1|\textbf{x}) (1-Pr(y=1|\textbf{x})) \\
					= \expect{y | \textbf{x}} (1 - \expect{y | \textbf{x}}) \\
					= (\beta_0 + \beta_1 x_1 + \dots + \beta_k x_k) (1 - \beta_0 - \beta_1 x_1 - \dots - \beta_k x_k) \\
					\neq \sigma_u^2
				\end{gather*}
			\end{proof}
	\section{Slide 11: Heteroskedasticity}
		\begin{definition}
			Consider model 
			\[
				y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + u_i
			\]
			the error of above model is heteroskedastic if for each sample point $\textbf{x}_i \in \R^{k+1}$, 
			\[
				Var(u_i | \textbf{x}_i) = \sigma_i^2
			\]
			and $\sigma_i^2$ is not the same for all $i$.
		\end{definition}
		\begin{remark}[Consequence]
			Without $MLR.5$, Gauss-Markov theorem does not hold and 
			\begin{enumerate}
				\item OLS estimator is still \ul{linear} and \ul{unbiased}.
				\item But \textbf{not} necessarily the best (variance is affected).
			\end{enumerate}
		\end{remark}
		\begin{proof}[Proof. unbiasedness, in simple regression]
			\begin{gather*}
				\hat{\beta}_1 = \frac{\sum_i(x_i - \overline{x})(y_i - \overline{y})}{\sum_i(x_i - \overline{x})^2} \\
				= \frac{\sum_i(x_i - \overline{x})(\beta_0 + \beta_1 x_1 + u_i - \overline{y})}{\sum_i(x_i - \overline{x})^2} \\
				= \frac{\sum_i(x_i - \overline{x})(\beta_0 + \beta_1 x_1 + \beta_1 \overline{x} - \beta_1 \overline{x} + u_i - \overline{y})}{\sum_i(x_i - \overline{x})^2} \\
				= \frac{\sum_i \beta_1 (x_i - \overline{x})^2 + (x_i - \overline{x}) (\beta_0 + \beta_1 \overline{x} - \overline{y} + u_i)}{\sum_i(x_i - \overline{x})^2} \\
				= \beta_1 + \frac{\sum_i (x_i - \overline{x}) (0 + u_i)}{\sum_i (x_i - \overline{x})^2} \\
				= \beta_1 + \frac{\sum_i (x_i - \overline{x})u_i}{\sum_i (x_i - \overline{x})^2} \\
				\tx{taking expectation conditional on \textbf{x} on both sides} \\
				\expect{\hat{\beta}_1|\textbf{x}} = \beta_1 
			\end{gather*}
		\end{proof}
		
		\begin{proof}[Proof. variance]
			\begin{gather*}
				Var(\hat{\beta}_1 | \textbf{x}) = \expect{(\hat{\beta} - \expect{\hat{\beta}_1 | \textbf{x}})^2 | \textbf{x}} \\
				= \expect{(\hat{\beta}_1 - \beta_1)^2 | \textbf{x}} \\
				= \expect{(\frac{\sum_i (x_i - \overline{x})u_i}{\sum_i (x_i - \overline{x})^2})^2|\textbf{x}} \\
				= \frac{\sum_i(x_i - \overline{x}) \expect{u_i | \textbf{x}}}{\Big( \sum_i (x_i - \overline{x})^2 \Big)^2} \\
				\neq \frac{\sigma^2}{SST_x}
			\end{gather*}
			For multiple regressions
			\begin{gather*}
				Var(\hat{\beta}_j | \textbf{x}) = \frac{\sum_i{\tilde{r}_{ij}^2 \sigma_i^2}}{SSR_j^{\textcolor{red}{2}}} \neq \frac{\sigma^2}{SSR_j} = \frac{\sigma}{(1-R^2_j)SST_j}
			\end{gather*}
		\end{proof}
		
		\paragraph{Remedies}
		\begin{enumerate}
			\item Change variables so that the new model is homoskedastic.
			\item Use robust standard errors.
			\item Generalized least square (GLS).
		\end{enumerate}
		
		\subsection{Robust Standard Errors}
			\paragraph{Idea} use $\hat{u}_i^2$ to estimate $\sigma_i^2$.\\
			Note that  
			\begin{gather*}
				Var(u_i | \textbf{x}) = \expect{(u_i - \expect{u_i})^2} \\
				= \expect{u_i^2 | \textbf{x}} + \expect{u_i | \textbf{x}}^2 \\
				= \expect{u_i^2 | \textbf{x}}
			\end{gather*}
			Consider model 
			\[
				y_i = \beta_0 + \beta_1 x_i + u_i
			\]
			OLS estimator is 
			\begin{gather*}
				\hat{\beta}_1 = \beta_1 + \frac{\sum_i (x_i - \overline{x})u_i}{\sum_i (x_i - \overline{x})^2} \\
				Var(\hat{\beta} | \textbf{x}) = \frac{\sum_i(x_i - \overline{x})^2 \sigma_i^2}{\sum_i(x_i - \overline{x})^2} \\
				\widehat{Var}(\hat{\beta} | \textbf{x}) = \frac{\sum_i(x_i - \overline{x})^2 \textcolor{red}{\hat{u}_i^2}}{\sum_i(x_i - \overline{x})^2}
			\end{gather*}
		\subsection{Test for Heteroskedasticity}
		\subsubsection{General Principle}
			\begin{gather*}
				H_0: \expect{u_i^2} = Var(u_i|\textbf{x}) = \sigma^2 \tx{ (Homoskedastic)} \\
				H_1: \expect{u_i^2} = Var(u_i|\textbf{x}) = \sigma_i^2 \tx{ (Heteroskedastic)}
			\end{gather*}
			\textbf{Methodology:} specify the variance in alternative hypothesis to be a specific function of $\textbf{x}$ or $y$. \\
			Consider the model: 
			\[
				y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + u_i
			\]
			And $H_1$ can be expressed as 
			\[
				H_1: \expect{u_i^2|\textbf{x}} = \delta_0 + \delta_1 z_1 + \delta_2 z_2 + \dots + \delta_p z_p
			\]
			then run the proxy hypothesis testing
			\begin{gather*}
				H_0': \delta_1 = \delta_2 = \dots = \delta_p = 0, \delta_0 = \sigma^2 \\
				H_1': \exists j\ s.t.\ \delta_j \neq 0
			\end{gather*}
			Note that the restricted model is homoskedastic. \\
			Firstly run the original regression model and get residual $\hat{u}_i$. \\
			Then test the proxy hypotheses with regression $\hat{u}_i^2$ on $z_1, z_2, \dots, z_p$ using full F-test.
			\begin{gather*}
				F = \frac{R^2_{\hat{u}^2}/p}{(1-R^2_{\hat{u}^2})/(n-p-1)} \sim F_{(p, n-p-1)} \\
				 \tx{and }n R^2_{\hat{u}^2} \sim \mc{X}_p^2
			\end{gather*}
			
			\subsubsection{Breusch-Pagan test}
				\par Use regressors $x_i$ for $z_i$. \\
				Auxiliary regression:
				\begin{gather*}
					\hat{u}_i^2 = \delta_0 + \delta_1 x_1 + \dots \delta_k x_k \\
					n R^2_{\hat{u}^2} \sim \mc{X}_k^2
				\end{gather*}
			\subsubsection{White test version 1}
				\par Use polynomials of $x_i$ for $z_i$.\\
				Auxiliary regression: (for the case of 2 regressors)
				\begin{gather*}
					\hat{u}_i^2 = \delta_0 + \delta_{i1} x_1 + \delta_2 x_{i2} + \delta_3 x_{i1}^2 + \delta_4 x_{i2}^2 + \delta_5 x_{i1} x_{i2} + \epsilon \\
					n R^2_{\hat{u}^2} \sim \mc{X}_5^2 \\
					\tx{or full F-test}
				\end{gather*}
			\subsubsection{White test version 2}
				\par Use \ul{predicted} response $\hat{y}$ (since its a linear combination of predictors) and its polynomial as $z_i$. \\
				Auxiliary regression:
				\begin{gather*}
					\hat{u}_i^2 = \delta_0 + \delta_1 \hat{y} + \delta_2 \hat{y}^2 + \epsilon \\
				\end{gather*}
				With hypotheses
				\begin{gather*}
					H_0: \delta_1 = \delta_2 = 0\\
					H_1: \delta_1 \neq 0 \lor \delta_2 \neq 0
				\end{gather*}
				\begin{gather*}
					n R^2_{\hat{u}^2} \sim \mc{X}_2^2 \\
					\tx{or full F-test}
				\end{gather*}
\end{document}





























