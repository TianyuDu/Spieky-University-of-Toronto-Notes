\documentclass[]{article}
\title{ECO375 Applied Econometrics I \\ \small Lecture Slide Notes}
\author{Tianyu Du}
\date{\today}

\usepackage{spikey}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{bm}


\counterwithin{equation}{section}


\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}

\begin{document}
	\maketitle
	\doclicenseThis
	\textbf{Updated} version can be found on \url{www.tianyudu.com/notes}
	
	\tableofcontents
	
	\newpage
	
	\setcounter{section}{3}
	\section{Slide 4: Simple \& Multiple Regression - Estimation}
	\subsection{Regression Model}
	\begin{assumption}
	    Assuming the population follows
	    \[
	        y = \beta_0 + \beta_1 x + u
	    \]
	    and assume that \emph{$x$ causes y}.
	\end{assumption}
	
	\subsection{OLS}
	\begin{gather*}
	    \min_{\vec{\beta}} \sum_{i} (y_i - \hat{y}_i)^2 \\
	    \tx{With FOC:} \\
	    \sum_{i}(y_i - \hat{y}_i) = 0 \\
	    \sum_{i}x_{ij}(y_i - \hat{y}_i) = 0,\ \forall j
	\end{gather*}
	
	\begin{remark}
	    Both $\hat{\beta}_0$ and $\hat{\beta}_j$ are functions of \emph{random variables} and therefore themselves \emph{random} with \emph{sampling distribution}. And the estimated coefficients are random up to random sample chosen.
	\end{remark}
	
	\begin{proposition}
	    Properties of OLS estimators
	    \begin{itemize}
	        \item \textbf{Unbiased} $\mathbb{E}[\hat{\beta} | X] = \beta$
	        \item \textbf{Consistent} $\hat{\beta} \to \beta$ as $n \to \infty$
	        \item \textbf{Efficient} min variance.
	    \end{itemize}
	\end{proposition}
	
	\begin{definition}
	    The \textbf{Simple Coefficient of Determination}
	    \[
	        R^2 = \frac{SSE}{SST}
	    \]
	    and $SS\underline{Total} = SS\underline{Explained} + SS\underline{Residual}$
	    \[
	        \sum_i {(y_i - \overline{y})^2} = \sum_i {(\hat{y}_i - \overline{y})^2} + \sum_i {(y_i - \hat{y}_i)^2}
	    \]
	\end{definition}
	
	\begin{proposition}[Logarithms] Interpretation with logarithmic transformation.
	    \begin{itemize}
	        \item $\ln{y} = \alpha + \beta \ln{y} + u$: \ul{$x$ increases by $1\%$, $y$ increases by $\beta \%$}.
	        \item $\ln{y} = \alpha + \beta x + u$: \ul{$x$ increases by 1 unit, $y$ increases by $100 \beta \%$}.
	        \item $y = \alpha + \beta \ln{x} + u$: \ul{$x$ increases by $1\%$, $y$ increases by $0.01\beta$ unit.}
	    \end{itemize}
	\end{proposition}
	
	\begin{assumption}(\hl{SLR})
	    Simple regression model assumptions
	    \begin{enumerate}
	        \item Model is \ul{linear} in parameter.
	        \item \ul{Random samples} $\{(x_i, y_i)\}_{i=1}^n$.
	        \item Sample outcomes $\{x_i\}_{i=1}^n$ are not the same.\footnote{Otherwise, $SST=0$.}
	        \item $\expect{u|x}=0$ conditional on random sample $x$.
	        \item Error is \ul{homoskedastic}. $Var(u|x) = \sigma^2$ for all $x$.
	    \end{enumerate}
	\end{assumption}
	\paragraph{Benefits of MLR compared with SLR}
	    \begin{itemize}
	        \item More accurate causal effect estimation.
	        \item More flexible function forms.
	        \item Could explicitly include more predictors so $\mathbb{E}(u|\textbf{x}) = 0$ is easier to be satisfied. \footnote{If we suspect some predictors may interact with certain component in $u$, moving the portion of $u$ to the predictor set solves the problem.}
	        \item MLR4 is less restrictive than SLR4.
	    \end{itemize}
	   
    \begin{proposition}
        MLR OLS residual satisfies
        \begin{gather*}
            \sum_i {\hat{u}_i} = 0\\
            \sum_i {x_{ji} \hat{u}_i} = 0,\ \forall i \in \{1, 2, \dots, k\}
        \end{gather*}
    \end{proposition}
    
    \begin{proposition}
        MLR OLS estimators $\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_k$ pass through the average point.
        \[
            \overline{y} = \hat{\beta}_0 + \hat{\beta}_1 \overline{x}_1 + \dots + \hat{\beta}_k \overline{x}_k
        \]
    \end{proposition}
    \begin{proof}
    	\begin{gather*}
    		\sum_i {\hat{u}_i} = 0 \\
    		\implies \sum_i {\hat{y}_i - y_i} = 0 \\
    		\implies \sum_i {\hat{\beta}_0 + \hat{\beta}_1 x_{1i} + \dots + \hat{\beta}_k x_{ki} - y_i} = 0 \\
    		\implies \hat{\beta}_0 + \hat{\beta}_1 \overline{x}_1 + \dots + \hat{\beta}_k \overline{x}_k = \overline{y}
    	\end{gather*}
    \end{proof}
    
    \subsection{Partialling Out}
    \subsubsection{Steps}
        \begin{enumerate}
            \item Regress $x_1$ on $x_2, x_3, \dots, x_K$ and calculate the residual $\widetilde{r}_1$. 
            \item Regress $y$ on $\widetilde{r}_1$ with simple regression and find the estimated coefficient $\hat{\lambda}_1$.
            \item Then the multiple regression coefficient estimator $\hat{\beta}_1$ is
            \[
                \hat{\beta}_1 = \hat{\lambda}_1 = \frac{\sum_{i}{y_i \widetilde{r}_{1i}}}{\sum_i {(\widetilde{r}_{1i})^2}}
            \]
        \end{enumerate}
        \begin{proof}
        	By the first order condition of OLS,
        	\begin{gather*}
        		\sum x_1 \hat{u} = 0 \\
        		\implies \sum (\hat{x}_1 + \tilde{r}_1) \hat{u} = 0 \\
        		\implies \sum \tilde{r}_1 \hat{u} = 0 \\
        		\implies \sum \tilde{r}_1 (y - \hat{y}) = 0 \\
        		\implies \sum \tilde{r}_1 (y - \hat{\beta}_0 - \hat{\beta}_1 x_1 - \hat{\beta}_2 x_2 - \dots - \hat{\beta}_k x_k) = 0 \\
        		\implies \sum \tilde{r}_1 y = \hat{\beta}_1 \sum \tilde{r}_1 x_1 \\
        		\implies \sum \tilde{r}_1 y = \hat{\beta}_1 \sum \tilde{r}_1 (\hat{x}_1 + \tilde{r}_1) = \hat{\beta}_1 \sum \tilde{r_1}^2\\
        		\implies \hat{\beta}_1 = \frac{\sum \tilde{r} y}{\sum \tilde{r}^2}
        	\end{gather*}
        \end{proof}
        
    \subsubsection{Interpretation}
    \par This OLS estimator only uses the \hl{unique variance} of one independent variable. And the parts of variation correlated with other independent variables is partialled out.
    
    \begin{assumption}(\hl{MLR}) Multiple Regression Assumptions
        \begin{enumerate}
            \item (MLR.1) The model is \hl{linear} in parameters. 
            \item (MLR.2) \hl{Random sample} from population $\{(x_{1i}, \dots x_{ki}, y_i\}_{i=1}^n$.
            \item (MLR.3) No perfect \hl{multicollinearity}.
            \item (MLR.4) \hl{Zero expected error} conditional on population slice given by $X$.
            \[
                \mathbb{E}(u|\textbf{x}) \equiv \mathbb{E}(u|x_1, x_2, \dots, x_k) = 0
            \]
            \item (MLR.5) \hl{Homoskedastic error} conditional on population slice given by $X$.
            \[
                Var(u|\textbf{x}) = \sigma^2
            \]
            \item (MLR.6, \emph{strict assumption}) \ul{Normally distributed error}
            \[
                u \sim \mc{N}(0, \sigma^2)
            \]
        \end{enumerate}
    \end{assumption}
    
    \subsection{Omitted Variable Bias}
    \par Suppose population follows the \emph{real model} 
    \begin{equation}
        y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_k x_{ki} + u_i
    \end{equation}
    Consider the \emph{alternative model}, and \ul{$x_k$ is omitted}, which is assumed to be relevant.
    \begin{equation}
        y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_{k-1} x_{(k-1)i} + r_i
    \end{equation}
    and use the partialling-out result on the second regression we have
    \[
        \tilde{\beta}_1 = \frac{\sum_i \tilde{r}_{1i} y_i}{\sum_i(\tilde{r}_{1i})^2}
    \]
    where 
    \[
    	\tilde{r}_{1i} = x_{1i} - \tilde{\alpha}_0 - \tilde{\alpha}_2 x_{2i} - \dots - \tilde{\alpha}_{\textcolor{red}{k-1}}x_{\textcolor{red}{(k-1)}i}
    \]
    and
    \begin{equation}
        \tilde{\beta}_1 = \hat{\beta}_1 + \hat{\beta}_k \frac{\sum_i (\tilde{r}_{1i} x_{ki})}{\sum_i (\tilde{r}_{1i})^2}
    \end{equation}
    and take the expectation 
    \begin{gather*}
        \mathbb{E}(\tilde{\beta}_1 | X) = \beta_1 + \tilde{\delta}_1 \beta_k \\
        Bias(\tilde{\beta}_1) = \tilde{\delta}_1 \beta_k
    \end{gather*}
    \paragraph{Conclusion} the \ul{sign} of bias depends on $Cov(x_1, x_k)$ and $\beta_k$.
    
    \begin{proof}
    	\textcolor{red}{TODO}
    \end{proof}
    
    \section{Slide 5: Matrix Algebra for Regression Analysis}
    	\begin{equation}
    		\textbf{y} = \textbf{A} \textbf{x} \implies \pd{\textbf{y}}{\textbf{x}} = \textbf{A}
    	\end{equation}
    	Let $\alpha = \textbf{y}' \textbf{A} \textbf{x}$, notice that $\alpha \in \R$, then 
    	\begin{gather}
    		\pd{\alpha}{\textbf{x}} = \textbf{y}' \textbf{A} \\
    		\pd{	\alpha}{\textbf{y}} = \textbf{x}' \textbf{A}'
    	\end{gather}
    	Consider special case $\alpha = \textbf{x}' \textbf{A} \textbf{x}$, then 
    	\begin{equation}
    		\pd{\alpha}{\textbf{x}} = \textbf{x}' \textbf{A} + \textbf{x}' \textbf{A}'
    	\end{equation}
    	and if $\textbf{A}$ is symmetric, 
    	\begin{equation}
    		\pd{\alpha}{\textbf{x}} = 2 \textbf{x}' \textbf{A}
    	\end{equation}
    
    \section{Slide 6: Multiple Regression in Matrix Algebra}
    	\subsection{The Model}
    		\paragraph{Independent Variable Matrix}
    			\[
    				\textbf{X} \in \mathbb{M}_{n \times (k+1)}(\R)
    			\]
    			where $n$ is the number of observations and $k$ is the number of features.
    			\[
    				\textbf{X} = \begin{bmatrix}
    					1 & x_{11} & \dots & x_{1k} \\
    					1 & x_{21} & \dots & x_{2k} \\
    					\vdots \\
    					1 & x_{n1} & \dots & x_{nk} \\
    				\end{bmatrix}_{n \times (k+1)}
    			\]
    		\paragraph{Model}
    			\[
    				\textbf{y} = \textbf{X} \vec{\beta} + \textbf{u}
    			\]
    		\paragraph{First order condition for OLS}
    			\begin{gather*}
%    				\textbf{X}' \hat{u} = \textbf{0} \in \R^{k+1} \\
%    				\iff \textbf{X}' (\textbf{y} - \textbf{X} \hat{\beta}) = \textbf{0} \in \R^{k+1}
					\pd{(\textbf{y} - \textbf{X}\vec{\beta})^2}{\vec{\beta}} = \textbf{0}
					\iff \textbf{X}' (\textbf{y} - \textbf{X}\vec{\beta}) = \textbf{0}
    			\end{gather*}
    		\paragraph{Estimator}	
    			\[
    				\hat{\beta} = (\textbf{X}' \textbf{X})^{-1} \textbf{X}' \textbf{y}
    			\]
    			\begin{proof}
    				From the first order condition for the OLS estimator
    				\begin{gather*}
    					\textbf{X}' (\textbf{y} - \textbf{X} \hat{\beta}) = \textbf{0} \\
    					\implies \textbf{X}' \textbf{y} - \textbf{X}' \textbf{X} \hat{\beta} = \textbf{0} \\
    					\implies \textbf{X}' \textbf{y} = \textbf{X}' \textbf{X} \hat{\beta} \\
    					\implies \hat{\beta} = (\textbf{X}' \textbf{X})^{-1} \textbf{X}' \textbf{y}
    				\end{gather*}
    				and
    				\begin{remark}
	    				note that $(\textbf{X}' \textbf{X})$ is guaranteed to be invertible by assumption \emph{no perfect multi-collinearity} and the implicit assumption that the number of features $k$ is sufficiently greater than the number of observations $n$. i.e. $k >> n$.
    				\end{remark}
    			\end{proof}
    		\paragraph{Sum Squared Residual}
    		\[
    			SSR(\hat{\beta}) = ||\hat{u}||^2 = ||\textbf{y} - \textbf{X} \hat{\beta}||^2
    		\]
    		
    		\subsection{Variance Matrix}
    			\par Consider 
    			\begin{gather*}
    				\textbf{z}_t = [z_{1t}, z_{2t}, \dots z_{nt}]' \\
    				\textbf{z}_s = [z_{1s}, z_{2s}, \dots z_{ns}]'
    			\end{gather*}
    			Notice that the variance and covariance are defined as
    			\begin{gather*}
    				Var(\vec{z}_t) = \expect{(\vec{z}_t - \expect{\vec{z}_t})^2} \\
    				Cov(\vec{z}_t, \vec{z}_s) = \expect{(\vec{z}_t - \expect{\vec{z}_t})(\vec{z}_s - \expect{\vec{z}_s})}
    			\end{gather*}
    			The \textbf{variance matrix} of $\textbf{z} = [z_1, z_2, \dots, z_n]$ is given by
    			\begin{gather*}
    				Var(\textbf{z}) = \begin{bmatrix}
    					Var(z_1) & Cov(z_1, z_2) & \dots & Cov(z_1, z_n) \\
    					Cov(z_2, z_1) & \dots \\
    					\vdots \\
    					Cov(z_n, z_1) & \dots & \dots & Var(z_n) \\
    				\end{bmatrix} \\
    				= 
    				\begin{bmatrix}
    					\expect{(z_1 - \overline{z}_1)^2} & \expect{(z_1 - \overline{z}_1)(z_2 - \overline{z}_2)} & \dots \\
    					\expect{(z_2 - \overline{z}_2)(z_1 - \overline{z}_1)} & \dots \\
    					\vdots \\
    					\expect{(z_n - \overline{z}_n)(z_1 - \overline{z}_1)} & \dots & \expect{(z_n - \overline{z}_n)^2} \\
    				\end{bmatrix} \\
    				= \textcolor{red}{
    				\expect{(\textbf{z} - \expect{\textbf{z}})_{n \times 1} \cdot (\textbf{z} - \expect{\textbf{z}})'_{1 \times n}} \in \mathbb{M}_{n \times n}
    				}
    			\end{gather*}
    			In the special case $\expect{\textbf{z}} = \textbf{0}$, variance is reduced to
    			\[
    				\textcolor{red}{
    					Var(\textbf{z}|\textbf{X}) = \expect{\textbf{z} \cdot \textbf{z}'|\textbf{X}}
    				}
    			\]
    			\paragraph{Residual} Since residual $u_i$ are \emph{i.i.d} with variance $\sigma^2$, the variance matrix of $\textbf{u}$ is
    			\[\textcolor{red}{
    				Var(\textbf{u}|\textbf{X}) = \expect{\textbf{u} \cdot \textbf{u}'|\textbf{X}} = \sigma^2 \textbf{I}_n
    			}\]
    			
    			\paragraph{Estimator} If $\hat{\beta}$ is unbiased, $\expect{\hat{\beta} | \textbf{X}} = \vec{\beta}$, then 
    			\[\textcolor{red}{
    				Var(\hat{\beta}|\textbf{X}) = \expect{(\hat{\beta} - \vec{\beta})\cdot(\hat{\beta} - \vec{\beta})' | \textbf{X}} \in \mathbb{M}_{(k+1) \times (k+1)}
    			}\]
    \section{Slide 7: Multiple Regression - Properties}
    	\subsection{Assumptions (MLRs) in Matrix Form}
    		\paragraph{E.0.} \hl{Random sample} from population.
    		\paragraph{E.1.} \hl{Linear in parameter}
    		\[
    			\textbf{y} = \textbf{X}\vec{\beta} + \textbf{u}
    		\]
    		\paragraph{E.2.} \hl{No perfect multi-collinearity}
    		\[
    			rank(\textbf{X}) = k +1
    		\]
    		\paragraph{E.3.} Error has expected value of \textbf{0} conditional on \textbf{X} and $\textbf{X}$ is \hl{orthogonal} to residual $\textbf{u}$.
    		\[
    			\expect{\textbf{u}|\textbf{X}} = \textbf{0}
    		\]
    		\paragraph{E.4.} Error \textbf{u} is \hl{homoskedastic}.
    		\[
    			Var(\textbf{u}|\textbf{X}) = \sigma^2 \textbf{I}_n
    		\]
    		\paragraph{E.5.} \hl{Normally distributed} error \textbf{u}. Note that this assumption is relatively strong.
    		\[
    			\textbf{u} \sim \mc{N}(\textbf{0}, \sigma^2 \textbf{I}_n)
    		\]
    	\subsection{Properties of OLS Estimator}
    	\begin{theorem}
    		Given \emph{E.1. E.2. E.3.}, the OLS estimator $\hat{\beta}$ is an unbiased estimator for $\vec{\beta}$.
    		\[
    			\expect{\hat{\beta}|\textbf{X}} = \vec{\beta}
    		\]
    	\end{theorem}
    	\begin{proof}
    		\begin{gather*}
    			\hat{\beta} = (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{y} \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' (\textbf{X}\vec{\beta} + \textbf{u}) \\
    			= \vec{\beta} + (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{u} \\
    			\tx{Taking expectation conditional on \textbf{X} on both sides, }\\
    			\expect{\hat{\beta}|\textbf{X}} = \vec{\beta} + (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{0} 
    			= \vec{\beta}
    		\end{gather*}
    	\end{proof}
    	
    	\begin{lemma}
    		Suppose $\textbf{A} \in \mathbb{M}_{m\times n}$ and $\textbf{z} \in \mathbb{M}_{n\times 1}$ then 
    		\[
    			\textcolor{red}{
    				Var(\textbf{A} \textbf{z}) = \textbf{A} Var(\textbf{z}) \textbf{A}'
    			}
    		\]
    	\end{lemma}
    	\begin{theorem}
    		Given $E.1 \sim E.4$
    		\[
    			\textcolor{red}{
    				Var(\hat{\beta}|\textbf{X}) = (\textbf{X}' \textbf{X})^{-1} \sigma^2
    			}
    		\]
    	\end{theorem}
    	\begin{proof}
    		\begin{gather*}
    			Var(\hat{\beta}|\textbf{X}) = Var((\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{y} | \textbf{X}) \\ 
    			= Var((\textbf{X}'\textbf{X})^{-1} \textbf{X}'(\textbf{X}\vec{\beta} + \textbf{u})|\textbf{X}) \\
    			= Var(\textbf{X}'\textbf{X})^{-1} \textbf{X}'\textbf{u}|\textbf{X}) \\
    			\tx{By the lemma above, } \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' Var(\textbf{u}|\textbf{X}) [(\textbf{X}'\textbf{X})^{-1} \textbf{X}']' \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' Var(\textbf{u}|\textbf{X}) \textbf{X}'' (\textbf{X}'\textbf{X})^{-1} \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \sigma^2 \textbf{I}_n \textbf{X} (\textbf{X}'\textbf{X})^{-1} \\
    			= \sigma^2 (\textbf{X}'\textbf{X})^{-1}
    		\end{gather*}
    	\end{proof}
    	
    	\begin{theorem}[\hl{Gauss-Markov Theorem}]
    		Given $E.1. \sim E.4.$, i.e.
    			\begin{enumerate}
    				\item Models is linear in parameters.
    				\item No perfect multi-collinearity presents.
    				\item Error has expected value of zero conditional on \textbf{X}.
    				\item Homoskedastic.
    			\end{enumerate}
    		the OLS estimator is the \ul{best linear unbiased estimator} (BLUE). \\(\emph{The best} here refers to the OLS has the least variance among all estimators.)
    	\end{theorem}
    	
    	\subsection{Variance Inflation}
    	\par Let $j \in \{1, 2, \dots, k\}$, then the variance of an individual estimator on particular feature $j$ is 
    	\[\textcolor{red}{
    		Var(\hat{\beta}_j) = \frac{\sigma^2}{(1-R_j^2)SST_j}
    	}\]
    	where
    	\begin{gather*}
    		SST_j = \sum_{i=1}^n{(x_{ij} - \overline{x}_j)^2}
    	\end{gather*}
    	and $R_j^2$ is the coefficient of determination while regressing $x_j$ on \ul{all other} features $x_i, \forall i \neq j$.
    	\begin{definition}
    		The \textbf{variance inflation} on estimator for feature $j$ is 
    		\[
    			VIF_j = \frac{1}{1-R_j^2}
    		\]
    	\end{definition}
    	\begin{remark}[Interpretation] the standard error of estimator on a particular variable ($\hat{\beta}_j$) is \emph{inflated} by it's($x_j$) relationship with other explanatory variables. \\
    		If a predictor is highly correlated with other predictors, it's estimated coefficient will be inefficient (i.e. with high variance/uncertainty)
    	\end{remark}	

    	\paragraph{Solutions to high VIF}
    		\begin{enumerate}
    			\item Drop the highly inflated explanatory variable.
    			\item Use ratio $\frac{x_i}{x_j}$ instead.
    			\item Ridge regression.
    		\end{enumerate}
    	\begin{remark}[Interpretation]
    		VIF highlights the importance of \textbf{not} including redundant predictors.
    	\end{remark}
    	
    \section{Slide 8: Multiple Regression - Inference}
    \paragraph{Hypothesis Testing} on multiple regression model 
    	\[
    		y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots \beta_k x_{ik} + u_i
    	\]
    	
    	\subsection{t-test for significance of individual predicator}
    		\paragraph{Test statistic} Given $MLR.1 \sim MLR.6$,\\
    		(\hl{requires $\textbf{u} \sim \mc{N}(\textbf{0}, \sigma^2 \textbf{I}_n)$ so that $t$-statistic follows the $t$ distribution}),
    		\[
    			t = \frac{\hat{\beta}_j - b}{s.e.(\hat{\beta}_j)} \sim t_{\textcolor{red}{n-k-1}}
    		\]
    		where 
    		\begin{gather*}
    			H_0: \beta_j = b \\
    			H_1: \beta_j (\neq, >, <) b
    		\end{gather*}
    	
    	
    	\subsection{t-test for comparing 2 coefficients}
    	\paragraph{Test statistic}
    	\[
    		t = \frac{(\hat{\beta}_i - \hat{\beta}_j) - b}{s.e.(\hat{\beta}_i - \hat{\beta}_j)} \sim t_{\textcolor{red}{n-k-1}}
    	\]
    	where 
    		\begin{gather*}
    			H_0: \beta_i - \beta_j = b \\
    			H_1: \beta_i - \beta_j (\neq, >, <) b
    		\end{gather*}
    	notice
    	\begin{gather*}
    		s.e.(\hat{\beta}_i - \hat{\beta}_j) = \sqrt{Var(\hat{\beta}_i - \hat{\beta}_j)} \\
    		= \sqrt{Var(\hat{\beta}_i) + Var(\hat{\beta}_j) - 2Cov(\hat{\beta}_i, \hat{\beta}_j)}
    	\end{gather*}
    	
    	\subsection{Partial F-test for joint significance}
    	\begin{gather*}
    		H_0: \beta_i = \beta_j = \beta_k = \dots = 0 \\
    		H_1: \exists\ z \in \{i, j, k, \dots \}\ s.t.\ \beta_z \neq 0
    	\end{gather*}
    	Test significance by comparing the \ul{restricted} and \ul{unrestricted} models, see whether restricting the model by removing certain explanatory variables "significantly" hurts the fit of the model.
    	\[
    		\textcolor{red}{df = (q, n-k-1)}
    	\]
    	\paragraph{Test statistic} Let $SSR$ denote the regression residual,
    	\begin{gather*}
    		F = \frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)} \sim F_{(q, n-k-1)} \\
    		\tx{or} \\
    		F' = \frac{(R^2_{ur} - R^2_r)/q}{(1-R^2_{ur})/(n-k-1)} \sim F_{(q, n-k-1)}
    	\end{gather*}
    	
    	\subsection{Full F-test for the significance of the model}
    	\begin{gather*}
    		H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0 \\
    		H_1: \exists\ i \in \{1, 2, \dots ,k\}\ s.t.\ \beta_i \neq 0
    	\end{gather*}
    	\begin{remark}
    		\hl{$R^2$ version only} and substitute $R^2_{r} = 0$ (restricted model explains nothing), since $SSR_{r}$ is undefined.
    	\end{remark}
    	\paragraph{Test statistic}
    	\[
    		F = \frac{R^2_{ur}/k}{(1-R^2_{ur})/(n-k-1)} \sim F_{(k, n-k-1)}
    	\]
    	
    	\subsection{F-test for general restrictions}
    	\begin{gather*}
    		H_0: \beta_1 + \beta_2 = 1 \\
    		H_1: \neg H_0
    	\end{gather*}
    	\paragraph{Procedure}
    		\begin{enumerate}
    			\item Substitute the restriction in $H_0$ into the original model to derive the restricted model.
    			\item Estimate both the original and restricted models and calculate their $SSR$.
    		\end{enumerate}
    		Test hypothesis with $F-statistic$
    		\[
    			F = \frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)}
    		\]
    		where $q$ denotes the number of restrictions (equations) in $H_0$.
    	\begin{remark}
    		\hl{Use the $SSR$ version only} of $F-statistic$ only since the $SST$ for restricted and unrestricted models are different.
    	\end{remark}
    	\begin{remark}
    		We only reject or failed to reject $H_0$, we never accept $H_0$ in a hypothesis test.
    	\end{remark}
    	
    \section{Slide 9: Multiple Regression - Further Issues}
    	\subsection{Data Scaling}
    		\subsubsection{Multiplier}
    		\begin{enumerate}
    			\item Enlarge $x_j$ by factor $a$: $\hat{\beta}_j$ shrinks by $a$.
    			\item Enlarge $y$ by factor $a$: \textbf{all} $\hat{\beta}_i$ enlarged by $a$.
    			\item \hl{Test statistic $t = \frac{\hat{\beta}}{s.e.(\hat{\beta})} = \frac{a\hat{\beta}}{s.e.(a \hat{\beta})}$ is unaffected.}
    		\end{enumerate}
    	
    		\subsubsection{Standardization}
    			\paragraph{Standardized variable} For $j^{th}$ observation of explanatory variable $x$, 
    				\[
    					z_j = \frac{x_j - \overline{x}}{\sigma_{x}}
    				\]
    				which satisfies
    				\[
    					\expect{z_j} = 0,\ Var(z_j) = 1
    				\]
    			\paragraph{Beta Coefficients} Consider model and find the estimator of regressing standardized $y$ on standardized $x$.
    				\begin{gather*}
    					y_i = \hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \dots + \hat{\beta}_k x_{ik} + \hat{u}_i \\
    					\land \quad \overline{y} = \hat{\beta}_0 + \hat{\beta}_1 \overline{x}_1 + \dots \hat{\beta}_k \overline{x}_k \\
    					\implies (y_i - \overline{y}) = \hat{\beta}_1 (x_{i1} - \overline{x}_1) + \dots + \hat{\beta}_k (x_{ik} - \overline{x}_k) + \hat{u}_i \\
    					\implies \frac{y_i - \overline{y}}{\sigma_y} = 
    					\frac{\hat{\beta}_1 \sigma_{x_1}}{\sigma_y} \frac{x_{i1} - \overline{x}_1}{\sigma_{x_1}} + \dots + 
    					\frac{\hat{\beta}_k \sigma_{x_k}}{\sigma_y} \frac{x_{ik} - \overline{x}_k}{\sigma_{x_k}} + \frac{\hat{u}_i}{\sigma_y} \\
    					\implies \textcolor{red}{b_j = \frac{\hat{\beta}_j \sigma_{x_j}}{\sigma_y}}
    				\end{gather*}
    			\begin{remark}[\hl{Unit-free} Interpretation]
    				$x_j$ increases by 1 \textbf{standard deviation}, y increases by $b_j$ \textbf{standard deviation}.
    			\end{remark}
    	\subsection{Regression Functional Forms}
    	\subsubsection{Logarithmic Function}
    		\paragraph{Exact} interpretation of log transformation.
    			\[
    				\widehat{\ln(y_i)} = \hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \dots \hat{\beta}_k x_{ik}
    			\]
    			\begin{proof}
	    			\begin{gather*}
	    				\ln(y_2) - \ln(y_1) = \hat{\beta}_j \Delta x_j \\
	    				\implies \ln(\frac{y_2}{y_1}) = \hat{\beta}_j \Delta x_j \\
	    				\implies \frac{y_2}{y_1} = exp(\hat{\beta}_j \Delta x_j) \\
	    				\implies \frac{y_2 - y_1}{y_1} 
	    				= \frac{y_2}{y_1} - 1 \\
	    				\implies \textcolor{red}{\%\Delta y= exp(\hat{\beta}_j \Delta x_j) - 1}
	    			\end{gather*}
    			\end{proof}
    	\subsubsection{Quadratics and Polynomials}
    		\paragraph{Model}
    			\[
    				y_i = \sum_{p=0}^k {\beta_p x_{i}^p} + u_i
    			\]
    		\begin{remark}
    			Consider the \textbf{interpretation} and \textbf{turning points}. The relation between dependent and independent variables varies across slices of population.
    		\end{remark}
    	\subsection{Interaction Effects}
    		\par Consider model
    		\[
    			y = \beta_0 + \beta_1 x + \beta_2 z + \textcolor{orange}{\beta_3 x z} + u
    		\]
    		then 
    		\[
    			\pd{y}{x} = \beta_1 + \textcolor{orange}{\beta_3 z}
    		\]
    		\begin{enumerate}
    			\item The effects of change of $x$ on $y$ depends on $z$.
    			\item Interpretation: \hl{evaluate} $\pd{y}{x}$ at a $z$ value that we are interested in.
    			\item Use \hl{conventional testing} (t-test) to check if interaction term is significant.
    		\end{enumerate}
    	\subsection{Regression Selection and Adjusted R-Square}
    		\par The adjusted R-square, $\overline{R^2}$, incorporates a \emph{penalty} for including more regressors (if insignificant).
    		\[
    			\textcolor{red}{\overline{R^2} = 1 - \frac{(1-R^2)(n-1)}{n-k-1}}
    		\]
    		\begin{remark}[Algebraic Fact]
    			$\overline{R^2}$ increases when adding new regressor(group of regressors) if and only if the $t$-statistic ($F$-statistic) for the individual regression(group of regressors) is more than 1.
    		\end{remark}
    	
    	\subsection{Over Controlling}
    		\begin{example}
    			If we want to access the indirect effect of $x$ on $y$ \ul{through $z$}. Then if we include $z$ into the regressors, the coefficient of $x$ in MLR would only pick up the \ul{indirect effect} of $x$ on $y$ \textbf{not} associated with $z$. (Partialling out)
    		\end{example}
    	
    	\subsection{Confidence Interval for Prediction}
    		\par Consider a prediction 
    		\[
    			\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \dots \hat{\beta}_k x_k
    		\]
    		Evaluate at an arbitrary data point (not necessarily an observation in sample)
    		\[
    			\textbf{c} = (c_1, c_2, \dots ,c_k)
    		\]
    		Then the estimation of $y$ at \textbf{c} is 
    		\begin{gather*}
    			\textcolor{red}{y_c} = \expect{y|x_1=c_1, x_2=c_2, \dots x_k=c_k} \\
    			= \beta_0 + \beta_1 c_1 + \beta_2 c_2 + \dots + \beta_k c_k \\
    			\implies \beta_0 = \textcolor{red}{y(c)} - \beta_1 c_1 - \beta_2 c_2 - \dots - \beta_k c_k
    		\end{gather*}
    		substitute back into the model
    		\[
    			y = \textcolor{red}{y_c} + \beta_1 (x_1 - c_1) + \beta_2 (x_2 - c_2) + \dots + \beta_k x_k + u
    		\]
    		And the \ul{margin of error} of confidence interval of prediction of $y$ at \textbf{c} can be found by inspecting the \ul{intercept} on above regression.
    		\[
    			ME = t_{\frac{\alpha}{2}} \times s.e. (intercept)
    		\]
      		The \ul{center} of confidence interval can be found from 
    		\[
    			\hat{y}_c = \hat{\beta}_0 + \hat{\beta}_1 c_1 + \dots + \hat{\beta}_k x_k
      		\]
      		The $\alpha$ confidence interval is given by
      		\[
      			\hat{y}_c \pm ME
      		\]
      		
	\section{Slide 10: Multiple Regression - Qualitative Information}
		\subsection{Binary Predictors}
			\begin{remark}
				With binary independent variables, $MLR.1 \sim MLR.6$ still holds, but the interpretations are different.
			\end{remark}
			
			\subsubsection{On Intercept}
			\[
				y = \delta_0 + \delta_1 male + \dots + u
			\]
			\begin{remark}
				\textcolor{red}{To avoid perfect multi-collinearity, never include all categories}.
			\end{remark}
			
			\subsubsection{On Slopes}
			\[
				y = \delta_0 + (\delta_1 + \delta_2 male)\times education + \dots + u
			\]
			
			\subsubsection{F-test(Chow test)}
			\par Test whether the \ul{true coefficients} in 2 linear regression models (e.g. for different gender groups) are equal. 
			\begin{enumerate}
				\item Restricted model ($SSR_r$) \[y = \beta_0 + \beta_1 x + u \]
				\item Unrestricted model ($SSR_{ur}$) \[y = (\beta_0 + \delta_0 indicator) + (\beta_1 + \delta_1 indicator) x + u\]
				\item Test whether the additional factors in coefficients ($\delta_0, \delta_1$) are significant. ($q=2$ in this case)
				\[
					F = \frac{(SSR_{r} - SSR_{ur})/q}{SSR_{ur}/(n-k-1)}
				\]
			\end{enumerate}
		\subsection{Linear Probability Model}
			\paragraph{Model} \emph{Qualitative binary dependent variable}
			\[
				y = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k + u,\ y \in \{0, 1\}
			\]
			\paragraph{Interpretation} the model above predicts the probability of $y=1$.
				\begin{proof}
					\begin{gather*}
						\expect{y|\textbf{x}} = 0 \times Pr(y=0|\textbf{x}) + 1 \times Pr(y=1|\textbf{x}) \\
						= Pr(y=1|\textbf{x})
					\end{gather*}
				\end{proof}
				
			\begin{definition}
				The \textbf{response probability} of independent variable $x_j$ is defined as
				\begin{equation}
					\beta_j = \pd{P(\textbf{x})}{x_j}
				\end{equation}
			\end{definition}
			
			\begin{definition}
				The \textbf{predicted probability} of $y$ to be 1 is defined as
				\begin{equation}
					\hat{P}(\textbf{x}) \equiv \textbf{x}' \hat{\bm{\beta}}
				\end{equation}
			\end{definition}
			
			\begin{remark}[\textcolor{red}{Out-of-range predictions}]
				Notice the prediction is not necessarily with the range of $[0,1]$ for some extreme values of \textbf{x}.
			\end{remark}
			
		\subsection{Heteroskedasticity of LPM}
			\begin{remark}
				For probability linear models, $MLR.5$(homoskedasticity) fails.
			\end{remark}
			\begin{proof}
				\begin{gather}
					y_i = \beta_0 + \beta_1 x_{i1} + \dots \beta_k x_{ik} + u_i \\
					\tx{For binary $y$} \\
					\textcolor{red}{
						Var(u) = Var(y) = Pr(y=1) (1-Pr(y=1))
					} \\
					Var(u|\textbf{x}) = Var(y - \beta_0 - \beta_1 x_1 - \beta_2 x_2 - \dots - \beta_k x_k | \textbf{x}) \\
					= Var(y | \textbf{x}) \\
					= Pr(y=1|\textbf{x}) (1-Pr(y=1|\textbf{x})) \\
					= \expect{y | \textbf{x}} (1 - \expect{y | \textbf{x}}) \\
					= (\beta_0 + \beta_1 x_1 + \dots + \beta_k x_k) (1 - \beta_0 - \beta_1 x_1 - \dots - \beta_k x_k) \\
					\neq \sigma_u^2
				\end{gather}
			\end{proof}
			
			
	\section{Slide 11: Heteroskedasticity}
		\begin{definition}
			Consider model 
			\begin{equation}
				y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + u_i
			\end{equation}
			the error of above model is heteroskedastic if for each \emph{slice} of population captured by $\textbf{x}_i \in \R^{k+1}$, 
			\begin{equation}
				Var(u_i | \textbf{x}_i) = \sigma_i^2
			\end{equation}
			and \hl{$\sigma_i^2$ is not the same for all $i$}.
		\end{definition}
		\begin{remark}[Consequence]
			Without $MLR.5$, Gauss-Markov theorem does not hold and 
			\begin{enumerate}
				\item OLS estimator is still \ul{linear} and \ul{unbiased}.
				\item But \textbf{not} necessarily the best (variance is affected).
			\end{enumerate}
		\end{remark}
		\begin{proof}[Proof. unbiasedness, in simple regression]
			\begin{gather}
				\hat{\beta}_1 = \frac{\sum_i(x_i - \overline{x})(y_i - \overline{y})}{\sum_i(x_i - \overline{x})^2} \\
				= \frac{\sum_i(x_i - \overline{x})(\beta_0 + \beta_1 x_1 + u_i - \overline{y})}{\sum_i(x_i - \overline{x})^2} \\
				= \frac{\sum_i(x_i - \overline{x})(\beta_0 + \beta_1 x_1 + \beta_1 \overline{x} - \beta_1 \overline{x} + u_i - \overline{y})}{\sum_i(x_i - \overline{x})^2} \\
				= \frac{\sum_i \beta_1 (x_i - \overline{x})^2 + (x_i - \overline{x}) (\beta_0 + \beta_1 \overline{x} - \overline{y} + u_i)}{\sum_i(x_i - \overline{x})^2} \\
				= \beta_1 + \frac{\sum_i (x_i - \overline{x}) (0 + u_i)}{\sum_i (x_i - \overline{x})^2} \\
				= \beta_1 + \textcolor{orange}{\frac{\sum_i (x_i - \overline{x})u_i}{\sum_i (x_i - \overline{x})^2}} \\
				\tx{Taking expectation conditional on \textbf{x} on both sides} \\
				\expect{\hat{\beta}_1|\textbf{x}} = \beta_1 
			\end{gather}
		\end{proof}
		
		\begin{proof}[Proof. variance]
			\begin{gather}
				Var(\hat{\beta}_1 | \textbf{x}) = \expect{(\hat{\beta} - \expect{\hat{\beta}_1 | \textbf{x}})^2 | \textbf{x}} \\
				= \expect{(\hat{\beta}_1 - \beta_1)^2 | \textbf{x}} \\
				= \expect{\Big(\textcolor{orange}{\frac{\sum_i (x_i - \overline{x})u_i}{\sum_i (x_i - \overline{x})^2}}\Big)^2|\textbf{x}} \\
				\tx{Note that } (x_i - \overline{x})u_i (x_j \overline{x})u_j = 0 \tx{ if } i \neq j \\
				\tx{By multi-nominal theorem, we can expand the square of summation as} \\
				= \frac{\sum_i(x_i - \overline{x})^2 \expect{u_i^2 | \textbf{x}}}{\Big( \sum_i (x_i - \overline{x})^2 \Big)^2} \\
				\neq \frac{\sigma^2}{SST_x}
			\end{gather}
			For multiple regressions
			\begin{gather}
				Var(\hat{\beta}_j | \textbf{x}) = \frac{\sum_i{\tilde{r}_{ij}^2 \sigma_i^2}}{SSR_j^{\textcolor{red}{2}}} \neq \frac{\sigma^2}{SSR_j} = \frac{\sigma}{(1-R^2_j)SST_j}
			\end{gather}
		\end{proof}
		
		\paragraph{Remedies}
		\begin{enumerate}
			\item Change variables so that the new model is homoskedastic.
			\item Use robust standard errors.
			\item Generalized least square (GLS).
		\end{enumerate}
		
		\subsection{Robust Standard Errors}
			\paragraph{Idea} use $\hat{u}_i^2$ to estimate $\sigma_i^2$.\\
			Note that  
			\begin{gather*}
				Var(u_i | \textbf{x}) = \expect{(u_i - \expect{u_i})^2} \\
				= \expect{u_i^2 | \textbf{x}} + \expect{u_i | \textbf{x}}^2 \\
				= \expect{u_i^2 | \textbf{x}}
			\end{gather*}
			Consider model 
			\[
				y_i = \beta_0 + \beta_1 x_i + u_i
			\]
			OLS estimator with it's variance is given as 
			\begin{gather*}
				\hat{\beta}_1 = \beta_1 + \frac{\sum_i (x_i - \overline{x})u_i}{\sum_i (x_i - \overline{x})^2} \\
				Var(\hat{\beta} | \textbf{x}) = \frac{\sum_i(x_i - \overline{x})^2 \sigma_i^2}{\sum_i(x_i - \overline{x})^2} 
			\end{gather*}
			And a valid estimate of the variance is
			\begin{gather*}
				\widehat{Var}(\hat{\beta} | \textbf{x}) = \frac{\sum_i(x_i - \overline{x})^2 \textcolor{red}{\hat{u}_i^2}}{\sum_i(x_i - \overline{x})^2}
			\end{gather*}
			where $\hat{u}_i$ is the residual term while running the OLS.
		\subsection{Test for Heteroskedasticity}
		\subsubsection{General Principle}
			\begin{gather*}
				H_0: \expect{u_i^2} = Var(u_i|\textbf{x}) = \sigma^2 \tx{ (Homoskedastic)} \\
				H_1: \expect{u_i^2} = Var(u_i|\textbf{x}) = \sigma_i^2 \tx{ (Heteroskedastic)}
			\end{gather*}
			\textbf{Methodology:} specify the variance in alternative hypothesis to be a specific function of $\textbf{x}$ or $y$. \\
			Consider the model: 
			\[
				y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + u_i
			\]
			And $H_1$ can be expressed as 
			\[
				H_1: \expect{u_i^2|\textbf{x}} = \delta_0 + \delta_1 z_1 + \delta_2 z_2 + \dots + \delta_p z_p
			\]
			then run the proxy hypothesis testing
			\begin{gather*}
				\textcolor{red}{H_0': \delta_1 = \delta_2 = \dots = \delta_p = 0, \delta_0 = \sigma^2} \\
				\textcolor{red}{H_1': \exists j\ s.t.\ \delta_j \neq 0}
			\end{gather*}
			Note that the restricted model is homoskedastic. \\
			Firstly run the original regression model and get residual $\hat{u}_i$. \\
			Then test the proxy hypotheses with regression $\hat{u}_i^2$ on $z_1, z_2, \dots, z_p$ using full F-test.
			\begin{gather*}
				F = \frac{R^2_{\hat{u}^2}/p}{(1-R^2_{\hat{u}^2})/(n-p-1)} \sim F_{(p, n-p-1)} \\
				 \tx{and }n R^2_{\hat{u}^2} \sim \mc{X}_p^2
			\end{gather*}
			
			\subsubsection{Breusch-Pagan test}
				\par Use regressors $x_i$ for $z_i$. \\
				Auxiliary regression:
				\begin{gather*}
					\hat{u}_i^2 = \delta_0 + \delta_1 x_1 + \dots \delta_k x_k \\
					n R^2_{\hat{u}^2} \sim \mc{X}_k^2
				\end{gather*}
			\subsubsection{White test version 1}
				\par Use polynomials of $x_i$ for $z_i$.\\
				Auxiliary regression: (for the case of 2 regressors)
				\begin{gather*}
					\hat{u}_i^2 = \delta_0 + \delta_{i1} x_1 + \delta_2 x_{i2} + \delta_3 x_{i1}^2 + \delta_4 x_{i2}^2 + \delta_5 x_{i1} x_{i2} + \epsilon \\
					n R^2_{\hat{u}^2} \sim \mc{X}_5^2 \\
					\tx{or full F-test}
				\end{gather*}
			\subsubsection{White test version 2}
				\par Use \ul{predicted} response $\hat{y}$ (since its a linear combination of predictors) and its polynomial as $z_i$. \\
				Auxiliary regression:
				\begin{gather*}
					\hat{u}_i^2 = \delta_0 + \delta_1 \hat{y} + \delta_2 \hat{y}^2 + \epsilon \\
				\end{gather*}
				With hypotheses
				\begin{gather*}
					H_0: \delta_1 = \delta_2 = 0\\
					H_1: \delta_1 \neq 0 \lor \delta_2 \neq 0
				\end{gather*}
				\begin{gather*}
					n R^2_{\hat{u}^2} \sim \mc{X}_2^2 \\
					\tx{or full F-test}
				\end{gather*}
		\subsection{Generalized/Weighted Least Squared}
			\paragraph{Motivation} when a regression model is suspicious for \emph{heteroskedasticity} (i.e. MLR5 fails), Gauss-Markov theorem does no longer hold and OLS still unbiased and consistent but \ul{no longer the most efficient} one. We wish to \emph{transform} the original model, by \ul{multiplying by weights}($p_i$), to a homoskedastic model. And then run OLS on the transformed model to get linear estimations for coefficients, which are efficient. (Guaranteed by Gauss-Markov theorem)
			
			\subsubsection{GLS with Known Functional Form}
				Suppose (\hl{central assumption})
				\[
					Var(u_i) = \expect{u_i^2|\textbf{X}} = h_i \sigma^2
				\]
				for some known function $h_i$. Take weight function
				\[
					p_i := \frac{1}{\sqrt{h_i}}
				\]
				The \textbf{transformed} equation becomes
				\begin{gather*}
					p_i y_i = p_i \beta_0 + \beta_1 p_i x_{i1} + \dots + \beta_k p_i x _{ik} + p_i u_i \\
					\iff y_i / \sqrt{h_i} = \beta_0 / \sqrt{h_i} + \beta_1 (x_{i1}/\sqrt{h_i}) + \dots + \beta_k (x_{ik}/\sqrt{h_i}) + u_i / \sqrt{h_i} \\
					\implies \expect{(u_i / \sqrt{h_i})^2 | \textbf{X}} = \frac{1}{h_i} h_i \sigma^2 = \sigma^2
				\end{gather*}
				which is homoskedastic.
				\begin{remark}
					In weighted least square with weight function $p_i$ above, the variance of residual at a certain data cross-section is proportional to $h_i$. And $p_i \equiv \frac{1}{\sqrt{h_i}}$, that's, \emph{observations with higher residual variance receive less weight}.
				\end{remark}
			\subsection{Feasible GLS}
				Suppose (\hl{central assumption})
				\[
					Var(u_i|\textbf{X}) = \expect{u_i^2|\textbf{X}} = \sigma^2 exp(\vec{\delta} \cdot \textbf{x}_i)
				\]
				for some constant $\sigma$. \\
				Equivalently,
				\[
					h(x) = exp(\delta_0 + \delta_1 x_1 + \dots + \delta_k x_k)
				\]
				\begin{remark}
					The \emph{exponential} operator in our assumption guarantees the error variance is strictly positive.
				\end{remark}
				To estimate the \textbf{variance}, we are going to model the squared residual $u^2$,
				\begin{gather*}
					u_i^2 = \sigma^2 exp(\vec{\delta} \cdot \textbf{x}_i)\textcolor{red}{v_i} \\
					\iff \ln(u_i^2) = [\ln(\sigma^2) + \delta_0] + [\delta_1 x_{i1} + \dots + \delta_k x_{ik}] + \textcolor{red}{e_i} \\
					\alpha_0 \equiv \ln(\sigma^2) + \delta_0,\quad e_i \equiv \ln(v_i)
				\end{gather*}
			\paragraph{FGLS Procedures}
				\begin{enumerate}
					\item Run OLS regression on the \ul{original} model, then estimate $\hat{u}_i^2$ and $\ln(\hat{u}_i^2)$.
					\item Estimate model 
						\[
							\ln(\hat{u_i}^2) = [\ln(\sigma^2) + \delta_0] + [\delta_1 x_{i1} + \dots + \delta_k x_{ik}] + \textcolor{red}{e_i}
						\]
					\item Compute 
						\[
							\hat{h}_i := exp(\widehat{\ln(\hat{u}s_i^2)})
						\]
					using result from above model. And compute 
					\[
						p_i := \frac{1}{\sqrt{\hat{h}_i}}
					\]
					\item Transform the original model using weights $p_i$ and estimate it using OLS. \hl{Note that the transformed model has \textbf{no constant term}. The constant is replaced with $(p_i \beta_0)$, which varies across observations.}
				\end{enumerate}
	\section{Slide 12: Specification and Data Problems}
		\paragraph{} A multiple regression model suffers from functional misspecification when it does not properly account for the relationship between the dependent and the observed explanatory variables.
		\subsection{Regression Specification Error Test (RESET)}
		\subsubsection{RESET: Nested Alternatives}
			\par \emph{Adding nonlinear functions of the regressors into the model and test for their significance.}
			\\
			Consider model 
			\begin{equation}
				y = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k + u
			\end{equation}
			If the original model satisfies MLR.4 ($\expect{u|\textbf{X}}=0$), then \textbf{no} nonlinear functions of the independent variables should be significant when added to equation (1). 
			
			\paragraph{Procedures}
			\begin{enumerate}
				\item Add polynomials in the OLS fitted values, $\hat{y}$, to equation (1). Typically squared and cubed terms are added.
					\begin{equation}
						y = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k + \delta_1 \textcolor{red}{\hat{y}^2} + \delta_2 \textcolor{red}{\hat{y}^3} + u
					\end{equation}
				\item Use F-test to test the joint significance with $H_0: \delta_1 = \delta_2 = 0$. And \hl{a significant $F$ suggests some sort of functional form problem.}
					\[
						F \sim \mc{F}_{(2, n-k-2)}
					\]
			\end{enumerate}
			\begin{remark}
				We will not be interested in the estimated parameters from (2); we only use this equation to test whether (1) has missed important non-linearities.
			\end{remark}
			
			
			\begin{definition}[Nested Alternatives]
				One model is \textbf{nested} in another if you can always obtain the first model by constraining some of the parameters of the second model.
			\end{definition}
			
			\begin{example}
				In above example, the original regression is \emph{nested} in the expanded regression. We can recover the original regression by constraining $\delta_1 = \delta_2 = 0$ in the expanded model.
			\end{example}
			
			
		\subsubsection{Non-nested Alternatives: RESET}
			\par Neither of the two models below is nested in the other one, \hl{we \textbf{cannot} use F-test}.
			\begin{gather}
				y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u \\
				y = \beta_0 + \beta_1 \log(x_1) + \beta_2 \log(x_2) + u
			\end{gather}
			\paragraph{Procedures}
			\begin{enumerate}
				\item Construct a \emph{comprehensive model} that contains each model as a special case and then to test the restrictions that led to each of the models.
				\begin{equation}
					y = \beta_0 + \gamma_1 x_1 + \gamma_2 x_2 + \gamma_3 \log(x_1) + \gamma_4 \log(x_4) + u
				\end{equation}
				\item Test competing specifications
				\begin{enumerate}
					\item (F) test for specification (4): $H_0: \gamma_1 = \gamma_2 = 0$.
					\item (F) test for specification (3): $H_0: \gamma_3 = \gamma_4 = 0$.
				\end{enumerate}
			\end{enumerate}
			
			
		\subsubsection{Non-nested alternatives: Davidson-MacKinnon test}
			\par Let $\hat{y}_3$ and $\hat{y}_4$ denote the fitted values from (3) and (4) respectively. \\
			If model (3) holds with $\expect{u|x_1, x_2}=0$, the \hl{fitted values} from the other model, (4), should be insignificant when added to equation (3).
			
			\paragraph{Procedures}
			\begin{enumerate}
				\item Test for specification (3) with $H_0: \theta_1=0,\ H_1: \theta_1 \neq 0$.
					\begin{equation}
						y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \theta_1 \hat{y}_{\textcolor{red}{4}} + u
					\end{equation}
				\item Test for specification (4) with $H_0: \theta_1=0,\ H_1: \theta_1 \neq 0$. \\ \hl{A significant $t$ statistic (against a two-sided alternative) is a rejection of (4)}.
					\begin{equation}
						y = \beta_0 + \beta_1 \log(x_1) + \beta_2 \log(x_2) + \theta_1 \hat{y}_{\textcolor{red}{3}} + u
					\end{equation}
			\end{enumerate}
			
			\begin{remark}[Porblems] \quad
				\begin{enumerate}
					\item In Davison-MacKinnon test, its possible for us to reject or accept both specifications.
					\begin{enumerate}
						\item If neither rejected, use adjusted R-square to choose one model.
						\item If both rejected, find another alternative.
					\end{enumerate}
					\item Note that a rejection of (3) does not mean (4) is the correct model.
					\item The case when competing models have \ul{different dependent variables} could be problematic. ($y = \dots $ against $\log(y) = \dots$)
				\end{enumerate}
			\end{remark}
		\subsection{Proxy Variables}
			\subsubsection{Procedures}
				\par Consider the true model
				\begin{equation}
					y = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k^{*} + u
				\end{equation}
				where $x_k^{*}$ is unobserved.
				
				\begin{notation}
					In this text, we always use starred-variable, $var^*$, to denote the \ul{true} (sometime unobservable) variable.
				\end{notation}
				
				\paragraph{(1) Selecting proxy} Choose an observed variable $x_k$ is a \textbf{proxy} for $x_k^*$ such that
				
				\begin{equation}
					x_k^{*} = \delta_0 + \delta_k x_k + v
				\end{equation}
				\begin{remark}
					Typically we want $\delta_k > 0$, and no restriction on $\delta_0$.
				\end{remark}
				
				
				\paragraph{(2) Plug-in the Proxy} Direct replacement
				\begin{gather}
					y = \beta_0 + \beta_1 x_1 + \dots + \beta_k (\textcolor{red}{\delta_0 + \delta_k x_k + v}) + u\\
					= (\beta_0 + \textcolor{red}{\beta_k \delta_0}) + \beta_1 x_1 + \dots + \textcolor{red}{\beta_k \delta_k} x_k + (u + \textcolor{red}{\beta_k v})
				\end{gather}
				\begin{assumption}
					For a \emph{consistent} estimator, we need to assume that
					\begin{enumerate}
						\item $u$ is uncorrelated with $x_1, x_2, \dots, x_k^{*}, x_k$.
						\item $v$ is uncorrelated with $x_1, x_2, \dots, x_k$.
						\begin{gather}
							\implies \expect{x_k^{*}|x_1,x_2,\dots,x_k} \\
							= \expect{\delta_0 + \delta_k x_k + v|x_1,x_2,\dots,x_k} = \delta_0 + \delta_k x_k
						\end{gather}
					\end{enumerate}
					\hl{To guarantee MLR.4 holds for both true model and the model with proxy substitution.}
				\end{assumption}
				\begin{remark}
					Under above assumptions and regressing $y$ on $x_1, x_2, \dots, \textcolor{red}{x_k}$, the OLS estimator for $(\beta_1, \beta_2, \dots, \beta_{k-1})$ is still \hl{consistent} and \hl{unbiased}.\\
					 \hl{But} for intercept and $k^{th}$ coefficient, we are effectively estimating $\beta_0 + \delta_0 \beta_k$ and $\delta_k \beta_k$.
				\end{remark}
			\subsubsection{Proxy Bias}
				If $x_k^{*}$ is correlated with all $\{x_1, x_2, \dots, x_k\}$ (collinearity), i.e. 
				\[
					x_k^{*} = \delta_0 + \delta_1 x_1 + \delta_2 x_2 + \dots + \delta_k x_k + v_k
				\]
				the for the coefficient of $x_j$ in the original regression, 
				\[
					plim(\hat{\beta}_j) = \beta_j + \beta_k \delta_j
				\]
				which means the estimation is still biased. \hl{In this case, using a proxy variable will not solve the omitted variable bias problem}.
				
		\subsection{Measurement Error in an Explanatory Variable}
			\par Consider the model 
			\[
				y = \beta_0 + \beta_1 x_1^{*} + u
			\]
			but we can only observe $x_1 = x_1^{*} + e_1$. 
			
			\begin{assumption}
				Assuming \textbf{measurement error} satisfies
				\[
					\expect{e_1} = 0
				\]
			\end{assumption}
			
			and the regression model becomes if we regress $y$ on the observed $x_1$.
			\begin{equation}
				y = \beta_0 + \beta_1 x_1 + (u - \beta_1 e_1)
			\end{equation}
			\begin{assumption}
				$u$ is uncorrelated with both $x_1$ and $x_1^{*}$, i.e. $x_1$ does not affect $y$ after $x_1^*$ has been controlled for.
			\end{assumption}
			
			\subsubsection{Case 1: $Cov(x_1, e_1) = 0$}
				\begin{remark}
					Since $e_1 = x_1 + x_1^*$, if $Cov(x_1, e_1) = 0$ then $Cov(x_1^*, e_1) \neq 0$.
				\end{remark}
				\begin{remark}
					\begin{gather*}
						\expect{u-\beta_1 e_1} = \expect{u} - \beta_1 \expect{e_1} = 0
					\end{gather*}
					MLR.3 still holds and \hl{estimator $\hat{\beta}_1$ is still consistent}.
				\end{remark}
				
				\begin{remark}
					Note that 
					\[
						Var(u - \beta_1 e_1) = \sigma_u^2 + \beta_1^2 \sigma_{e_1}^2
					\]
					the variance of estimators is inflated unless $\beta_1 = 0$.
				\end{remark}
				
			\subsubsection{Case 2 $Cov(x_1^{*}, e_1) = 0$: Classical errors-in-variance(CEV)}
				\begin{remark}
					\begin{gather*}
						Cov(x_1, e_1) = \expect{(x_1 - \overline{x}_1)(e_1 - \overline{e}_1)} \\
						= \expect{x_1 e_1} \\
						= \expect{(x_1^{*} + e_1) e_1} \\
						= \expect{x_1^{*}e_1 + e_1^2} \\
						= 0 + \expect{e_1^2} \\
						= \expect{(e_1 - \overline{e}_1)^2} \\
						= \sigma_{e_1}^2 \neq 0
					\end{gather*}
				Thus the covariance between $x_1$ and $x_1$ is equal to the variance of the measurement error under CEV assumption.
				\end{remark}
				
				\begin{remark}
					From equation (11), the new residual is $(u - \beta_1 e_1)$ and
					\begin{gather*}
						Cov(x_1, u - \beta_1 e_1) = \sum{(x_1 - \overline{x}_1) (u - \beta_1 e_1)} \\
						= \sum {x_1 u} - \beta_1 \sum {x_1 e_1} \\
						= Cov(x_1, u) - \beta_1 \sum {(x_1 - \overline{x}_1)(e_1 - 0)} \\
						= 0 - \beta_1 Cov(x_1, e_1) \\
						= \sigma_{e_1}^2 \neq 0
					\end{gather*}
					this fails MLR.4 and the OLS regression of $y$ on $x_1$ gives a \hl{biased} and \hl{inconsistent} estimator.
				\end{remark}

		\subsection{Measurement Error in Dependent Variable}
			\par Consider model 
			\begin{equation}
				y^{*} = \textbf{X}\vec{\beta} + u
			\end{equation}
			and the actually observed $y$ is $y = y^{*} + e_0$, with \textbf{measurement error} $e_0$. If we regress the observed $y$ on explanatory variables, we are effectively estimating
			\begin{equation}
				y = \textbf{X}\vec{\beta} + \textcolor{red}{(u+e_0)}
			\end{equation}
			\begin{remark}
				\hl{Assuming the measurement error in $y$ is statistically independent of each explanatory variable}, the OLS estimator from (12) is consistent and unbiased (Gauss-Markov Holds).
			\end{remark}
			\begin{remark}
			Note that we would now have higher residual variance $\sigma_u^2 + \sigma_{e_0}^2$ and the variance for OLS estimator is inflated
				\[
					Var(\vec{\beta}) = (\sigma_u^2 + \sigma_{e_0}^2) (\textbf{X}'\textbf{X})^{-1}
				\]
			\end{remark}
			
			
			
	\section{Slide 13: Instrumental Variables}
		\subsection{Endogeneity}
			\begin{definition}
				If a predictor $x_j$ is correlated with $u$ for any reason, and MLR.4 is violated, then $x_j$ is said to be an \textbf{endogenous} explanatory variable.
			\end{definition}
			\[
				\expect{u|\textbf{x}} \neq 0
			\]
			
			\begin{equation}
				y = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k + u
			\end{equation}
			
			\paragraph{Sources of Endogeneity}
				\begin{itemize}
					\item Omitted variable bias.
					\item Sample selection bias.
					\item Simultaneity (bidirectional causality).
					\item Measurement error bias.
				\end{itemize}
			\paragraph{Remedies}
				\begin{itemize}
					\item Control for confounding variables.\footnote{A \textbf{confounding variable} is a variable that influences both the dependent variable and independent variable causing a spurious association.}
					\item Instrumental variables or two stage least square.
					\item Differences in difference. (repeated cross-section data)
					\item Fixed effects. (panel data)
				\end{itemize}
		\subsection{Instrumental Variables}
			\paragraph{The Problem} For the simple regression model
			\[
				y = \beta_0 + \beta x + u
			\]
			estimator $\hat{\beta}$ would be biased if endogeneity presents ($Cov(x, u) \neq 0$). \\
			Then OLS is actually estimating 
			\[
				\pd{y}{x} = \beta + \pd{u}{x}
			\]
			instead of purely $\beta$, where $\pd{u}{x} \neq 0$ due to endogeneity.\\
			\emph{We need a method to generate only exogenous variation in $x$, without changing $u$, and measure its impact on $y$ via $\beta$ only.}
			
			\begin{definition}
				An \textbf{instrument} $z$ for predictor $x$ is a variable the property that
				\begin{enumerate}
					\item (\ul{Exogeneity condition}) uncorrelated with $u$. \[ Cov(z,u) = 0\]
					\item (\ul{Relevance condition}) correlated (either positively or negatively) with $x$. \[Cov(z,x) \neq 0\]
				\end{enumerate}
			\end{definition}
			
			\begin{remark}
				There no perfect test for exogeneity condition and we have to argue it by appealing to economic theory. So we cannot prove exogeneity condition formally.
			\end{remark}
			\begin{remark}
				For the relevance condition, we can test it by testing the significance of $\pi_1$ in the regression below
					\[
						x = \pi_0 + \pi_1 z + v
					\]
			\end{remark}
		\subsection{Implementation of IV: Method of Moments}
			\paragraph{Procedure}
				\begin{enumerate}
					\item Identify $\beta$ in terms of \emph{population moments}.
					\item Replace the population moments with the sample moments.\footnote{By \textbf{analogy principle}, such replacement will lead to a consistent estimator.}
				\end{enumerate}
			\subsubsection{In Simple Regression}
				\paragraph{Identification} Consider the model with instrumental variable $z$ for $x$,
					\[
						y = \beta_0 + \beta_1 x + u
					\]
					subtract both sides the corresponding expectations,
					\[
						y - \expect{y} = \beta_1 (x - \expect{x}) + (u - \expect{u})
					\]
					multiplying both sides by $(z - \expect{z})$ and take expectation
					\begin{gather*}
						\expect{(y-\expect{y})(z-\expect{z})} = \beta_1\expect{(x-\expect{x})(z-\expect{z})} + \expect{(u-\expect{u})(z-\expect{z})} \\
						\implies Cov(y, z) = \beta_1 Cov(x,z) + Cov(u,z) \\
						\tx{By exogeneity condition and relevance condition} \\
						Cov(x,z) \neq 0 \land Cov(z,u) = 0\\
						\implies \textcolor{red}{\beta_1 = \frac{Cov(y,z)}{Cov(x,z)}}
					\end{gather*}
				\paragraph{Replacement} calculate the \ul{sample} covariances between $y,z$ and $x,z$ and substitute into above expression, the \textbf{IV estimator} of $\beta_1$ is
				\[
					\hat{\beta}_1 = \frac{\sum_i (y_i - \overline{y})(z_i - \overline{z})}{\sum_i(x_i - \overline{x})(z_i - \overline{z})} 
				\]
				and the \textbf{IV estimator} of $\beta_0$ is 
				\[
					\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}
				\]
				\begin{remark}
					When $z=x$ the IV estimator is equivalent to the OLS estimator. And the IV estimator is consistent even when MLR.4 does not hold.
				\end{remark}
			\subsubsection{Inference}
				Assuming 
				\[
					\expect{u^2|z}=\sigma^2=Var(u)
				\]
				Then the variance of $\hat{\beta}_1$ is 
				\[
				\textcolor{red}{
					Var(\hat{\beta}_1) = \frac{\sigma^2}{n\sigma_x^2 \rho_{x,z}^2}
				}
				\]
				with sample analogs and $R^2_{x,z}$ from regression of $x_i$ on $z_i$, the estimated variance is 
				\[
					\widehat{Var(\hat{\beta}_1)} = \frac{\hat{\sigma}^2}{SST_x R^2_{x,z}}
				\]
				Note that the variance of OLS estimator is estimated to be
				\[
					\widehat{Var(\hat{\beta}_1)} = \frac{\hat{\sigma}^2}{SST_x}
				\]
				Therefore the $IV$ estimator is always larger than OLS variance. \\
				Note that as $z \to x$, $R^2_{x,z} \to 1$ and IV estimator is approaching and ultimately equivalent to the OLS estimator.
			\subsubsection{Properties}
				If $z$ and $x$ are weakly correlated (aka. \textbf{weak instrument}).
				\begin{itemize}
					\item IV estimators can have large standard errors. (small $R^2_{x,z}$)
					\item IV estimators can have large \ul{asymptotic bias} if $Corr(z,u) \neq 0$ (since we cannot check exogeneity condition formally, so we cannot rule out this probability).
				\end{itemize}
				For IV estimator,
				\[
					plim \hat{\beta}_{1, IV} = \beta_1 + \frac{Corr(z,u) \sigma_u}{\textcolor{red}{Corr(z,x)} \sigma_x}
				\]
				compared with OLS estimator
				\[
					plim \hat{\beta}_{1,OLS} = \beta_1 + Corr(x,u) \frac{\sigma_u}{\sigma_x}
				\]
				
				\begin{remark}
					The $R^2$ in IV estimation can be negative, and we should be careful about interpreting $R^2$ in IV estimation.
				\end{remark}
			\subsection{IV in Multiple Regression}
			\par Consider the multiple regression model on $k$ predictors, where $y_2$ is endogenous. The \textbf{structural model} is given in (2) below.
			\begin{equation}
				y_1 = \beta_0 + \beta_1 \textcolor{red}{y_2} + \beta_2 z_1 + \cdots + \beta_k z_{k-1} + u_1
			\end{equation}
			\paragraph{Identification} Let $z_k$ be an instrumental variable for $y_2$ the exogenity condition can be expressed as
			\[
				Cov(z_k, u_1) = 0
			\]
			and assuming all other explanatory variables $z_i$ are uncorrelated with $u_1$. Also assume the \emph{zero-mean-error},
			\begin{gather*}
				Cov(z_i, u_1) = 0,\ \forall i \in \{1, 2, \dots, k - 1\} \\
				\expect{u_1} = 0
			\end{gather*}
			Above conditions can be re-written as 
			\begin{gather*}
				\expect{z_i u_1} = 0,\ \forall i \in \{1,2, \dots, k\} \\
				\expect{u_1} = 0
			\end{gather*}
			Above $k+1$ equations identify $\beta_0, \beta_1, \dots, \beta_k$.
			
			\paragraph{Replacement} Replacing $u_1$ with $\hat{u}_1$ from regression (2),
			\begin{gather*}
				\sum_{i=1}^n(y_{i1} - \hat{\beta}_0 - \hat{\beta}_1 \textcolor{red}{y_{i2}} - \hat{\beta_2}z_{i1} - \cdots - \hat{\beta}_k z_{k-1}) = 0 \\
				\sum_{i=1}^n \textcolor{red}{z_{i1}} (y_{i1} - \hat{\beta}_0 - \hat{\beta}_1 \textcolor{red}{y_{i2}} - \hat{\beta_2}z_{i1} - \cdots - \hat{\beta}_k z_{k-1}) = 0 \\
				\sum_{i=1}^n z_{i2} (y_{i1} - \hat{\beta}_0 - \hat{\beta}_1 \textcolor{red}{y_{i2}} - \hat{\beta_2}z_{i1} - \cdots - \hat{\beta}_k z_{k-1}) = 0 \\
				\vdots \\
				\sum_{i=1}^n z_{ik-1}(y_{i1} - \hat{\beta}_0 - \hat{\beta}_1 \textcolor{red}{y_{i2}} - \hat{\beta_2}z_{i1} - \cdots - \hat{\beta}_k z_{k-1}) = 0
			\end{gather*}
			And solving above $k+1$ equations and replacing  give the IV estimations of $\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_{k}$.
			\newline
			The \ul{relevance condition} $Corr(y_2, z_k)$ can be verified using \textbf{reduced-form(auxiliary) equation} below with $H_0: \pi_k = 0$ and $H_1: \pi_k \neq 0$.
			\[
				y_2 = \pi_0 + \pi_1 z_1 + \pi_2 z_2 + \cdots + \pi_k \textcolor{red}{z_k} + v_2
			\]
	
	\section{Slide 14: Two Stage Least Square}
	\subsection{Procedure}
		\paragraph{Motivation} Multiple good instrumental variables for the endogenous variable. \\
		\textbf{Structural Equation}: 
		\begin{gather}
			y = \beta_0 + \beta_1 y_2 + \beta_2 z_1 + u_1
		\end{gather}
		with \textbf{Reduced Form Equation}:
		\begin{gather}
			y_2 = \pi_0 + \pi_1 z_1 + \pi_2 z_2 + \pi_3 z_3 + v_2
		\end{gather}
		where at least one of $\pi_2, \pi_2 \neq 0$. (Relevance condition) \\
		\paragraph{2SLS Procedures}
		\begin{enumerate}
			\item \textbf{Stage 1} Run regression on REF and compute $\hat{y}_2$, which is a linear combination of $z_1, z_2, z_3$. So $\hat{y}_2 \perp u_1$ by exogeneity condition. Note that, $v_2 \centernot \perp u_1$.
			\[
				\hat{y}_2 = \hat{\pi}_0 + \hat{\pi}_1 z_1 + \hat{\pi}_2 z_2 + \hat{\pi}_3 z_3
			\]
			\item Check significance of $z_2$ and $z_3$ to verify relevance condition. 
			\item \textbf{Stage 2} Regress $y_1$ on $\hat{y}_2$ and $z_1$ to obtain $\hat{\beta}_{1, 2SLS}$.
		\end{enumerate}
		
		\begin{remark}
			The first stage of 2SLS removes endogeneity of $y_2$ (dropped with $v_2$).
		\end{remark}

		\paragraph{2SLS Procedures: general case}
		\begin{enumerate}
			\item \textbf{Stage 1} For each included endogenous explanatory variables, construct its reduced form equation with instrumental variables (excluded exogenous) and included exogenous variables.
			\item Check significance of every instrumental variables using $t$ test and/or the joint significance of all instrumental variables used.
			\item \textbf{Stage 2} Regress $y$ all included exogenous variables and the estimated reduced form equations ($\hat{y}_j$) for all included endogenous variables.
		\end{enumerate}		

		\begin{remark}[Number of IVs, the general case]
			With $k$ predictors in total, if $m$ of them are endogenous, we need at least $m$ excluded exogenous variables to run 2SLS.\\
			Otherwise, in the second stage regression, we would have less explanatory variables than parameters to be estimated. (\emph{perfect collinearity})
		\end{remark}
		
		\subsection{Equivalence between IV and 2SLS}
			\par On the simple regression 
			\[
				y = \beta_0 + \beta_1 x + u
			\]
			and let $z$ be the excluded exogenous variable used as the instrumental for $x$. \\
			For simplicity, assume $\overline{x} = \overline{y} = \overline{z} = 0$. \\
			Then IV estimator 
			\[
				\hat{\beta}_{1, IV} = \frac{Cov(z,y)}{Cov(z,x)} = \frac{\sum{yz}}{\sum{xz}}
			\]
			And 2SLS estimator
			\begin{gather*}
				\hat{\beta}_{1,2SLS} = \frac{\sum (\hat{x} - \overline{\hat{x}}) (y - \overline{y})}{\sum (\hat{x} - \overline{\hat{x}})^2} \\
				= \frac{\sum \hat{x} y}{\sum \hat{x}^2}
				= \frac{\sum (\hat{\pi}_0 + \hat{\pi}_1 z) y}{\sum (\hat{\pi}_0 + \hat{\pi}_1 z)^2} \\
				= \frac{\sum \hat{\pi}_1 y z}{\sum \hat{\pi}_1^2 z^2}
				= \frac{1}{\hat{\pi}_1} \frac{\sum yz}{\sum z^2} \\
				= \frac{\sum z^2}{\sum zx} \frac{\sum yz}{\sum z^2} = \frac{\sum yz}{\sum xz} = \hat{\beta}_{1, IV}
			\end{gather*}
	\subsection{Evaluating 2SLS}
		\subsubsection{Regressor Endogeneity}
			\par OLS is BLUE, if OLS is consistent we should not use the relatively less efficient 2SLS.
			\paragraph{Hausman's Test for OLS Consistency}
			\begin{figure*}
				\centering
				\begin{tabular}{|c|c|c|}
					\hline
					& $H_0$ & $H_1$ \\
					\hline
					$\hat{\vec{\beta}}_{OLS}$ & Consistent and Efficient & Inconsistent \\
					\hline
					$\hat{\vec{\beta}}_{2SLS}$ & Consistent but less Efficient & Consistent \\
					\hline
				\end{tabular}
			\end{figure*}
			\par If $H_0$ if failed to be rejected use OLS as BLUE, if we reject $H_0$ then use 2SLS.
			\begin{gather*}
				H_0: plim\ \hat{\beta}_{OLS} = plim\ \hat{\beta}_{2SLS} = \vec{\beta} \\
				H_1: plim\ \hat{\beta}_{OLS} \neq \vec{\beta} \land plim\ \hat{\beta}_{2SLS} = \vec{\beta}
			\end{gather*}
			Take 
			\[
				d = \hat{\beta}_{OLS} - \hat{\beta}_{2SLS}
			\]
			Under the Null Hypothesis, a normalized $d$ statistic is distributed as a $\chi_g$ where $g$ is the number of parameters in the test.
			
		\subsubsection{Instrument Relevance}
			\par Check the significance of instrumental variables in \textbf{reduced form equations} with t-test or F-test. If certain IV is not significant in reduced form equation, then do not use this IV.
			\par Consider model
			\begin{equation}
				y_1 = \beta_0 + \beta_1 y_2 + \beta_2 z_1 + \beta_3 z_2 + u
			\end{equation}
			where $y_2$ is suspended to be endogenous and $(z_3, z_4)$ are used as instrumental variables.
		\subsubsection{Instrument Exogeneity}
			\par Theoretically impossible to test.
			\begin{itemize}
				\item Solution (1): economic sense.
				\item Solution (2): over-confidence test (with $z_3$ and $z_4$ as instrumental variables)
				\begin{enumerate}
					\item Assume $z_3$ is a valid instrumental variable, use $z_3$ as IV to recover $\hat{u}_1$. 
					\[
						\hat{u}_1 = y_1 - \hat{\beta}_{0, IV} - \hat{\beta}_{1, IV} y_2 - \hat{\beta}_{2, IV} z_1 - \hat{\beta}_{3, IV} z_3
					\]
					\item Test if $Cov(z_4, \hat{u}_1) = 0$ to test the validity of $z_4$.
					\[
						\hat{u}_1 = \delta_0 + \delta_1 z_1 + \delta_2 z_2 + \delta_3 z_3 + \delta_4 z_4 + \epsilon
					\]
					with $H_0$ all insignificant (exogenous) and $H_1$ at least one of $z_i$ is significant (endogenous). And under $H_0$,
					\[
						nR^2_{u,z} \sim \chi^2_q
					\]
					where $q$ is the \textbf{degree of overconfidence}, which is the number of IV excluded from the main regression minus the number of endogenous variables.
					\item Use $z_4$ to recover $\hat{u}_1$ and test again.
				\end{enumerate}
			\end{itemize}
	\section{Slide 15: Simultaneous Equations}
		\paragraph{Motivation} Variables are \emph{jointly determined}.
		\begin{example}
			Linear supply and demand framework.
			\[
				\begin{cases}
					p = \beta_{11} + \beta_{12} q_s + \beta_{13} z_1 + u_1 \\
					p = \beta_{21} + \beta_{22} q_d + \beta_{23} z_1 + u_2 \\
					p_d = p_s \\
				\end{cases}
			\]
			where $z_1$ and $z_2$ are exogenous variables. (aka \textbf{supply and demand shifters})\\
			$q_s, q_d, p$ are endogenous variables. \\
			$u_1$ and $u_2$ are structural errors.
		\end{example}
		
		\subsection{Simultaneity Bias in OLS}
			\par Above simultaneous model can be written as reduced from below
			\begin{gather}
				\begin{cases}
					q_s = q_d = \frac{1}{\beta_{12} - \beta_{22}} (\beta_{11} - \beta_{21} + \beta_{13} z_1 - \beta_{23}z_2 + u_1 - u_2)\\
					p = \frac{\beta_{12}}{\beta_{12} - \beta_{22}} (\beta_{11} - \beta_{21} + \beta_{13} z_1 - \beta_{23}z_2 + u_1 - u_2) + \beta_{11} + \beta_{13}z_1 + u_1
				\end{cases} \\
				\implies \begin{cases}
					q_s = q_d = \pi_{11} + \pi_{12} z_1 + \pi_{13} z_2 + v_1 \\
					p = \pi_{21} + \pi_{22} z_1 + \pi_{23} z_2 + v_2
				\end{cases}
			\end{gather}
			From (1), obviously $q$ and $p$ are correlated with $u_1,u_2$. \\
			And $v_1, v_2 \centernot \perp u_1, u_2$. \\
			\begin{notation}
					$\beta$: \textbf{structural form parameters}.
					$\pi$: \textbf{reduced form parameters}.
					$(v_1, v_2)$: \textbf{reduced form errors}. \\
				And note that $z_1, z_2 \perp v_1, v_2$.
			\end{notation}
			If we use (2) and OLS to regress $q$ and $p$ based on exogenous variables $z_1, z_2$, \hl{we will get consistent reduced form parameters but not structural form parameters}.
		\subsection{IV Estimator and 2SLS}
			\paragraph{Key} From (1) or (2), we can show that $p, q$ are correlated with $z_1, z_2$. (Relevance condition) \\
			Also $z_1, z_2 \perp v_1, v_2$ implies exogeneity condition holds. \\
			Use $z_1$ and $z_2$ as instrument for $p, q$.
			\paragraph{Procedures}
			\begin{enumerate}
				\item (Stage 1 OLS) Regress $q$ on $z_1, z_2$.
				\item Estimate $\hat{q}.= \hat{\pi}_{11} + \hat{\pi}_{12} z_1 + \hat{\pi}_{13} z_2$.
				\item (Stage 2 OLS)
				\begin{enumerate}
					\item Regress $p$ on $\hat{q}, z_1$ to obtain supply function.
					\item Regress $p$ on $\hat{q}, z_2$ to obtain demand function.
				\end{enumerate}
			\end{enumerate}
			
			\paragraph{Special case} consider the case
			\begin{equation}
				\begin{cases}
					q_s = \alpha_1 p + \beta_1 z_1 + u_1 \\
					q_d = \alpha_2 p + u_2 \\
					q_s = q_d \\
				\end{cases}
			\end{equation}
			We cannot recover $q_s$ since $\hat{p}$ would be a function of $z_1$ only and we would encounter perfect co-linearity when regress $q_s$ on $\hat{p}, z_1$. \\
			\hl{In general, variables that appear \textbf{only} in the demand function can be valid instrument to estimate supply, vice versa. (Otherwise, perfect multi-collinearity)}
			\paragraph{Exclusion Restriction} $z_1$ omitted in demand and $z_2$ omitted in supply.
			\paragraph{Rank Condition}(Sufficiency, not covered) tells us when such exclusion restrictions are sufficient to estimate structural parameters and \hl{ensures unique solution for structural parameters}.
			\paragraph{Order Condition}(Necessary condition for identification) At least as many \emph{excluded exogenous variables (instrument)} are required as \emph{included endogenous variables} in the structural equation. (Otherwise, perfect multi-collinearity)
			
			\begin{remark}
				In 2SLS, we are basically replacing \emph{included endogenous variables} with linear combinations (OLS prediction) of \emph{all exogenous variables}. If one equation is unidentified, the total number of exogenous variables \ul{after} replacement would be less than number of parameters
			\end{remark}
			\begin{definition} (Identifications) \\
				\textbf{Over identified} equation: more excluded exogenous variables than included endogenous variables. \\
				\textbf{Just identified} same number of excluded exogenous variables and included endogenous variables. \\
				\textbf{Unidentified} less excluded exogenous variables than included endogenous variables.
			\end{definition}
			\begin{remark}
				\hl{Only over-identified and just identified equations can be correlated estimated by 2SLS.}
			\end{remark}
			
	\setcounter{section}{16}
	\section{Slide 17: Intro to Time Series}
		\begin{definition}
			A \textbf{time series}(or stochastic process) is a sequence of random variables
			\[
				\{y_t\},\quad t = 1,2,\dots,n
			\]
		\end{definition}
		\begin{itemize}
			\item Order matters.
			\item Only a single \emph{realization} of "economic history" (a stochastic process).
			\item Model the statistics as if we could observe repeated realization of the entire sequence.
		\end{itemize}
		
		\begin{example}[Static Phillips Curve]
			captures \emph{contemporaneous relationships}.
			\[
				inflation_t = \beta_0 + \beta_1 unemployment_t + u_t
			\]
		\end{example}
		
		\begin{example}[Expectation Augmented Phillips Curve]
			\[
				\Delta infl_t = infl_t - infl_{t-1} = \beta_0 + \beta_1 unemp_t + u_t
			\]
		\end{example}
		
		\subsection{Random Walk}
		
		\begin{definition}
			\textbf{Random walk} process.
			\[
				y_t = y_{t-1}+ e_t
			\]
			where $e_t$ follows \emph{white noise} satisfying
			\begin{enumerate}
				\item $\expect{e_t} = 0$
				\item $Var(e_t) = \sigma_e^2$
				\item $\expect{e_t e_s} = 0,\ t \neq s$ (i.e. $Cov(e_t, e_s) = 0,\ t \neq s$)
			\end{enumerate}
		\end{definition}
		\paragraph{Properties of Random Walker Process}
			\begin{enumerate}
				\item $y_t = \sum_{i=1}^t e_i + y_0$
				\item $Var(y_t) = Var(y_0) + t^2 \sigma_e^2$
				\item $\expect{y_t y_{t+h}} = \expect{y_t (y_t + e_{t+1} + \dots + e_{t+h})} = \expect{y_t^2}$
				\begin{enumerate}
					\item If $\expect{y_0} = Var(y_0)$ (i.e. $y_0=0$), then $\expect{y_t^2} = t^2 \sigma_e^2$.
				\end{enumerate}
			\end{enumerate}
		
		\subsection{Trend}
			\begin{itemize}
				\item If trend presents (i.e. $t$ should be included as a regressor), then ignoring the trend would introduce \hl{omitted variable bias}.
				\item (In practice) partialling out trends.
				\item \hl{\textbf{Note:}} \ul{$R^2$ tends to be high when using trending data and the high $R^2$ may reflect the explanatory power of trend ($t$) but not the explanatory power of $\textbf{x}_t$}
			\end{itemize}
		
		\subsection{Seasonality}
			\paragraph{Simple Solution}: including dummy variables representing different sections in a complete season loop.
			\[
				y_t = \alpha_0 + \textcolor{green}{\alpha_1 t} + \textcolor{red}{\delta_1 Q_{t1} + \delta_2 Q_{t2} + \delta_3 Q_{t3}} + \beta \textbf{x} + u_t
			\]
			where $\alpha_1 t$ captures the trend and $\delta_i Q{ti}$ captures the seasonalities.
		
		\subsection{Serial Correlation}
			\begin{definition}
				\[
					\expect{u_t u_s | \textbf{X}} \neq 0, \tx{ for } t \neq s
				\]
			\end{definition}
			\begin{remark}
				Similar to heteroskedasticity, $\expect{u_t^2 | \textbf{X}} = \sigma_t^2 \neq \sigma^2$.
			\end{remark}
			\paragraph{Consequences} OLS will still be
				\begin{enumerate}
					\item Consistent
					\item and unbiased
				\end{enumerate}
				but
				\begin{enumerate}
					\item not the best (i.e. not the most efficient)
					\item biased/inconsistent variance-covariance matrix and biased inferences.
				\end{enumerate}
		\subsection{Stationarity}
			\begin{definition}
				A \emph{stochastic process} 
				\[
					\{x_t | t = 1, 2, \dots \}
				\]
				is \textbf{stationary} if the \emph{joint distribution} of $\{x_{t1}, x_{t2}, \dots\}$ is the same as $\{x_{t1+h}, x_{t2+h}, \dots\}$. \\
				i.e. \begin{enumerate}
					\item $x_t$ is identically distributed for all $t \in \N$,
					\item and $corr(x_t, x_{t+1}) = corr(x_{t+h}, x_{t+h+1})$.
				\end{enumerate}
			\end{definition}
		
		\subsection{Weakly Dependent Time Series}
			\begin{definition}
				A time series is \textbf{weakly dependent} if, loosely speaking, $x_{t}$ and $x_{t+h}$ are "almost independent" as $n$ increases without bound. \\
				Similar to the \emph{random sample} assumption in MLR. \\
				\[
					\lim_{h\to \infty} corr(x_t, x_{t+h}) = 0
				\]
			\end{definition}
		\begin{remark}
			To use OLS on time series, the series must be \ul{both}
			\begin{enumerate}
				\item Stationary
				\item \textbf{and} weakly dependent
			\end{enumerate}
		\end{remark}
	
	\section{Slide 18: Asymptotic Analysis}
		\begin{definition}
			A sequence of random variable $\{Z_n\}$ is said to \textbf{converge in distribution} to a random variable $Z$ if 
			\[
				\lim_{n \to \infty} F_n(x) = F(x)
			\]
			for $x \in \R$ at which $F$ is continuous, where $F_n$ is the \emph{cdf} of $Z_n$ and $F$ is the \emph{cdf} of $Z$.
			\[
				Z_n \xrightarrow{\textcolor{red}{d}} Z
			\]
			and $F$ is the \textbf{limit distribution} of $\{Z_n\}$.
		\end{definition}
		
		\begin{theorem}[Classical Central Limit Theorem]
			Let $\{X_1, \dots, X_n\}$ be a sequence of $n$ i.i.d. random variable, with 
			\begin{enumerate}
				\item $\expect{X_i} = \mu < \infty$ and
				\item $0 < Var(X_i) = \sigma^2 < \infty$
			\end{enumerate}
			Then as $n \to \infty$, the distribution of $\overline{X}_n \equiv \frac{\sum_{i=1}^n X_i}{n}$ converges to the \emph{normal distribution} with mean $\mu$ and variance $\frac{\sigma^2}{n}$, i.e.
			\[
				\overline{X}_n \xrightarrow{d} \mc{N}(\mu, \frac{\sigma^2}{\textcolor{red}{n}})
			\]
			irrespective of the shape of the original distribution of $X_i$.
		\end{theorem}
		\begin{corollary}
			\[
				\sqrt{n} \frac{(\overline{X}_n - \mu)}{\sigma} \xrightarrow{d} \mc{N}(0,1)
			\]
		\end{corollary}
		
		\begin{definition}
			Let $\{a_n\}$ be a sequence of \emph{deterministic} (i.e. non-random) real numbers. If 
			\[
				\forall \epsilon > 0\ \exists n^* \in \N \ s.t.\ |a_n - a| < \epsilon\ \forall n > n^*
			\]
			then $a_n$ \textbf{deterministically converges} to $a$ as $n \to \infty$. \\
			Equivalent notations:
			\begin{enumerate}
				\item $P(|a_n - a|>\epsilon) = 0,\ \forall n > n^*$
				\item $a_t \to a$
				\item $\lim(a_n) = 0$
			\end{enumerate}
		\end{definition}
		
		\begin{definition}
			Let $\{Z_n\}$ be a sequence of \emph{random} variables. If 
			\[
				\forall \epsilon, \delta > 0\ \exists n^* \in \N\ s.t.\ \forall n > n^*,\ P(|Z_n - a |>\epsilon) < \delta
			\]
			then $Z_n$ \textbf{converges in probability} to $a$ as $n \to \infty$. \\
			Equivalent notations:
			\begin{enumerate}
				\item $\lim_{n \to \infty} P(|Z_n - a| > \epsilon) = 0,\ \forall \epsilon > 0$
				\item $Z_n \xrightarrow{\textcolor{red}{p}}a$
				\item $plim(Z_n) = a$
			\end{enumerate}
		\end{definition}
		
		\begin{theorem}
			convergence in probability $\implies$ convergence in distribution. i.e.
			\[
				Z_n \xrightarrow{p} a \implies Z_n \xrightarrow{d} a
			\]
		\end{theorem}
		\begin{proof}
			
		\end{proof}
		
		\begin{theorem}[Law of Large Numbers]
			Let $\{X_1, \dots, X_n\}$ be a sequence of $n$ independent and iid random variables, with $\expect{X_i} = \mu < \infty$. Then
			\[
				plim_{n \to \infty}(\overline{X}_n) = \mu
			\]
		\end{theorem}
		
		\begin{theorem}[Continuous Mapping Theorem (Transformation Theorem) I]
			If $T_n \xrightarrow{p} a$ and $U_n \xrightarrow{p} b$, then
				\begin{gather*}
					(T_n + U_n) \xrightarrow{p} a + b \\
					T_n U_n \xrightarrow{p} ab \\
					\frac{T_n}{U_n} \xrightarrow{p} \frac{a}{b} \tx{ if } b \neq 0
				\end{gather*}
		\end{theorem}
		
		\begin{theorem}[Continuous Mapping Theorem (Transformation Theorem) II]
			If $Z_n \xrightarrow{d} Z$ and $U_n \xrightarrow{p} b$, where $Z$ is \emph{random variable}, then
			\begin{gather*}
				Z_n + U_n \xrightarrow{d} Z + b \\
				Z_n U_n \xrightarrow{d} Zb \\
				\frac{Z_n}{U_n} \xrightarrow{d} \frac{Z}{b} \tx{ if } P(U_n = 0) = 0 \land b \neq 0
			\end{gather*}
		\end{theorem}
		
		\subsection{Estimator Properties}
			\begin{definition}
				$\hat{\theta}_n$ is a \textbf{consistent} estimator of $\theta$ if and only if
				\[
					\hat{\theta}_n \xrightarrow{p} \theta
				\]
			\end{definition}
			
			\begin{definition}
				$\hat{\theta}_n$ is an \textbf{unbiased} estimator of $\theta$ if and only if
				\[
					\expect{\hat{\theta}_n} = \theta
				\]
			\end{definition}
			
			\begin{example}[Unbiased but inconsistent]
				Let $\{Z_n\}$ be a sequence of random variable, and
				\begin{gather*}
					\expect{Z_n} = \mu \\
					\lim_{n \to \infty} Var(Z_n) \neq 0
				\end{gather*}
			\end{example}
			
			\begin{example}[Biased but consistent]
				Let $\{Z_n\}$ be a sequence of random variable, and
				\begin{gather*}
					\expect{Z_n} = Z_n + \frac{c}{n},\ c \in \R \\
					plim(\hat{\theta}_n) = \theta
				\end{gather*}
				$Z_n$ is biased for small samples but consistent.
			\end{example}
			
			\begin{proposition}
				If $\hat{\theta}_n$ is an unbiased estimator for $\theta$ and $\lim_{n \to \infty}{\hat{\theta}_n} = 0$, then $\hat{\theta}_n$ is a consistent estimator for $\theta$, i.e.
				\[
					\expect{\hat{\theta}_n} = \theta \land \lim_{n \to \infty} Var(\theta_n) = 0 \implies plim(\hat{\theta}_n) = \theta
				\]
			\end{proposition}
		\subsection{OLS Consistency}
			\begin{remark}
				If MLR.6 holds, then the sampling distribution of OLSEs follows \emph{exact} normal distribution, even with finite sample size, $n < \infty$.
			\end{remark}
			
			\begin{assumption}[MLR.4']
				\begin{gather*}
					\expect{u} = 0 \\
					Cov(x_j, u) = 0,\ \forall j = 1,\dots,k
				\end{gather*}
			\end{assumption}
			\begin{proposition}
				Note that MLR.4 $\implies$ MLR.4', i.e. MLR.4 is an assumption stronger than MLR.4'.
				\begin{proof}
				\begin{gather*}
					\expect{u} = \expect{\expect{u|\textbf{x}}} \\
					= \expect{0} \tx{ by MLR.4} \\
					= 0 \\
					Cov(x_j, u) = \expect{x_j u} - \expect{x_j} \expect{u} \\
					= \expect{x_j u} \\
					= \expect{x_j \expect{u|\textbf{x}}} = 0
				\end{gather*}
				\end{proof}
			\end{proposition}
		\begin{theorem}[OLSE Consistency]
			Given assumption MLR.1 - MLR.4', the OLSE $\hat{\beta}$ is \textbf{consistent} for $\vec{\beta}$.
		\end{theorem}
		
		\begin{proof}
			\begin{gather*}
				\hat{\bm{\beta}} = (\bm{X}' \bm{X})^{-1} \bm{X}' \bm{y} \\
				= (\bm{X}' \bm{X})^{-1} \bm{X}' (\bm{X}\bm{\beta} + \bm{u}) \\
				= \bm{\beta} + (\bm{X}' \bm{X})^{-1} \bm{X}' \bm{u} \\
				= \bm{\beta} + (\frac{1}{n} \bm{X}' \bm{X})^{-1} \frac{1}{n} \bm{X}' \bm{u} \\
				= \bm{\beta} + (\frac{1}{n} \sum_{t=1}^n \bm{x}_t \bm{x}_t')^{-1} (\frac{1}{n} \sum_{t=1}^n \bm{x}_t u_t) 
			\end{gather*}
			By law of large number, 
			\begin{gather*}
				\frac{1}{n} \sum_{t=1}^n \bm{x}_t \bm{x}_t' \xrightarrow{p} \expect{\bm{x} \bm{x}'} \textcolor{red}{\equiv \bm{A}}
			\end{gather*}
			and 
			\begin{gather*}
				\frac{1}{n} \sum_{t=1}^n \bm{x}_t u_t \xrightarrow{p} \expect{\bm{x} u} = \bm{0}
			\end{gather*}
			So by continuous mapping theorem,
			\begin{gather*}
				\hat{\bm{\beta}} \xrightarrow{p} \bm{\beta} \bm{A}^{-1} \bm{0} = \bm{\beta}
			\end{gather*}
		\end{proof}
		
		\begin{remark}
			To prove the \textbf{unbiasedness}, we need MLR.4.
		\end{remark}
		
		\begin{remark}[On Omitted Variable Bias]
			Omitted variable bias also violates MLR.4'. So omitted variable bias is called an \textbf{inconsistency bias} or \textbf{asymptotic bias} in $\hat{\bm{\beta}}$.
		\end{remark}
		\begin{example}
			\[
				\tilde{\beta}_1 \xrightarrow{p} \beta_1 + \beta_2 \frac{Cov(x_1, x_2)}{Var(x_1)}
			\]
		\end{example}
		
	\subsection{Asymptotic Normality}
		\begin{theorem}
			Given assumptions MLR.1-MLR.5 or E.1-E.4, the OLSE, $\bm{\beta}$ is \textbf{asymptotically normal}, and
			\begin{equation}
				\textcolor{red}{
				\sqrt{n}(\hat{\bm{\beta}} - \bm{\beta}) \xrightarrow{p} \mc{N}(\textbf{0}, \sigma^2 \bm{A}^{-1})
				}
			\end{equation}
		\end{theorem}
			\begin{proof}
				From the results above in consistency proof, 
				\begin{gather*}
					\sqrt{n}(\hat{\bm{\beta}} - \bm{\beta}) = (\frac{1}{n} \sum_{t=1}^n \bm{x}_t \bm{x}_t')^{-1} \sqrt{n} (\frac{1}{n} \sum_{t=1}^n \bm{x}_t u_t) 
				\end{gather*}
				By law of large number, 
				\begin{gather*}
					(\frac{1}{n} \sum_{t=1}^n \bm{x}_t \bm{x}_t')^{-1} \xrightarrow{p} \bm{A}^{-1}
				\end{gather*}
				and let $i \neq j \in \{1, 2, \dots n\}$, investigate
				\begin{gather*}
					Cov(\bm{x}_i u_i, \bm{x}_j u_j) = \expect{\bm{x}_i u_i  \bm{x}_j u_j} \\
					= \expect{\bm{x}_i \bm{x}_j \expect{u_i u_j | \bm{x}_i \bm{x}_j}} \\
					= \expect{0} = 0
				\end{gather*}
				and for variance of $\bm{x}_t u_t$ for an arbitrary $t$,
				\begin{gather*}
					Var(\bm{x}_t u_t) = \expect{\bm{x}_t \bm{x}_t' u_t^2} \\
					= \expect{\expect{\bm{x}_t \bm{x}_t' u_t^2 | \bm{x}_t}} \\
					= \expect{\expect{u_t^2|\bm{x}_t} \bm{x}_t \bm{x}_t'} \\
					= \expect{\sigma^2 \bm{x}_t \bm{x}_t'} \\
					= \sigma^2 \expect{\bm{x}_t \bm{x}_t'} \\
					\equiv \sigma^2 \bm{A}
				\end{gather*}
				Therefore, all $\bm{x} u$ are \ul{independent} and we know $\expect{\bm{x} u} = \bm{0}$. Also by MLR.2, they are randomly drawn from population, so they are identically distributed. \\
				Therefore, by central limit theorem,
				\begin{gather}
					\frac{1}{n} \sum_{t=1}^n \bm{x}_t u_t \xrightarrow{d} \mc{N}(\bm{0}, \frac{\sigma^2 \bm{A}}{n}) \\
					\implies \sqrt{n} (\frac{1}{n} \sum_{t=1}^n \bm{x}_t u_t) \xrightarrow{d} \mc{N}(\bm{0}, \sigma^2 \bm{A})
				\end{gather}
				By continuous mapping theorem, $\hat{\bm{\beta}} - \bm{\beta}$ is also normally distributed, and has variance
				\begin{gather}
					Var(\sqrt{n} (\hat{\bm{\beta}} - \bm{\beta})) = \bm{A}^{-1} Var(\frac{1}{\sqrt{n}} \sum_{t=1}^n \bm{x}_t u_t) \bm{A}^{-1} \\
					= \sigma^2 \bm{A}^{-1}
				\end{gather}
				Therefore
				\begin{gather}
					\sqrt{n}(\hat{\bm{\beta}} - \bm{\beta}) \xrightarrow{p} \mc{N}(\bm{0}, \sigma^2 \bm{A}^{-1})
				\end{gather}
			\end{proof}
\end{document}





























