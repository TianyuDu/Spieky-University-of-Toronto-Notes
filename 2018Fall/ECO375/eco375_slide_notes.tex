\documentclass[]{article}
\title{ECO375 Applied Econometrics I \\ \small Lecture Slide Notes}
\author{Tianyu Du}
\date{\today}

\usepackage{spikey}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}


\counterwithin*{equation}{section}


\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}

\begin{document}
	\maketitle
	\doclicenseThis
	\textbf{Updated} version can be found on \url{www.tianyudu.com/notes}
	
	\tableofcontents
	\section{Slide 4: Simple \& Multiple Regression - Estimation}
	\subsection{Regression Model}
	\begin{assumption}
	    Assuming the population follows
	    \[
	        y = \beta_0 + \beta_1 x + u
	    \]
	    and assume that \emph{$x$ causes y}.
	\end{assumption}
	
	\subsection{OLS}
	\begin{gather*}
	    \min_{\vec{\beta}} \sum_{i} (y_i - \hat{y}_i)^2 \\
	    \tx{With FOC:} \\
	    \sum_{i}(y_i - \hat{y}_i) = 0 \\
	    \sum_{i}x_{ij}(y_i - \hat{y}_i) = 0,\ \forall j
	\end{gather*}
	
	\begin{remark}
	    Both $\hat{\beta}_0$ and $\hat{\beta}_j$ are functions of \emph{random variables} and therefore themselves \emph{random} with \emph{sampling distribution}. And the estimated coefficients are random up to random sample chosen.
	\end{remark}
	
	\begin{property}
	    Properties of OLS estimators
	    \begin{itemize}
	        \item \textbf{Unbiased} $\mathbb{E}[\hat{\beta} | X] = \beta$
	        \item \textbf{Consistent} $\hat{\beta} \to \beta$ as $n \to \infty$
	        \item \textbf{Efficient/Good} min variance.
	    \end{itemize}
	\end{property}
	
	\begin{definition}
	    The \textbf{Simple Coefficient of Determination}
	    \[
	        R^2 = \frac{SSE}{SST}
	    \]
	    and $SS\underline{Total} = SS\underline{Explained} + SS\underline{Residual}$
	    \[
	        \sum_i {(y_i - \overline{y})^2} = \sum_i {(\hat{y}_i - \overline{y})^2} + \sum_i {(y_i - \hat{y}_i)^2}
	    \]
	\end{definition}
	
	\begin{proposition}[Logarithms] Interpretation with logarithmic transformation.
	    \begin{itemize}
	        \item $\ln{y} = \alpha + \beta \ln{y} + u$: \ul{$x$ increases by $1\%$, $y$ increases by $\beta \%$}.
	        \item $\ln{y} = \alpha + \beta x + u$: \ul{$x$ increases by 1 unit, $y$ increases by $100 \beta \%$}.
	        \item $y = \alpha + \beta \ln{x} + u$: \ul{$x$ increases by $1\%$, $y$ increases by $0.01\beta$ unit.}
	    \end{itemize}
	\end{proposition}
	
	\begin{assumption}
	    Simple regression model assumptions
	    \begin{enumerate}
	        \item Model is \ul{linear} in parameter.
	        \item \ul{Random samples} $\{(x_i, y_i)\}_{i=1}^n$.
	        \item Sample outcomes $\{x_i\}_{i=1}^n$ are not the same.
	        \item $\mathbb{E}(u|x)=0$ conditional on random sample $x$.
	        \item Error is \ul{homoskedastic}. $Var(u|x) = \sigma^2$ for all $x$.
	    \end{enumerate}
	\end{assumption}
	\paragraph{Benefits of MLR compared with SLR}
	    \begin{itemize}
	        \item More accurate causal effect estimation.
	        \item More flexible function forms.
	        \item Could explicitly include more predictors so $\mathbb{E}(u|X) = 0$ is easier to be satisfied.
	        \item MLR4 is less restrictive than SLR4.
	    \end{itemize}
	   
    \begin{property}
        MLR OLS residual satisfies
        \begin{gather*}
            \sum_i {\hat{u_i}} = 0\\
            \sum_i {x_{ji} \hat{u_i}} = 0,\ \forall i \in \{1, 2, \dots, k\}
        \end{gather*}
    \end{property}
    
    \begin{property}
        MLR OLS estimators $\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_k$ pass through the average point.
        \[
            \overline{y} = \hat{\beta}_0 + \hat{\beta}_1 \overline{x}_1 + \dots + \hat{\beta}_k \overline{x}_k
        \]
    \end{property}
    \begin{proof}
    \end{proof}
    
    \subsection{Partialling Out}
    \subsubsection{Steps}
        \begin{enumerate}
            \item Regress $x_1$ on $x_2, x_3, \dots, x_K$ and calculate the residual $\widetilde{r}_1$. 
            \item Regress $y$ on $\widetilde{r}_1$ with simple regression and find the estimated coefficient $\hat{\lambda}_1$.
            \item Then the multiple regression coefficient estimator $\hat{\beta}_1$ is
            \[
                \hat{\beta}_1 = \hat{\lambda}_1 = \frac{\sum_{i}{y_i \widetilde{r}_{1i}}}{\sum_i {(\widetilde{r}_{1i})^2}}
            \]
        \end{enumerate}
        \begin{proof}
        \end{proof}
        
    \subsubsection{Interpretation}
    \par This OLS estimator only uses the \ul{unique variance} of one independent variable. And the parts of variation correlated with other independent variables is partialled out.
    
    \begin{assumption}Multiple Regression Assumptions
        \begin{enumerate}
            \item (MLR1) The model is \ul{linear} in parameters. 
            \item (MLR2) \ul{Random sample} from population $\{(x_{1i}, \dots x_{ki}, y_i\}_{i=1}^n$.
            \item (MLR3) No perfect \ul{multicollinearity}.
            \item (MLR4) \ul{Zero expected error} conditional on population slice given by $X$.
            \[
                \mathbb{E}(u|X) = \mathbb{E}(u|x_1, x_2, \dots, x_k) = 0
            \]
            \item (MLR5) \ul{Homoskedastic error} conditional on population slice given by $X$.
            \[
                Var(u|X) = \sigma^2
            \]
            \item (MLR6, \emph{strict assumption}) \ul{Normally distributed error}
            \[
                u \sim \mc{N}(0, \sigma^2)
            \]
        \end{enumerate}
    \end{assumption}
    
    \subsection{Omitted Variable Bias}
    \par Suppose population follows the \emph{real model} 
    \begin{equation}
        y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_k x_{ki} + u_i
    \end{equation}
    Consider the \emph{alternative model}, and \ul{$x_k$ is omitted}, which is assumed to be relevant.
    \begin{equation}
        y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \dots + \beta_{k-1} x_{(k-1)i} + r_i
    \end{equation}
    and use the partialling-out result on the second regression we have
    \[
        \tilde{\beta}_1 = \frac{\sum_i \tilde{r}_{1i} y_i}{( \tilde{r}_{1i})^2}
    \]
    where $\tilde{r_{1i}} = x_{1i} - \tilde{\alpha}_0 - \tilde{\alpha}_2 x_{2i} - \dots - \tilde{\alpha}_{k-1}x_{(k-1)i}$
    
    
    \begin{equation}
        \tilde{\beta}_1 = \hat{\beta}_1 + \hat{\beta}_k \frac{\sum (\tilde{r}_{1i} x_{ki})}{\sum (\tilde{r}_{1i})^2}
    \end{equation}
    and take the expectation 
    \begin{gather*}
        \mathbb{E}(\tilde{\beta}_1 | X) = \beta_1 + \tilde{\delta}_1 \beta_k \\
        Bias(\tilde{\beta}_1) = \tilde{\delta}_1 \beta_k
    \end{gather*}
    \paragraph{Conclusion} the sign of bias depends on $cov(x_1, x_k)$ and $\beta_k$.
    
    \begin{proof}
    	\textcolor{red}{TODO}
    \end{proof}
    
    \section{Slide 5: Matrix Algebra for Regression Analysis}
    	\begin{equation}
    		\textbf{y} = \textbf{A} \textbf{x} \implies \pd{\textbf{y}}{\textbf{x}} = \textbf{A}
    	\end{equation}
    	Let $\alpha = \textbf{y}' \textbf{A} \textbf{x}$, notice that $\alpha \in \R$, then 
    	\begin{gather}
    		\pd{\alpha}{\textbf{x}} = \textbf{y}' \textbf{A} \\
    		\pd{	\alpha}{\textbf{y}} = \textbf{x}' \textbf{A}'
    	\end{gather}
    	Consider special case $\alpha = \textbf{x}' \textbf{A} \textbf{x}$, then 
    	\begin{equation}
    		\pd{\alpha}{\textbf{x}} = \textbf{x}' \textbf{A} + \textbf{x}' \textbf{A}'
    	\end{equation}
    	and if $\textbf{A}$ is symmetric, 
    	\begin{equation}
    		\pd{\alpha}{\textbf{x}} = 2 \textbf{x}' \textbf{A}
    	\end{equation}
    
    \section{Slide 6: Multiple Regression in Matrix Algebra}
    	\subsection{The Model}
    		\paragraph{Predictor}
    			\[
    				\textbf{X} \in \mathbb{M}_{n \times (k+1)}(\R)
    			\]
    			where $n$ is the number of observations and $k$ is the number of features.
    			\[
    				\textbf{X} = \begin{bmatrix}
    					1 & x_{11} & \dots & x_{1k} \\
    					1 & x_{21} & \dots & x_{2k} \\
    					\vdots \\
    					1 & x_{n1} & \dots & x_{nk} \\
    				\end{bmatrix}_{n \times (k+1)}
    			\]
    		\paragraph{Model}
    			\[
    				\textbf{y} = \textbf{X} \vec{\beta} + \textbf{u}
    			\]
    		\paragraph{First order condition for OLS}
    			\begin{gather*}
    				\textbf{X}' \hat{u} = \textbf{0} \in \R^{k+1} \\
    				\iff \textbf{X}' (\textbf{y} - \textbf{X} \hat{\beta}) = \textbf{0} \in \R^{k+1}
    			\end{gather*}
    		\paragraph{Estimator}	
    			\[
    				\hat{\beta} = (\textbf{X}' \textbf{X})^{-1} \textbf{X}' \textbf{y}
    			\]
    			\begin{proof}
    				From the first order condition for the OLS estimator
    				\begin{gather*}
    					\textbf{X}' (\textbf{y} - \textbf{X} \hat{\beta}) = \textbf{0} \\
    					\implies \textbf{X}' \textbf{y} - \textbf{X}' \textbf{X} \hat{\beta} = \textbf{0} \\
    					\implies \textbf{X}' \textbf{y} = \textbf{X}' \textbf{X} \hat{\beta} \\
    					\implies \hat{\beta} = (\textbf{X}' \textbf{X})^{-1} \textbf{X}' \textbf{y}
    				\end{gather*}
    				and note that $(\textbf{X}' \textbf{X})$ is guaranteed to be invertible by assumption \emph{no perfect multi-collinearity}.
    			\end{proof}
    		\paragraph{Sum Squared Residual}
    		\[
    			SSR(\hat{\beta}) = \hat{u}' \cdot \hat{u} = (\textbf{y} - \textbf{X} \hat{\beta})' \cdot (\textbf{y} - \textbf{X} \hat{\beta})
    		\]
    		
    		\subsection{Variance Matrix}
    			\par Consider 
    			\begin{gather*}
    				\vec{z}_t = [z_{1t}, z_{2t}, \dots z_{nt}]' \\
    				\vec{z}_s = [z_{1s}, z_{2s}, \dots z_{ns}]'
    			\end{gather*}
    			Notice that the variance and covariance are defined as
    			\begin{gather*}
    				Var(\vec{z}_t) = \expect{(\vec{z}_t - \expect{\vec{z}_t})^2} \\
    				Cov(\vec{z}_t, \vec{z}_s) = \expect{(\vec{z}_t - \expect{\vec{z}_t})(\vec{z}_s - \expect{\vec{z}_s})}
    			\end{gather*}
    			The \textbf{variance matrix} of $\textbf{z} = [z_1, z_2, \dots, z_n]$ is given by
    			\begin{gather*}
    				Var(\textbf{z}) = \begin{bmatrix}
    					Var(z_1) & Cov(z_1, z_2) & \dots & Cov(z_1, z_n) \\
    					Cov(z_2, z_1) & \dots \\
    					\vdots \\
    					Cov(z_n, z_1) & \dots & \dots & Var(z_n) \\
    				\end{bmatrix} \\
    				= 
    				\begin{bmatrix}
    					\expect{(z_1 - \overline{z}_1)^2} & \expect{(z_1 - \overline{z}_1)(z_2 - \overline{z}_2)} & \dots \\
    					\expect{(z_2 - \overline{z}_2)(z_1 - \overline{z}_1)} & \dots \\
    					\vdots \\
    					\expect{(z_n - \overline{z}_n)(z_1 - \overline{z}_1)} & \dots & \expect{(z_n - \overline{z}_n)^2} \\
    				\end{bmatrix} \\
    				= \textcolor{red}{
    				\expect{(\textbf{z} - \expect{\textbf{z}})_{n \times 1} \cdot (\textbf{z} - \expect{\textbf{z}})'_{1 \times n}} \in \mathbb{M}_{n \times n}
    				}
    			\end{gather*}
    			In the special case $\expect{\vec{z}} = \vec{0}$, variance is reduced to
    			\[
    				\textcolor{red}{
    					Var(\textbf{z}) = \expect{\textbf{z} \cdot \textbf{z}'}
    				}
    			\]
    			\paragraph{Residual} Since residual $u_i$ are \emph{i.i.d} with variance $\sigma^2$, the variance matrix of $\textbf{u}$ is
    			\[\textcolor{red}{
    				Var(\textbf{u}) = \expect{\textbf{u} \cdot \textbf{u}'} = \sigma^2 \textbf{I}_n
    			}\]
    			
    			\paragraph{Estimator} If $\hat{\beta}$ is unbiased, $\expect{\hat{\beta} | \textbf{X}} = \vec{\beta}$, then 
    			\[\textcolor{red}{
    				Var(\hat{\beta}|\textbf{X}) = \expect{(\hat{\beta} - \vec{\beta})\cdot(\hat{\beta} - \vec{\beta})' | \textbf{X}} \in \mathbb{M}_{(k+1) \times (k+1)}
    			}\]
    \section{Slide 7: Multiple Regression - Properties}
    	\subsection{Assumptions (MLRs) in Matrix Form}
    		\paragraph{E.1.} \emph{linear in parameter}
    		\[
    			\textbf{y} = \textbf{X}\vec{\beta} + \textbf{u}
    		\]
    		\paragraph{E.2.} \emph{no perfect multi-collinearity}
    		\[
    			rank(\textbf{X}) = k +1
    		\]
    		\paragraph{E.3.} Error has expected value of \textbf{0} conditional on \textbf{X}.
    		\[
    			\expect{\textbf{u}|\textbf{X}} = \textbf{0}
    		\]
    		\paragraph{E.4.} Error \textbf{u} is \emph{homoscedastic}.
    		\[
    			Var(\textbf{u}|\textbf{X}) = \sigma^2 \textbf{I}_n
    		\]
    		\paragraph{E.5.} \emph{Normally distributed} error \textbf{u}. Note that this assumption is relatively strong.
    		\[
    			\textbf{u} \sim \mc{N}(\textbf{0}, \sigma^2 \textbf{I}_n)
    		\]
    	\subsection{Properties of OLS Estimator}
    	\begin{theorem}
    		Given \emph{E.1. E.2. E.3.}, the OLS estimator $\hat{\beta}$ is an unbiased estimator for $\vec{\beta}$.
    		\[
    			\expect{\hat{\beta}|\textbf{X}} = \vec{\beta}
    		\]
    	\end{theorem}
    	\begin{proof}
    		\begin{gather*}
    			\hat{\beta} = (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{y} \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' (\textbf{X}\vec{\beta} + \textbf{u}) \\
    			= \vec{\beta} + (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{u} \\
    			\tx{Taking expectation conditional on \textbf{X} on both sides, }\\
    			\expect{\hat{\beta}|\textbf{X}} = \vec{\beta} + (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{0} 
    			= \vec{\beta}
    		\end{gather*}
    	\end{proof}
    	
    	\begin{lemma}
    		Suppose $\textbf{A} \in \mathbb{M}_{m\times n}$ and $\textbf{z} \in \mathbb{M}_{n\times 1}$ then 
    		\[
    			Var(\textbf{A} \textbf{z}) = \textbf{A} Var(\textbf{z}) \textbf{A}'
    		\]
    	\end{lemma}
    	\begin{theorem}
    		Given $E.1 \sim E.4$
    		\[
    			Var(\hat{\beta}|\textbf{X}) = (\textbf{X}' \textbf{X})^{-1} \sigma^2
    		\]
    	\end{theorem}
    	\begin{proof}
    		\begin{gather*}
    			Var(\hat{\beta}|\textbf{X}) = Var((\textbf{X}'\textbf{X})^{-1} \textbf{X}' \textbf{y} | \textbf{X}) \\ 
    			= Var((\textbf{X}'\textbf{X})^{-1} \textbf{X}'(\textbf{X}\vec{\beta} + \textbf{u})|\textbf{X}) \\
    			= Var(\textbf{X}'\textbf{X})^{-1} \textbf{X}'\textbf{u}|\textbf{X}) \\
    			\tx{By the lemma above, } \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' Var(\textbf{u}|\textbf{X}) [(\textbf{X}'\textbf{X})^{-1} \textbf{X}']' \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' Var(\textbf{u}|\textbf{X}) \textbf{X}'' (\textbf{X}'\textbf{X})^{-1} \\
    			= (\textbf{X}'\textbf{X})^{-1} \textbf{X}' \sigma^2 \textbf{I}_n \textbf{X} (\textbf{X}'\textbf{X})^{-1} \\
    			= \sigma^2 (\textbf{X}'\textbf{X})^{-1}
    		\end{gather*}
    	\end{proof}
    	
    	\begin{theorem}[Gause-Markov]
    		Given $E.1. \sim E.4.$, the OLS estimator is the \ul{best linear unbiased estimator}(BLUE). \\(\emph{The best} here means the OLS has the least variance among all estimators.)
    	\end{theorem}
    	
    	\subsection{Variance Inflation}
    	\par Let $j \in \{1, 2, \dots, k\}$, then the variance of an individual estimator on particular feature $j$ is 
    	\[\textcolor{red}{
    		Var(\hat{\beta}_j) = \frac{\sigma^2}{(1-R_j^2)SST_j}
    	}\]
    	where
    	\begin{gather*}
    		SST_j = \sum_{i=1}^n{(x_{ij} - \overline{x}_j)^2}
    	\end{gather*}
    	and $R_j^2$ is the coefficient of determination while regressing $x_j$ on \ul{all other} features $x_i, \forall i \neq j$.
    	\begin{definition}
    		The \textbf{variance inflation} on estimator for feature $j$ is 
    		\[
    			VIF_j = \frac{1}{1-R_j^2}
    		\]
    	\end{definition}
    	\begin{remark}[Interpretation] the standard error of estimator on a particular variable ($\hat{\beta}_j$) is \emph{inflated} by it's($x_j$) relationship with other explanatory variables.
    	\end{remark}	

    	\paragraph{Solutions to high VIF}
    		\begin{enumerate}
    			\item Drop the explanatory variable.
    			\item Use ratio $\frac{x_i}{x_j}$ instead.
    			\item Ridge regression.
    		\end{enumerate}
    	\begin{remark}
    		VIF highlights the importantce of \textbf{not} including redundant predictors.
    	\end{remark}
    	
    \section{Slide 8: Multiple Regression - Inference}
    \paragraph{Hypothesis Testing} on multiple regression model 
    	\[
    		y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots \beta_k x_{ik} + u_i
    	\]
    	
    	\subsection{t-test for significance of individual predicator}
    		\paragraph{Test statistic} Given $MLR.1 \sim MLR.6$ (need $\textbf{u} \sim \mc{N}(\textbf{0}, \sigma^2 \textbf{I}_n)$),
    		\[
    			t = \frac{\hat{\beta}_j - b}{s.e.(\hat{\beta}_j)} \sim t_{n-k-1}
    		\]
    		where 
    		\begin{gather*}
    			H_0: \beta_j = b \\
    			H_1: \beta_j (\neq, >, <) b
    		\end{gather*}
    	
    	
    	\subsection{t-test for comparing 2 coefficients}
    	\paragraph{Test statistic}
    	\[
    		t = \frac{(\hat{\beta}_i - \hat{\beta}_j) - b}{s.e.(\hat{\beta}_i - \hat{\beta}_j)} \sim t_{n-k-1}
    	\]
    	where 
    		\begin{gather*}
    			H_0: \beta_i - \beta_j = b \\
    			H_1: \beta_i - \beta_j (\neq, >, <) b
    		\end{gather*}
    	notice
    	\begin{gather*}
    		s.e.(\hat{\beta}_i - \hat{\beta}_j) = \sqrt{Var(\hat{\beta}_i - \hat{\beta}_j)} \\
    		= \sqrt{Var(\hat{\beta}_i) + Var(\hat{\beta}_j) - 2Cov(\hat{\beta}_i, \hat{\beta}_j)}
    	\end{gather*}
    	
    	\subsection{Partial F-test for joint significance}
    	\begin{gather*}
    		H_0: \beta_i = \beta_j = \beta_k = \dots = 0 \\
    		H_1: \exists\ z \in \{i, j, k, \dots \}\ s.t.\ \beta_z \neq 0
    	\end{gather*}
    	Test significance by comparing the \emph{restricted} and \emph{unrestricted} models, see whether restricting the model by removing certain explanatory variables "significantly" hurts the fit of the model.
    	\[
    		df = (q, n-k-1)
    	\]
    	\paragraph{Test statistic}
    	\begin{gather*}
    		F = \frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)} \sim F_{(q, n-k-1)} \\
    		\tx{or} \\
    		F' = \frac{(R^2_{ur} - R^2_r)/q}{(1-R^2_{ur})/(n-k-1)} \sim F_{(q, n-k-1)}
    	\end{gather*}
    	
    	\subsection{Full F-test for the significance of the model}
    	\begin{gather*}
    		H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0 \\
    		H_1: \exists\ i \in \{1, 2, \dots ,3\}\ s.t.\ \beta_i \neq 0
    	\end{gather*}
    	\begin{remark}
    		$R^2$ version only and substitute $R^2_{r} = 0$, since $SSR_{r}$ is undefined.
    	\end{remark}
    	\paragraph{Test statistic}
    	\[
    		F = \frac{R^2_{ur}/k}{(1-R^2_{ur})/(n-k-1)} \sim F_{(k, n-k-1)}
    	\]
    	
    	\subsection{F-test for general restrictions}
    	\begin{remark}
    		Use the $SSR$ version of $Fstatistic$ only since the $SST$ for restricted and unrestricted models are different.
    	\end{remark}
    	\begin{remark}
    		We only reject or failed to reject $H_0$, we never accept $H_0$ in a hypothesis test.
    	\end{remark}
    	
    \section{Slide 9: Multiple Regression - Further Issues}
    	\subsection{Data Scaling}
    		\subsubsection{Mutiplier}
    		\begin{enumerate}
    			\item Enlarge $x_j$ by factor $a$: $\hat{\beta}_j$ shrinks by $a$.
    			\item Enlarge $y$ by factor $a$: \textbf{all} $\hat{\beta}_i$ enlarged by $a$.
    			\item \textcolor{red}{Test statistic $t = \frac{\hat{\beta}}{s.e.(\hat{\beta})} = \frac{a\hat{\beta}}{s.e.(a \hat{\beta})}$ is unaffected.}
    		\end{enumerate}
    	
    		\subsubsection{Standardization}
    			\paragraph{Standardized variable} For $j^{th}$ observation of explanatory variable $x$, 
    				\[
    					z_j = \frac{x_j - \overline{x}}{\sigma_{x}}
    				\]
    				which satisfies
    				\[
    					\expect{z_j} = 0,\ Var(z_j) = 1
    				\]
    			\paragraph{Properties} Consider model and find the estimator of regressing standardized $y$ on standardized $x$.
    				\[
    					y_i = \hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \dots + \hat{\beta}_k x_{ik} + \hat{u}_i
    				\]
    				Since OLS estimator passes through the mean,
    				\begin{gather*}
    					\overline{y} = \hat{\beta}_0 + \hat{\beta}_1 \overline{x}_1 + \dots \hat{\beta}_k \overline{x}_k \\
    					\implies (y_i - \overline{y}) = \hat{\beta}_1 (x_{i1} - \overline{x}_1) + \dots + \hat{\beta}_k (x_{ik} - \overline{x}_k) + \hat{u}_i \\
    					\implies \frac{y_i - \overline{y}}{\sigma_y} = 
    					\frac{\hat{\beta}_1 \sigma_{x_1}}{\sigma_y} \frac{x_{i1} - \overline{x}_1}{\sigma_{x_1}} + \dots + 
    					\frac{\hat{\beta}_k \sigma_{x_k}}{\sigma_y} \frac{x_{ik} - \overline{x}_k}{\sigma_{x_k}} + \frac{\hat{u}_i}{\sigma_y} \\
    					\implies b_j = \frac{\hat{\beta}_j \sigma_{x_j}}{\sigma_y}
    				\end{gather*}
    			\begin{remark}[Interpretation]
    				$x_j$ increases by 1 \textbf{std}, y increases by $b_j = \frac{\hat{\beta}_j \sigma_{x_j}}{\sigma_y}$ \textbf{std}, \emph{ceteris paribus}.
    			\end{remark}
    	\subsection{Logarithmic Function}
    		\par \textbf{Exact} interpretation of log transformation.
    			\[
    				\ln(y_i) = \hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \dots \hat{\beta}_k x_{ik} + \hat{u}_i
    			\]
    			\begin{proof}[Derive]
	    			\begin{gather*}
	    				\ln(y_2) - \ln(y_1) = \hat{\beta}_j \Delta x_j \\
	    				\implies \ln(\frac{y_2}{y_1}) = \hat{\beta}_j \Delta x_j \\
	    				\implies \frac{y_2}{y_1} = exp(\hat{\beta}_j \Delta x_j) \\
	    				\implies \frac{y_2 - y_1}{y_1} 
	    				= \frac{y_2}{y_1} - 1 \\
	    				\implies \%\Delta y= exp(\hat{\beta}_j \Delta x_j) - 1
	    			\end{gather*}
    			\end{proof}
    	\subsection{Quadratics and Polynomials}
    		\paragraph{Model}
    			\[
    				y_i = \sum_{p=0}^k {\beta_p x_{i}^p} + u_i
    			\]
    		\begin{remark}
    			Consider the \textbf{interpretation} and \textbf{turning points}.
    		\end{remark}
    	\subsection{Interaction Effects}
    		\par Consider model
    		\[
    			y = \beta_0 + \beta_1 x + \beta_2 z + \beta_3 xz + u
    		\]
    		then 
    		\[
    			\pd{y}{x} = \beta_1 + \beta_3 z
    		\]
    		\begin{enumerate}
    			\item The effects of change of $x$ on $y$ depends on $z$.
    			\item Interpretation: \emph{evaluate} $\pd{y}{x}$ at a $z$ point that we are interested in.
    			\item Use \emph{conventional testing} (t-test) to check if interaction term is significant.
    		\end{enumerate}
    	\subsection{Regression Selection and Adjusted R-square}
    		\par The adjusted R-square, $\overline{R^2}$, incorporates a \emph{penalty} for including more regressors (if insignificant).
    		\[
    			\overline{R^2} = 1 - \frac{(1-R^2)(n-1)}{n-k-1}
    		\]
    		\begin{remark}
    			$\overline{R^2}$ increases when adding new regressor(or a group of regressors) if and only if the $t$ value ($F$) for the individual regression(group of regressors) is more than 1.
    		\end{remark}
    	
    	\subsection{Causal Mechanism}
    	
    	\subsection{Confidence Interval for Prediction}
    		\par Consider a prediction 
    		\[
    			\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \dots \hat{\beta}_k x_k
    		\]
    		Evaluate at an arbitrary data point (not necessarily an observation in sample)
    		\[
    			\textbf{c} = (c_1, c_2, \dots ,c_k)
    		\]
    		Then the estimation of $y$ at \textbf{c} is 
    		\begin{gather*}
    			\theta_0 = \expect{y|x_1=c_1, x_2=c_2, \dots x_k=c_k} \\
    			= \beta_0 + \beta_1 c_1 + \beta_2 c_2 + \dots + \beta_k c_k \\
    			\implies \beta_0 = \theta_0 - \beta_1 c_1 - \beta_2 c_2 - \dots - \beta_k c_k
    		\end{gather*}
    		substitute back into the model
    		\[
    			y = \theta_0 + \beta_1 (x_1 - c_1) + \beta_2 (x_2 - c_2) + \dots + \beta_k x_k + u
    		\]
    		And the \ul{margin of error} of confidence interval of prediction of $y$ at \textbf{c} can be found by inspecting the \ul{intercept} on above regression.
    		\[
    			ME = t_{\frac{\alpha}{2}} \times s.e. (intercept)
    		\]
      		The \ul{center} of confidence interval can be found from 
    		\[
    			\hat{\theta}_0 = \hat{\beta}_0 + \hat{\beta}_1 c_1 + \dots + \hat{\beta}_k x_k
      		\]
      		The $\alpha$ confidence interval is given by
      		\[
      			\hat{\theta}_0 \pm ME
      		\]
      		
	\section{Slide 10: Multiple Regression - Qualitative Information}
		\subsection{Binary predictors}
			\begin{remark}
				With binary independent variables, $MLR.1 \sim MLR.6$ still holds, but the interpretations are different.
			\end{remark}
			
			\subsubsection{On Intercept}
			\[
				y = \delta_0 + \delta_1 male + \dots + u
			\]
			\begin{remark}
				To avoid perfect multi-collinearity, never include all categories.
			\end{remark}
			
			\subsubsection{On Slopes}
			\[
				y = \delta_0 + (\delta_1 + \delta_2 male)\times education + \dots + u
			\]
			
			\subsubsection{F-test(Chow test)}
			\par Test whether the \ul{true coefficients} in 2 linear regression models (e.g. for different gender groups) are equal. 
			\begin{enumerate}
				\item Restricted model ($SSR_r$) \[y = \beta_0 + \beta_1 x + u \]
				\item Unrestricted model ($SSR_{ur}$) \[y = (\beta_0 + \delta_0 indicator) + (\beta_1 + \delta_1 indicator) x + u\]
				\item Test whether the additional factors in coefficients ($\delta_0, \delta_1$) are significant. ($q=2$ in this case)
				\[
					F = \frac{(SSR_{r} - SSR_{ur})/q}{SSR_{ur}/(n-k-1)}
				\]
			\end{enumerate}
		\subsection{Linear Probability Model}
			\emph{Qualitative binary dependent variable}
			\[
				y = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k + u,\ y \in \{0, 1\}
			\]
			\paragraph{Interpretation} the model above predicts the probability of $y=1$.
				\begin{proof}
					\begin{gather*}
						\expect{y|\textbf{x}} = 0 \times Pr(y=0|\textbf{x}) + 1 \times Pr(y=1|\textbf{x}) \\
						= Pr(y=1|\textbf{x})
					\end{gather*}
				\end{proof}
			\begin{remark}
				$\beta_j = \pd{P(\textbf{x})}{x_j}$ is the \textbf{response probability}, and $\hat{P}(\textbf{x})$ is the \textbf{predicted probability} of $y$ to be 1.
			\end{remark}
			\begin{remark}[Out-of-range predictions]
				Notice the prediction is not necessarily with the range of $[0,1]$ for some extreme values of \textbf{x}.
			\end{remark}
		\subsection{Heterskedasticity of LPM}
			\begin{remark}
				For probability linear models, $MLR.5$(homoskedasticity) fails.
			\end{remark}
			\begin{proof}
				\begin{gather*}
					y_i = \beta_0 + \beta_1 x_{i1} + \dots \beta_k x_{ik} + u_i \\
					\tx{For binary $y$} \\
					\textcolor{red}{
						Var(u) = Var(y) = Pr(y=1) (1-Pr(y=1))
					} \\
					Var(u|\textbf{x}) = Var(y - \beta_0 - \beta_1 x_1 - \beta_2 x_2 - \dots - \beta_k x_k | \textbf{x}) \\
					= Var(y | \textbf{x}) \\
					= Pr(y=1|\textbf{x}) (1-Pr(y=1|\textbf{x})) \\
					= \expect{y | \textbf{x}} (1 - \expect{y | \textbf{x}}) \\
					= (\beta_0 + \beta_1 x_1 + \dots + \beta_k x_k) (1 - \beta_0 - \beta_1 x_1 - \dots - \beta_k x_k) \\
					\neq \sigma_u^2
				\end{gather*}
			\end{proof}
	\section{Slide 11: Heteroskedasticity}
		\begin{definition}
			Consider model 
			\[
				y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + u_i
			\]
			the error of above model is heteroskedastic if for each sample point $\textbf{x}_i \in \R^{k+1}$, 
			\[
				Var(u_i | \textbf{x}_i) = \sigma_i^2
			\]
			and $\sigma_i^2$ is not the same for all $i$.
		\end{definition}
		\begin{remark}[Consequence]
			Without $MLR.5$, Gauss-Markov theorem does not hold and 
			\begin{enumerate}
				\item OLS estimator is still \ul{linear} and \ul{unbiased}.
				\item But \textbf{not} necessarily the best (variance is affected).
			\end{enumerate}
		\end{remark}
		\begin{proof}[Proof. unbiasedness, in simple regression]
			\begin{gather*}
				\hat{\beta}_1 = \frac{\sum_i(x_i - \overline{x})(y_i - \overline{y})}{\sum_i(x_i - \overline{x})^2} \\
				= \frac{\sum_i(x_i - \overline{x})(\beta_0 + \beta_1 x_1 + u_i - \overline{y})}{\sum_i(x_i - \overline{x})^2} \\
				= \frac{\sum_i(x_i - \overline{x})(\beta_0 + \beta_1 x_1 + \beta_1 \overline{x} - \beta_1 \overline{x} + u_i - \overline{y})}{\sum_i(x_i - \overline{x})^2} \\
				= \frac{\sum_i \beta_1 (x_i - \overline{x})^2 + (x_i - \overline{x}) (\beta_0 + \beta_1 \overline{x} - \overline{y} + u_i)}{\sum_i(x_i - \overline{x})^2} \\
				= \beta_1 + \frac{\sum_i (x_i - \overline{x}) (0 + u_i)}{\sum_i (x_i - \overline{x})^2} \\
				= \beta_1 + \frac{\sum_i (x_i - \overline{x})u_i}{\sum_i (x_i - \overline{x})^2} \\
				\tx{taking expectation conditional on \textbf{x} on both sides} \\
				\expect{\hat{\beta}_1|\textbf{x}} = \beta_1 
			\end{gather*}
		\end{proof}
		
		\begin{proof}[Proof. variance]
			\begin{gather*}
				Var(\hat{\beta}_1 | \textbf{x}) = \expect{(\hat{\beta} - \expect{\hat{\beta}_1 | \textbf{x}})^2 | \textbf{x}} \\
				= \expect{(\hat{\beta}_1 - \beta_1)^2 | \textbf{x}} \\
				= \expect{(\frac{\sum_i (x_i - \overline{x})u_i}{\sum_i (x_i - \overline{x})^2})^2|\textbf{x}} \\
				= \frac{\sum_i(x_i - \overline{x}) \expect{u_i | \textbf{x}}}{\Big( \sum_i (x_i - \overline{x})^2 \Big)^2} \\
				\neq \frac{\sigma^2}{SST_x}
			\end{gather*}
			For multiple regressions
			\begin{gather*}
				Var(\hat{\beta}_j | \textbf{x}) = \frac{\sum_i{\tilde{r}_{ij}^2 \sigma_i^2}}{SSR_j^{\textcolor{red}{2}}} \neq \frac{\sigma^2}{SSR_j} = \frac{\sigma}{(1-R^2_j)SST_j}
			\end{gather*}
		\end{proof}
		
		\paragraph{Remedies}
		\begin{enumerate}
			\item Change variables so that the new model is homoskedastic.
			\item Use robust standard errors.
			\item Generalized least square (GLS).
		\end{enumerate}
		
		\subsection{Robust Standard Errors}
			\paragraph{Idea} use $\hat{u}_i^2$ to estimate $\sigma_i^2$.\\
			Note that  
			\begin{gather*}
				Var(u_i | \textbf{x}) = \expect{(u_i - \expect{u_i})^2} \\
				= \expect{u_i^2 | \textbf{x}} + \expect{u_i | \textbf{x}}^2 \\
				= \expect{u_i^2 | \textbf{x}}
			\end{gather*}
			Consider model 
			\[
				y_i = \beta_0 + \beta_1 x_i + u_i
			\]
			OLS estimator is 
			\begin{gather*}
				\hat{\beta}_1 = \beta_1 + \frac{\sum_i (x_i - \overline{x})u_i}{\sum_i (x_i - \overline{x})^2} \\
				Var(\hat{\beta} | \textbf{x}) = \frac{\sum_i(x_i - \overline{x})^2 \sigma_i^2}{\sum_i(x_i - \overline{x})^2} \\
				\widehat{Var}(\hat{\beta} | \textbf{x}) = \frac{\sum_i(x_i - \overline{x})^2 \textcolor{red}{\hat{u}_i^2}}{\sum_i(x_i - \overline{x})^2}
			\end{gather*}
		\subsection{Test for Heteroskedasticity}
		\subsubsection{General Principle}
			\begin{gather*}
				H_0: \expect{u_i^2} = Var(u_i|\textbf{x}) = \sigma^2 \tx{ (Homoskedastic)} \\
				H_1: \expect{u_i^2} = Var(u_i|\textbf{x}) = \sigma_i^2 \tx{ (Heteroskedastic)}
			\end{gather*}
			\textbf{Methodology:} specify the variance in alternative hypothesis to be a specific function of $\textbf{x}$ or $y$. \\
			Consider the model: 
			\[
				y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_k x_{ik} + u_i
			\]
			And $H_1$ can be expressed as 
			\[
				H_1: \expect{u_i^2|\textbf{x}} = \delta_0 + \delta_1 z_1 + \delta_2 z_2 + \dots + \delta_p z_p
			\]
			then run the proxy hypothesis testing
			\begin{gather*}
				H_0': \delta_1 = \delta_2 = \dots = \delta_p = 0, \delta_0 = \sigma^2 \\
				H_1': \exists j\ s.t.\ \delta_j \neq 0
			\end{gather*}
			Note that the restricted model is homoskedastic. \\
			Firstly run the original regression model and get residual $\hat{u}_i$. \\
			Then test the proxy hypotheses with regression $\hat{u}_i^2$ on $z_1, z_2, \dots, z_p$ using full F-test.
			\begin{gather*}
				F = \frac{R^2_{\hat{u}^2}/p}{(1-R^2_{\hat{u}^2})/(n-p-1)} \sim F_{(p, n-p-1)} \\
				 \tx{and }n R^2_{\hat{u}^2} \sim \mc{X}_p^2
			\end{gather*}
			
			\subsubsection{Breusch-Pagan test}
				\par Use regressors $x_i$ for $z_i$. \\
				Auxiliary regression:
				\begin{gather*}
					\hat{u}_i^2 = \delta_0 + \delta_1 x_1 + \dots \delta_k x_k \\
					n R^2_{\hat{u}^2} \sim \mc{X}_k^2
				\end{gather*}
			\subsubsection{White test version 1}
				\par Use polynomials of $x_i$ for $z_i$.\\
				Auxiliary regression: (for the case of 2 regressors)
				\begin{gather*}
					\hat{u}_i^2 = \delta_0 + \delta_{i1} x_1 + \delta_2 x_{i2} + \delta_3 x_{i1}^2 + \delta_4 x_{i2}^2 + \delta_5 x_{i1} x_{i2} + \epsilon \\
					n R^2_{\hat{u}^2} \sim \mc{X}_5^2 \\
					\tx{or full F-test}
				\end{gather*}
			\subsubsection{White test version 2}
				\par Use \ul{predicted} response $\hat{y}$ (since its a linear combination of predictors) and its polynomial as $z_i$. \\
				Auxiliary regression:
				\begin{gather*}
					\hat{u}_i^2 = \delta_0 + \delta_1 \hat{y} + \delta_2 \hat{y}^2 + \epsilon \\
				\end{gather*}
				With hypotheses
				\begin{gather*}
					H_0: \delta_1 = \delta_2 = 0\\
					H_1: \delta_1 \neq 0 \lor \delta_2 \neq 0
				\end{gather*}
				\begin{gather*}
					n R^2_{\hat{u}^2} \sim \mc{X}_2^2 \\
					\tx{or full F-test}
				\end{gather*}
	
	\section{Slide 12: Specification and Data Problems}
		\paragraph{} A multiple regression model suffers from functional misspecification when it does not properly account for the relationship between the dependent and the observed explanatory variables.
		\subsection{Regression Specification Error Test (RESET)}
		\subsubsection{RESET: Nested Alternatives}
			\par \emph{Adding nonlinear functions of the regressors into the model and test for their significance.}
			\\
			Consider model 
			\begin{equation}
				y = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k + u
			\end{equation}
			If the original model satisfies MLR.4 ($\expect{u|\textbf{X}}=0$), then \textbf{no} nonlinear functions of the independent variables should be significant when added to equation (1). 
			
			\paragraph{Procedures}
			\begin{enumerate}
				\item Add polynomials in the OLS fitted values, $\hat{y}$, to equation (1). Typically squared and cubed terms are added.
					\begin{equation}
						y = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k + \delta_1 \textcolor{red}{\hat{y}^2} + \delta_2 \textcolor{red}{\hat{y}^3} + u
					\end{equation}
				\item Use F-test to test the joint significance with $H_0: \delta_1 = \delta_2 = 0$. And \hl{a significant $F$ suggests some sort of functional form problem.}
					\[
						F \sim \mc{F}_{(2, n-k-2)}
					\]
			\end{enumerate}
			\begin{remark}
				We will not be interested in the estimated parameters from (2); we only use this equation to test whether (1) has missed important non-linearities.
			\end{remark}
			
			
			\begin{remark}[Nested Alternatives]
				One model is \textbf{nested} in another if you can always obtain the first model by constraining some of the parameters of the second model.
			\end{remark}
			\begin{example}
				In above example, the original regression is \emph{nested} in the expanded regression. We can recover the original regression by constraining $\delta_1 = \delta_2 = 0$ in the expanded model.
			\end{example}
			
			
		\subsubsection{Non-nested Alternatives: RESET}
			\par Neither of the two models below is nested in the other one, \hl{we \textbf{cannot} use F-test}.
			\begin{gather}
				y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u \\
				y = \beta_0 + \beta_1 \log(x_1) + \beta_2 \log(x_2) + u
			\end{gather}
			\paragraph{Procedures}
			\begin{enumerate}
				\item Construct a \emph{comprehensive model} that contains each model as a special case and then to test the restrictions that led to each of the models.
				\begin{equation}
					y = \beta_0 + \gamma_1 x_1 + \gamma_2 x_2 + \gamma_3 \log(x_1) + \gamma_4 \log(x_4) + u
				\end{equation}
				\item Test competing specifications
				\begin{enumerate}
					\item (F) test for specification (4): $H_0: \gamma_1 = \gamma_2 = 0$.
					\item (F) test for specification (3): $H_0: \gamma_3 = \gamma_4 = 0$.
				\end{enumerate}
			\end{enumerate}
			
			
		\subsubsection{Non-nested alternatives: Davidson-MacKinnon test}
			\par Let $\hat{y}_3$ and $\hat{y}_4$ denote the fitted values from (3) and (4) respectively. \\
			If model (3) holds with $\expect{u|x_1, x_2}=0$, the \hl{fitted values} from the other model, (4), should be insignificant when added to equation (3).
			
			\paragraph{Procedures}
			\begin{enumerate}
				\item Test for specification (3) with $H_0: \theta_1=0,\ H_1: \theta_1 \neq 0$.
					\begin{equation}
						y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \theta_1 \hat{y}_{\textcolor{red}{4}} + u
					\end{equation}
				\item Test for specification (4) with $H_0: \theta_1=0,\ H_1: \theta_1 \neq 0$. \\ \hl{A significant $t$ statistic (against a two-sided alternative) is a rejection of (4)}.
					\begin{equation}
						y = \beta_0 + \beta_1 \log(x_1) + \beta_2 \log(x_2) + \theta_1 \hat{y}_{\textcolor{red}{3}} + u
					\end{equation}
			\end{enumerate}
			
			\begin{remark}[Porblems] \quad
				\begin{enumerate}
					\item In Davison-MacKinnon test, its possible for us to reject or accept both specifications.
					\begin{enumerate}
						\item If neither rejected, use adjusted R-square to choose one model.
						\item If both rejected, find another alternative.
					\end{enumerate}
					\item Note that a rejection of (3) does not mean (4) is the correct model.
					\item The case when competing models have \ul{different dependent variables} could be problematic. ($y = \dots $ against $\log(y) = \dots$)
				\end{enumerate}
			\end{remark}
		\subsection{Proxy Variables}
			\subsubsection{Procedures}
				\par For the original model
				\begin{equation}
					y = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k^{*} + u
				\end{equation}
				where $x_k^{*}$ is unobserved. \\
				
				\paragraph{(1)Select proxy} Choose an observed variable $x_k$ is a \textbf{proxy} for $x_k^{uob}$ such that
				
				\begin{equation}
					x_k^{*} = \delta_0 + \delta_k x_k + v_3
				\end{equation}
				\begin{assumption}
					Typically we want $\delta_k > 0$, and no restriction on $\delta_0$.
				\end{assumption}
				
				
				\paragraph{(2)Plug-in solution to the omitted variables problem} directly replace $x^*_k$ with $\delta_0 + \delta_k x_k + v_3$
				\begin{equation}
					y = (\beta_0 + \beta_k \delta_0) + \beta_1 x_1 + \dots + \textcolor{red}{\beta_k \delta_k x_k} + (u + \beta_k v)
				\end{equation}
				\begin{assumption}
					For a consistent estimator, we need to assume that
					\begin{enumerate}
						\item $u$ is uncorrelated with $x_1, x_2, \dots, x_k^{*}, x_k$. 
						\item $v$ is uncorrelated with $x_1, x_2, \dots, x_k$.
						\[
							\expect{x_k^{*}|x_1,x_2,\dots,x_k} = \expect{\delta_0 + \delta_k x_k + v|x_1,x_2,\dots,x_k} = \delta_0 + \delta_k x_k
						\]
					\end{enumerate}
				\end{assumption}
				\begin{remark}
					Under above assumptions and regressing $y$ on $x_1, x_2, \dots, \textcolor{red}{x_k}$, the OLS estimator for $(\beta_1, \beta_2, \dots, \beta_{k-1})$ is still \hl{consistent} and \hl{unbiased}.\\
					 But for intercept and $k^{th}$ coefficient, we are effectively estimating $\beta_0 + \delta_0 \beta_k$ and $\delta_k \beta_k$.
				\end{remark}
			\subsubsection{Proxy Bias}
				If $x_k^{*}$ is correlated with all $\{x_1, x_2, \dots, x_k\}$ (collinearity), i.e. 
				\[
					x_k^{*} = \delta_0 + \delta_1 x_1 + \delta_2 x_2 + \dots + \delta_k x_k + v_k
				\]
				the for the coefficient of $x_j$ in the original regression, 
				\[
					plim(\hat{\beta}_j) = \beta_j + \beta_k \delta_j
				\]
				which means the estimation is still biased. \hl{In this case, using a proxy variable will not solve the omitted variable bias problem}.
				
		\subsection{Measurement Error in an Explanatory Variable}
			\par Consider the model 
			\[
				y = \beta_0 + \beta_1 x_1^{*} + u
			\]
			but we can only observe $x_1 = x_1^{*} + e_1$. 
			
			\begin{assumption}
				Assuming \textbf{measurement error} satisfies
				\[
					\expect{e_1} = 0
				\]
			\end{assumption}
			
			and the regression model becomes if we regress $y$ on the observed $x_1$.
			\begin{equation}
				y = \beta_0 + \beta_1 x_1 + (u - \beta_1 e_1)
			\end{equation}
			\begin{assumption}
				$u$ is uncorrelated with both $x_1$ and $x_1^{*}$, i.e. $x_1$ does not affect $y$ after $x_1^*$ has been controlled for.
			\end{assumption}
			
			\subsubsection{Case 1: $Cov(x_1, e_1) = 0$}
				\begin{remark}
					Since $e_1 = x_1 + x_1^*$, if $Cov(x_1, e_1) = 0$ then $Cov(x_1^*, e_1) \neq 0$.
				\end{remark}
				\begin{remark}
					\begin{gather*}
						\expect{u-\beta_1 e_1} = \expect{u} - \beta_1 \expect{e_1} = 0
					\end{gather*}
					MLR.3 still holds and \hl{estimator $\hat{\beta}_1$ is still consistent}.
				\end{remark}
				
				\begin{remark}
					Note that 
					\[
						Var(u - \beta_1 e_1) = \sigma_u^2 + \beta_1^2 \sigma_{e_1}^2
					\]
					the variance of estimators is inflated unless $\beta_1 = 0$.
				\end{remark}
				
			\subsubsection{Case 2 $Cov(x_1^{*}, e_1) = 0$: Classical errors-in-variance(CEV)}
				\begin{remark}
					\begin{gather*}
						Cov(x_1, e_1) = \expect{(x_1 - \overline{x}_1)(e_1 - \overline{e}_1)} \\
						= \expect{x_1 e_1} \\
						= \expect{(x_1^{*} + e_1) e_1} \\
						= \expect{x_1^{*}e_1 + e_1^2} \\
						= 0 + \expect{e_1^2} \\
						= \expect{(e_1 - \overline{e}_1)^2} \\
						= \sigma_{e_1}^2 \neq 0
					\end{gather*}
				Thus the covariance between $x_1$ and $x_1$ is equal to the variance of the measurement error under CEV assumption.
				\end{remark}
				
				\begin{remark}
					From equation (11), the new residual is $(u - \beta_1 e_1)$ and
					\begin{gather*}
						Cov(x_1, u - \beta_1 e_1) = \sum{(x_1 - \overline{x}_1) (u - \beta_1 e_1)} \\
						= \sum {x_1 u} - \beta_1 \sum {x_1 e_1} \\
						= Cov(x_1, u) - \beta_1 \sum {(x_1 - \overline{x}_1)(e_1 - 0)} \\
						= 0 - \beta_1 Cov(x_1, e_1) \\
						= \sigma_{e_1}^2 \neq 0
					\end{gather*}
					this fails MLR.4 and the OLS regression of $y$ on $x_1$ gives a \hl{biased} and \hl{inconsistent} estimator.
				\end{remark}

		\subsection{Measurement Error in Dependent Variable}
			\par Consider model 
			\begin{equation}
				y^{*} = \textbf{X}\vec{\beta} + u
			\end{equation}
			and the actually observed $y$ is $y = y^{*} + e_0$, with \textbf{measurement error} $e_0$. If we regress the observed $y$ on explanatory variables, we are effectively estimating
			\begin{equation}
				y = \textbf{X}\vec{\beta} + \textcolor{red}{(u+e_0)}
			\end{equation}
			\begin{remark}
				\hl{Assuming the measurement error in $y$ is statistically independent of each explanatory variable}, the OLS estimator from (12) is consistent and unbiased (Gauss-Markov Holds).
			\end{remark}
			\begin{remark}
			Note that we would now have higher residual variance $\sigma_u^2 + \sigma_{e_0}^2$ and the variance for OLS estimator is inflated
				\[
					Var(\vec{\beta}) = (\sigma_u^2 + \sigma_{e_0}^2) (\textbf{X}'\textbf{X})^{-1}
				\]
			\end{remark}
			
			
			
	\section{Slide 13: Instrumental Variables}
		\subsection{Endogeneity}
			\begin{definition}
				If a predictor $x_j$ is correlated with $u$ for any reason, and MLR.4 is violated, then $x_j$ is said to be an \textbf{endogenous} explanatory variable.
			\end{definition}
			\[
				\expect{u|\textbf{x}} \neq 0
			\]
			
			\begin{equation}
				y = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k + u
			\end{equation}
			
			\paragraph{Sources of Endogeneity}
				\begin{itemize}
					\item Omitted variable bias.
					\item Sample selection bias.
					\item Simultaneity (bidirectional causality).
					\item Measurement error bias.
				\end{itemize}
			\paragraph{Remedies}
				\begin{itemize}
					\item Control for confounding variables.\footnote{A \textbf{confounding variable} is a variable that influences both the dependent variable and independent variable causing a spurious association.}
					\item Instrumental variables or two stage least square.
					\item Differences in difference. (repeated cross-section data)
					\item Fixed effects. (panel data)
				\end{itemize}
		\subsection{Instrumental Variables}
			\paragraph{The Problem} For the simple regression model
			\[
				y = \beta_0 + \beta x + u
			\]
			estimator $\hat{\beta}$ would be biased if endogeneity presents ($Cov(x, u) \neq 0$). \\
			Then OLS is actually estimating 
			\[
				\pd{y}{x} = \beta + \pd{u}{x}
			\]
			instead of purely $\beta$, where $\pd{u}{x} \neq 0$ due to endogeneity.\\
			\emph{We need a method to generate only exogenous variation in $x$, without changing $u$, and measure its impact on $y$ via $\beta$ only.}
			
			\begin{definition}
				An \textbf{instrument} $z$ for predictor $x$ is a variable the property that
				\begin{enumerate}
					\item (\ul{Exogeneity condition}) uncorrelated with $u$. \[ Cov(z,u) = 0\]
					\item (\ul{Relevance condition}) correlated (either positively or negatively) with $x$. \[Cov(z,x) \neq 0\]
				\end{enumerate}
			\end{definition}
			
			\begin{remark}
				There no perfect test for exogeneity condition and we have to argue it by appealing to economic theory. So we cannot prove exogeneity condition formally.
			\end{remark}
			\begin{remark}
				For the relevance condition, we can test it by testing the significance of $\pi_1$ in the regression below
					\[
						x = \pi_0 + \pi_1 z + v
					\]
			\end{remark}
		\subsection{Implementation of IV: Method of Moments}
			\paragraph{Procedure}
				\begin{enumerate}
					\item Identify $\beta$ in terms of \emph{population moments}.
					\item Replace the population moments with the sample moments.\footnote{By \textbf{analogy principle}, such replacement will lead to a consistent estimator.}
				\end{enumerate}
			\subsubsection{In Simple Regression}
				\paragraph{Identification} Consider the model with instrumental variable $z$ for $x$,
					\[
						y = \beta_0 + \beta_1 x + u
					\]
					subtract both sides the corresponding expectations,
					\[
						y - \expect{y} = \beta_1 (x - \expect{x}) + (u - \expect{u})
					\]
					multiplying both sides by $(z - \expect{z})$ and take expectation
					\begin{gather*}
						\expect{(y-\expect{y})(z-\expect{z})} = \beta_1\expect{(x-\expect{x})(z-\expect{z})} + \expect{(u-\expect{u})(z-\expect{z})} \\
						\implies Cov(y, z) = \beta_1 Cov(x,z) + Cov(u,z) \\
						\tx{By exogeneity condition and relevance condition} \\
						Cov(x,z) \neq 0 \land Cov(z,u) = 0\\
						\implies \textcolor{red}{\beta_1 = \frac{Cov(y,z)}{Cov(x,z)}}
					\end{gather*}
				\paragraph{Replacement} calculate the \ul{sample} covariances between $y,z$ and $x,z$ and substitute into above expression, the \textbf{IV estimator} of $\beta_1$ is
				\[
					\hat{\beta}_1 = \frac{\sum_i (y_i - \overline{y})(z_i - \overline{z})}{\sum_i(x_i - \overline{x})(z_i - \overline{z})} 
				\]
				and the \textbf{IV estimator} of $\beta_0$ is 
				\[
					\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}
				\]
				\begin{remark}
					When $z=x$ the IV estimator is equivalent to the OLS estimator. And the IV estimator is consistent even when MLR.4 does not hold.
				\end{remark}
			\subsubsection{Inference}
				Assuming 
				\[
					\expect{u^2|z}=\sigma^2=Var(u)
				\]
				Then the variance of $\hat{\beta}_1$ is 
				\[
				\textcolor{red}{
					Var(\hat{\beta}_1) = \frac{\sigma^2}{n\sigma_x^2 \rho_{x,z}^2}
				}
				\]
				with sample analogs and $R^2_{x,z}$ from regression of $x_i$ on $z_i$, the estimated variance is 
				\[
					\widehat{Var(\hat{\beta}_1)} = \frac{\hat{\sigma}^2}{SST_x R^2_{x,z}}
				\]
				Note that the variance of OLS estimator is estimated to be
				\[
					\widehat{Var(\hat{\beta}_1)} = \frac{\hat{\sigma}^2}{SST_x}
				\]
				Therefore the $IV$ estimator is always larger than OLS variance. \\
				Note that as $z \to x$, $R^2_{x,z} \to 1$ and IV estimator is approaching and ultimately equivalent to the OLS estimator.
			\subsubsection{Properties}
				If $z$ and $x$ are weakly correlated (aka. \textbf{weak instrument}).
				\begin{itemize}
					\item IV estimators can have large standard errors. (small $R^2_{x,z}$)
					\item IV estimators can have large \ul{asymptotic bias} if $Corr(z,u) \neq 0$ (since we cannot check exogeneity condition formally, so we cannot rule out this probability).
				\end{itemize}
				For IV estimator,
				\[
					plim \hat{\beta}_{1, IV} = \beta_1 + \frac{Corr(z,u) \sigma_u}{\textcolor{red}{Corr(z,x)} \sigma_x}
				\]
				compared with OLS estimator
				\[
					plim \hat{\beta}_{1,OLS} = \beta_1 + Corr(x,u) \frac{\sigma_u}{\sigma_x}
				\]
				
				\begin{remark}
					The $R^2$ in IV estimation can be negative, and we should be careful about interpreting $R^2$ in IV estimation.
				\end{remark}
			\subsection{IV in Multiple Regression}
			\par Consider the multiple regression model on $k$ predictors, where $y_2$ is endogenous. The \textbf{structural model} is given in (2) below.
			\begin{equation}
				y_1 = \beta_0 + \beta_1 \textcolor{red}{y_2} + \beta_2 z_1 + \cdots + \beta_k z_{k-1} + u_1
			\end{equation}
			\paragraph{Identification} Let $z_k$ be an instrumental variable for $y_2$ the exogenity condition can be expressed as
			\[
				Cov(z_k, u_1) = 0
			\]
			and assuming all other explanatory variables $z_i$ are uncorrelated with $u_1$. Also assume the \emph{zero-mean-error},
			\begin{gather*}
				Cov(z_i, u_1) = 0,\ \forall i \in \{1, 2, \dots, k - 1\} \\
				\expect{u_1} = 0
			\end{gather*}
			Above conditions can be re-written as 
			\begin{gather*}
				\expect{z_i u_1} = 0,\ \forall i \in \{1,2, \dots, k\} \\
				\expect{u_1} = 0
			\end{gather*}
			Above $k+1$ equations identify $\beta_0, \beta_1, \dots, \beta_k$.
			
			\paragraph{Replacement} Replacing $u_1$ with $\hat{u}_1$ from regression (2),
			\begin{gather*}
				\sum_{i=1}^n(y_{i1} - \hat{\beta}_0 - \hat{\beta}_1 \textcolor{red}{y_{i2}} - \hat{\beta_2}z_{i1} - \cdots - \hat{\beta}_k z_{k-1}) = 0 \\
				\sum_{i=1}^n \textcolor{red}{z_{i1}} (y_{i1} - \hat{\beta}_0 - \hat{\beta}_1 \textcolor{red}{y_{i2}} - \hat{\beta_2}z_{i1} - \cdots - \hat{\beta}_k z_{k-1}) = 0 \\
				\sum_{i=1}^n z_{i2} (y_{i1} - \hat{\beta}_0 - \hat{\beta}_1 \textcolor{red}{y_{i2}} - \hat{\beta_2}z_{i1} - \cdots - \hat{\beta}_k z_{k-1}) = 0 \\
				\vdots \\
				\sum_{i=1}^n z_{ik-1}(y_{i1} - \hat{\beta}_0 - \hat{\beta}_1 \textcolor{red}{y_{i2}} - \hat{\beta_2}z_{i1} - \cdots - \hat{\beta}_k z_{k-1}) = 0
			\end{gather*}
			And solving above $k+1$ equations and replacing  give the IV estimations of $\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_{k}$.
			\newline
			The \ul{relevance condition} $Corr(y_2, z_k)$ can be verified using \textbf{reduced-form(auxiliary) equation} below with $H_0: \pi_k = 0$ and $H_1: \pi_k \neq 0$.
			\[
				y_2 = \pi_0 + \pi_1 z_1 + \pi_2 z_2 + \cdots + \pi_k \textcolor{red}{z_k} + v_2
			\]
	
	\section{Slide 14: Two Stage Least Square}
		\paragraph{Motivation} Multiple good instrumental variables for the endogenous variable. \\
		\textbf{Structural Equation}: 
		\begin{gather}
			y = \beta_0 + \beta_1 y_2 + \beta_2 z_1 + u_1
		\end{gather}
		with \textbf{Reduced Form Equation}:
		\begin{gather}
			y_2 = \pi_0 + \pi_1 z_1 + \pi_2 z_2 + \pi_3 z_3 + v_2
		\end{gather}
		where at least one of $\pi_2, \pi_2 \neq 0$. (Relevance condition) \\
		\paragraph{2SLS Procedures}
		\begin{enumerate}
			\item \textbf{Stage 1} Run regression on REF and compute $\hat{y}_2$, which is a linear combination of $z_1, z_2, z_3$. So $\hat{y}_2 \perp u_1$ by exogeneity condition. Note that, $v_2 \centernot \perp u_1$.
			\[
				\hat{y}_2 = \hat{\pi}_0 + \hat{\pi}_1 z_1 + \hat{\pi}_2 z_2 + \hat{\pi}_3 z_3
			\]
			\item Check significance of $z_2$ and $z_3$ to verify relevance condition. 
			\item \textbf{Stage 2} Regress $y_1$ on $\hat{y}_2$ and $z_1$ to obtain $\hat{\beta}_{1, 2SLS}$.
		\end{enumerate}
		
		\begin{remark}
			The first stage of 2SLS removes endogeneity of $y_2$ (dropped with $v_2$).
		\end{remark}

		\paragraph{2SLS Procedures: general case}
		\begin{enumerate}
			\item \textbf{Stage 1} Run regression on REF and compute $\hat{y}_2$, which is a linear combination of $z_1, z_2, z_3$. So $\hat{y}_2 \perp u_1$ by exogeneity condition. Note that, $v_2 \centernot \perp u_1$.
			\[
				\hat{y}_2 = \hat{\pi}_0 + \hat{\pi}_1 z_1 + \hat{\pi}_2 z_2 + \hat{\pi}_3 z_3
			\]
			\item Check significance of $z_2$ and $z_3$ to verify relevance condition. 
			\item \textbf{Stage 2} Regress $y_1$ on $\hat{y}_2$ and $z_1$ to obtain $\hat{\beta}_{1, 2SLS}$.
		\end{enumerate}		

		\begin{remark}[Number of IVs in the general case]
			With $k$ predictors in total, if $m$ of them are endogenous, we need at least $m$ excluded exogenous variables to run 2SLS.\\
			Otherwise, in the second stage regression, we would have less explanatory variables than parameters to be estimated. (\emph{perfect collinearity})
		\end{remark}
\end{document}





























