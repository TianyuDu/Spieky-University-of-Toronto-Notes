\documentclass{article}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1truein]{geometry}
\usepackage{tkz-graph}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{setspace}
\linespread{1.15}
\usepackage[margin=1truein]{geometry}

\counterwithin{equation}{section}
\counterwithin{figure}{section}

\pagestyle{fancy}
\lhead{STA347: Probability}

\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}


\title{STA347: Probability}
\date{\today}
\author{Tianyu Du}
\begin{document}
    \maketitle
    \tableofcontents
    \newpage
   	\section{Preliminaries}
   	\subsection{Random Processes}
   	\begin{definition}
   		A \textbf{process}\footnote{This is just a process, not necessarily a random process.} $W$ is a mechanism generating \textbf{outcomes} $w$ from a sample space $\Omega$. Any realized trail of process $W$ can be denoted as a potentially infinite sequence in $\Omega$:
   		\begin{align}
   			W: w_1, w_2, \cdots, w_n, \cdots
   		\end{align}
   	\end{definition}
   	
   	\begin{definition}
   		A \textbf{random variable} (extended process), $X := g(W)$, can be constructed from a process $W$ and a real-valued function $g: \Omega \to \R$.
   	\end{definition}
   	
   	\begin{definition}
   		Given a random variable $X = g(W)$, the \textbf{sample mean} (i.e. empirical expectation) of the first $n$ trials from a sequence of realizations, $g(w_1), \cdots, g(w_n), \cdots$, is defined to be
   		\begin{align}
   			\hat{\expe}_n g(W) &:= \frac{\sum_{i=1}^n g(w_i)}{n}
   		\end{align}
   	\end{definition}
   	
   	\begin{definition}
   		A process $W$ is said to be a \textbf{random process/variable} if it satisfies the \emph{empirical law of large numbers}, in that, $\forall g \in \R^\Omega$:
   		\begin{enumerate}[(i)]
   			\item \emph{stability}: $(\hat{\expe}_ng(W))_{n\in \N}$ converges;
   			\item \emph{Invariance}: $\forall\ (w_n)_{n \in \N} \subseteq \Omega$, the limits of $(\hat{\expe}_ng(W))_{n\in \N}$ are the same.
   		\end{enumerate}
   	\end{definition}
   	
   	\begin{definition}
   		Let $W$ be a random process and $g \in \R^\Omega$, the \textbf{expected value} of $g(W)$ is defined as
   		\begin{align}
   			\expe g(W) &:= \lim_{n \to \infty} \hat{\expe}_ng(W)
   		\end{align}
   		the limit is well-defined given ELLN.
   	\end{definition}
   	
   	\begin{definition}
   		Let $W$ be a random process. For every $A \subseteq \Omega$, take $g := I_A \in \R^\Omega$, the \textbf{empirical relative frequencies} (i.e. empirical probability) is defined as
   		\begin{align}
   			\hat{P}(W \in A) &:= \hat{\expe}_n I_A(W)
   		\end{align}
   		Given ELLN, the limit is well-defined, then the \textbf{probability} is defined to be the limit:
   		\begin{align}
   			P(W \in A) &:= \lim_{n \to \infty} \hat{P}(W \in A)
   		\end{align}
   	\end{definition}
   	
   	\begin{remark}
   		The notation of expected values and probabilities on $W$ is well-defined only when $W$ satisfies the empirical law of large numbers, that is, $W$ is a random process. \\
   		Given $W$ defined on $\Omega$ satisfies ELLN, the behaviour of $W$ can be fully characterized by its \textbf{probability distribution}.
   		\begin{align}
   			W \sim P_W \tx{ on } \Omega
   		\end{align}
   	\end{remark}

	\subsection{Indicator Functions and Set Operations}
	\begin{proposition}
		The \textbf{indicator map} $I: \mc{P}(\Omega) \to \{0, 1\}^\Omega$ is bijective.
	\end{proposition}
	
	\begin{proof}
		\emph{Injective:} Let $f, g \in \{0, 1\}^\Omega$, show the corresponding sets $A_f, A_g$ are the same. \\
		Let $w \in A_f$, then $f(w) = 1 = g(w)$, which implies $w \in A_g$. Therefore, $A_f \subseteq A_g$. \\
		The other direction is similar. \\
		\emph{Surjective:} Let $f \in \{0, 1\}^\Omega$, then one can construct $A_f := \{w \in \Omega: f(w) = 1\}$ so that $I(A_f) = f$.
	\end{proof}
	
	\begin{proposition}
		Let $W$ be a random variable defined on outcome space $\Omega$, and let $(A_n)$ be a collection\footnote{The lecture note presents it as a countable set, indeed, it can be an arbitrary set.}, then
		\begin{align}
			I\bigcap_{n=1}^\infty A_n &= \inf_{n=1}^\infty I A_n \\
			I\bigcup_{n=1}^\infty A_n &= \sup_{n=1}^\infty I A_n
		\end{align}
	\end{proposition}

	\subsection{Real Analysis}
	\begin{definition}
		A sequence $(x_n)$ is \textbf{Cauchy} if $\sup_{i, j \geq n} \abs{x_i - x_j} \to 0$ as $n \to \infty$.
	\end{definition}
	
	\begin{proposition}
		\begin{align}
			\sup_{i, j \geq n} \abs{x_i - x_j} = \sup_{i \geq n} x_i - \inf_{j \geq n} x_j
		\end{align}
	\end{proposition}
	
	\begin{proposition}
		The sequence $(\sup_{n \geq k} a_k)_k$ is decreasing, and $(\inf_{n \geq k} a_k)_k$ is increasing.
	\end{proposition}

	\begin{theorem}
		Let $(x_n)$ be a sequence in a Hilbert space, then 
		\begin{align}
			(x_n) \to x \iff \liminf x_n = \limsup x_n
		\end{align}
	\end{theorem}
	
	\begin{proposition}[Corollary of Order Limit Theorem]
	It is evident that
	\begin{align}
		\forall n \in \N, \inf_{i\geq n} x_i \leq \sup_{i \geq n} x_i	
	\end{align}
	therefore, 
	\begin{align}
		\liminf x_n \leq \limsup x_n
	\end{align}
	\end{proposition}









   	\section{Distributions}
	
	\subsection{Construction of Uniform Distributions}
   	\begin{definition}
   		For any $n \in \N$, a random variable $X$ is said to have a \textbf{(finite discrete) uniform distribution} on the sample space $\Omega = \{1,\cdots,n\}$ if
   		\begin{align}
   			P(X=k) = \frac{1}{n}\ \forall k \in \{1,\cdots,n\}
   		\end{align}
   		Denoted as $X \sim unif\{1,\cdots,n\}$.
   	\end{definition}

   	\begin{definition}
   		A random variable $U$ wit $\Omega = [0, 1]$ is said to follow a \textbf{(continuous) standard uniform} if 
   		\begin{align}
   			P(U \leq u) = u\ \forall u \in [0, 1]	
   		\end{align}
   		Denoted as $U \sim unif[0, 1]$.
   	\end{definition}
   	
   	\paragraph{Construction of Continuous Uniform} Let $Y$ be a random variable on $\{0, \cdots, 9\}$. For each sequence of realizations of $Y$, $(Y_i)_{i \in \N}$, one can construct $U \in [0, 1]$ using the following decimal expansion:
   	\begin{align}
   		U := \sum_{i=1}^\infty \frac{Y_i}{10^i}
   	\end{align}
   	
   	\begin{proposition}
   		It is evident that each finite decimal  of $Y$, $(Y_1, \cdots, Y_n)$, partitions the unitary into $10^n$ small intervals 
   		\begin{align}
   			P(0.Y_1\cdots Y_n \leq U \leq 0.Y_1\cdots Y_n + 10^{-n}) &= \frac{1}{10^n}
   		\end{align}
   		and 
   		\begin{align}
   			P(0 \leq U \leq 0.Y_1\cdots Y_n) &= \frac{Y_1 Y_2 \cdots Y_n}{10^n} = 0.Y_1\cdots Y_n
   		\end{align}
   	\end{proposition}
   	
   	\begin{proposition}
   		$P(a \leq U \leq b) = b - a$.
   	\end{proposition}
   	
   	\begin{corollary}
   		$P(U = u) = P(u \leq U \leq u) = u - u = 0$.
   	\end{corollary}
   	
   	\begin{proposition}
   		Let $U \sim unif[0, 1]$ and $V:=1-U$, then $V \eqd U$.
   	\end{proposition}
   	
   	\begin{proof}
   		\begin{align}
   			P(V \leq u) &= P(1 - U \leq u) 
   			= P(U \geq 1 - u) \\
   			&= 1 - P(U \leq 1 - u)
   			= 1 - 1 + u = u
   		\end{align}
   	\end{proof}
   	
   	\begin{theorem}
   		If $U = \sum_{n=1}^\infty Z_i p^{-i}$, then the following are equivalent:
   		\begin{enumerate}[(i)]
   			\item $U \sim unif[0, 1]$;
   			\item $Z_i \overset{i.i.d.}{\sim} Z \overset{d}{=} unif\{0, \cdots, p-1\}$ for some $p \geq 2$.
   		\end{enumerate}
   	\end{theorem}
   	
   	\begin{proof}
   		The result should be evident by construction of uniform distribution, (ii) is simply the base $p$ expansion of real numbers, which is equivalent to the decimal expansion.
   	\end{proof}
   	
   	\begin{definition}
   		Two random processes $W_1, W_2$ defined on $\Omega$ are \textbf{identically distributed}, $W_1 \eqd W_1$ if 
   		\begin{align}
   			\expe g(W_1) = \expe g(W_2) \quad \forall g: \Omega \to \R
   		\end{align}
   	\end{definition}
   	
   	\begin{theorem}[Invariance I]
   		If $X \eqd Y$, then 
   		\begin{align}
   			\varphi(X) \overset{d}{=} \varphi(Y)\quad \forall \varphi: \Omega \to \mc{X}
   		\end{align}
   		Note that $\varphi(X)$ and $\varepsilon(Y)$ are random variable defined on sample space $\mc{X}$.
   	\end{theorem}
   	\begin{proof}
   		Let $h: \mc{X} \to \R$, note that $h \circ \varepsilon: \Omega \to \R$. It is evident that $\expe h \circ \varphi(X) = \expe h \circ \varphi(Y)$.
   	\end{proof}
   	
   	\begin{theorem}[Invariance II]
   		\begin{align}
   			W_1 \eqd W_2 \iff g(W_1) \eqd g(W_2)\quad \forall g: \Omega \to \R
   		\end{align}
   	\end{theorem}
   	
   	\begin{proof}
   		($\implies$) The sufficient direction is direct by previous theorem. \\
   		($\impliedby$) Suppose $g(W_1) \eqd g(W_2)\quad \forall g: \Omega \to \R$. The proof is immediate by applying identity mapping in the definition of $g(W_1) \eqd g(W_2)$, which implies $\expe g(W_1) = \expe g(W_2)$ for any function $g: \Omega \to \R$.
   	\end{proof}

   	\begin{corollary}
   		Let $A \overset{d}{=} B$, for each $A \subseteq \Omega$, take $g = I_A$. Then,
	   	\begin{align}
	   		P(X \in A) = \expe I_A(X) = \expe I_A(Y) = P(Y \in A)
	   	\end{align}
   	\end{corollary}
   	
   	\subsection{Constructing Other Distributions}
   	
   	\begin{definition}
   		Let $Z = - \ln U$ with $u \sim unif[0,1]$. Then $Z$ is said to follow \textbf{exponential distribution} such that
   		\begin{align}
   			P(s \leq Z \leq t) = e^{-s} - e^{-t}
   		\end{align}
   		The derivation of distribution is immediate from the monotone property of $-\ln$.
   	\end{definition}
   	
   	\begin{definition}
   		The random variable $Z$ is said to have a \textbf{standard exponential distribution} on $\R_+$, denoted as $Z \sim exp(1)$ if
   		\begin{align}
   			P(Z \leq z) = 1 - \exp(-z)
   		\end{align}
   	\end{definition}
   	
   	\begin{definition}
   		The random variable $X$ is said to have a \textbf{scaled exponential distribution} with some $\theta > 0$, denoted as $X \sim exp(\theta)$ if
   		\begin{align}
   			X \eqd \theta Z\quad Z \sim exp(1)
   		\end{align}
   	\end{definition}
   	
   	\begin{definition}
   		For any real-valued random variable, $X$\footnote{A real-valued random variable can be deemed as the composite of an arbitrary random variable $W$ on $\Omega$ and a function $g: \Omega \to \R$}, the \textbf{distribution function} of $X$ is defined as
   		\begin{align}
   			F_X(x) := P(X \leq x)\ \forall x \in \R
   		\end{align}
   	\end{definition}
   	
   	\begin{definition}
   		A real-valued random variable, $X$, is said to be \textbf{absolutely continuous} with respective to length measure if there exists $f: \R \to \R_+$ such that
   		\begin{align}
   			P(s < X \leq t) = \int_s^t f_X(x)\ dx\quad \forall s \leq t
   		\end{align}
   		Where $f_X$ is defined as the \textbf{probability density function} of $X$. \\
   		\emph{Remark 1: A random variable is absolutely continuous if there exists an integrable density function $f$.} \\
   		\emph{Remark 2: The density function $f_X$ is not necessarily unique.}
   	\end{definition}
   	
   	\begin{definition}
   		The \textbf{percentile/quantile function}, $x_p = g(p): [0, 1] \to \R$, is defined so that
   		\begin{align}
   			F(X \leq x_p) = p
   		\end{align}
   	\end{definition}
   	
   	\begin{remark}
   		In any event, it is (or certainly should be) perfectly clear that each of the three methods outlined above will lead to the same basic result, each with its own probability density function, f, its own distribution function, F, and its own percentile function, g.
   	\end{remark}
   	
   	\subsection{More on Expectation Operators}
   	
   	\begin{remark}
   		The \textbf{expectation} operator
   		\begin{align}
   			\mathbb{E}: \mc{R} \to \R \cup \{\pm\infty\} \cup \{ \tx{DNE} \}
   		\end{align}
   		where $\mc{R}$ is the space of \emph{real-valued} random processes.
   	\end{remark}
   	
   	\begin{theorem}
   		By the algebraic limit theorem, it is evident that for any pair of real-valued random process $X, Y$ and scalar $a$, the expectation operator is linear:
   		\begin{enumerate}[(i)]
   			\item $\expe (X+Y) + \expe X + \expe Y$;
   			\item $\expe (aX) = a \expe X$.
   		\end{enumerate}
   	\end{theorem}
   	
   	\begin{proposition}[Expectation is Normed]
   		$\expe c = c\ \forall c \in \R$.
   	\end{proposition}
   	
   	\begin{proposition}
   		$X \geq 0 \implies \expe X \geq 0$ by order limit theorem.
   	\end{proposition}
   	
   	\begin{notation}
   		Disjoint unions are often denoted as
   		\begin{align}
   			\sum_{n=1}^\infty A_i &:= \bigcup_{n=1}^\infty A_i
   		\end{align}
   	\end{notation}
   	
   	\begin{theorem}
   		Let $W$ be a real-valued random variable. Then for any \ul{finite mutually disjoint} set $(A_1, \cdots, A_n)$, 
   		\begin{align}
   			P(W \in \sum_{i=1}^n A_i) = \sum_{i=1}^n P(W \in A_i)
   		\end{align}
   		That is, the probability measure is linear given mutual disjoining.
   	\end{theorem}
   	
   	\begin{proposition}
   		Properties of probability measure:
   		\begin{enumerate}[(i)]
   			\item $I_\Omega(W) = 1 \implies P(W \in \Omega) = \expe 1 = 1$;
   			\item $0 \leq I_A(W) \leq 1 \implies 0 \leq P(W \in A) = \expe I_A(W) \leq 1$.
   		\end{enumerate}
   	\end{proposition}
   	
   	

	\subsection{Expected Value for an Arbitrary Discrete Distribution}
	\begin{definition}
		A \textbf{finite scheme} or a \textbf{finite discrete distribution} can be written as
		\begin{align}
			W \sim\left(\begin{array}{lll}{\omega_{1}} & {\cdots} & {\omega_{N}} \\ {p_{1}} & {\cdots} & {p_{N}}\end{array}\right)
		\end{align}
		with \textbf{probability mass function} (pmf)
		\begin{align}
			P\left(W=\omega_{i}\right)=p_{i}\ s.t.\ \sum_{i=1}^N p_i = 1
		\end{align}
		With vector notation
		\begin{align}
			W \sim\left(\begin{array}{l}{\omega} \\ p\end{array}\right)\ s.t.\ \omega \subseteq \Omega,\ \inner{p}{1} = 1
		\end{align}
	\end{definition}
	
	\begin{proposition}
		The expected value of a finite discrete random variable is more or less obvious:
		\begin{align}
			E g(W)=\sum_{i=1}^{N} g\left(\omega_{i}\right) P\left(W=\omega_{i}\right)=\sum_{i=1}^{N} g\left(\omega_{i}\right) p_{i}
		\end{align}
	\end{proposition}
	
	\begin{proposition}
		A finite discrete random variable, $g(W)$, can be explicitly represented as a \emph{finite linear combination of simple indicator functions}:
		\begin{align}
			g(W)=\sum_{i=1}^{N} g\left(\omega_{i}\right) I\left(W=\omega_{i}\right)=\sum_{i=1}^{N} g\left(\omega_{i}\right) I_{\left\{\omega_{i}\right\}}(W)
		\end{align}
	\end{proposition}
	
	\subsection{Discrete Uniform Distributions}
	   	\begin{proposition}
   		Let $W \sim unif\{1, \cdots, n\}$, then
   		\begin{align}
   			n + 1 - W &\overset{d}{=} W \\
   			\implies (n + 1 - W)^2 &\overset{d}{=} W^2 \\
   			\implies (n+1)^2 - 2(n+1)W + W^2 &\overset{d}{=} W^2 \\
   			\implies \expect{(n+1)^2 - 2(n+1)W + W^2} &= \expect{W^2} \\
   			\implies \expect{W} &= \frac{n+1}{2}
   		\end{align}
   	\end{proposition}
   	
   	\begin{proposition}
   		\begin{align}
   			(n+1-W)^3 &\overset{d}{=} W^3 \\
   			\implies 2 \expect{W^3} &= (n+1)^3 - 3(n+1)^2 \expect{W} + 3(n+1) \expect{W^2} \\
   			\implies 2 \expect{W^3} &= (n+1)^3 - 3(n+1)^2 \frac{n+1}{2} + 3(n+1) \expect{W^2} \\
   			\implies 2 \expect{W^3} &= - \frac{(n+1)^2}{2} + 3(n+1) \expect{W^2} \\
   			\implies \expect{W^3} &= n (\expect{W})^2
   		\end{align}
   	\end{proposition}
   	
   	\begin{proposition}
   		$\expect{W^4}$. \hl{TODO}
   	\end{proposition}
   	
   	\begin{definition}
   		$W \sim unif\{1, \cdots, n\}$, then the \emph{distance between} $W^2$ and $\expect{W^2}$ is defined as
   		\begin{align}
   			d(W^2, \expect{W^2}) &:= \sqrt{\expect{W^2 - \expect{W})^2}^2} = \sqrt{\var{W^2}} = \sigma_{W^2}
   		\end{align}
   	\end{definition}
   	
   	\begin{corollary}[Corollary of Jensen's Inequality]
   		\begin{align}
   			\expect{W^2} \geq (\expect{W})^2
   		\end{align}
   		and equality holds \ul{if and only if}
   		\begin{align}
   			\expect{(W - \expect{W})^2} &= 0
   		\end{align}
   		which is equivalent to
   		\begin{align}
   			P(W = \expect{W}) = 1
   		\end{align}
   	\end{corollary}
   	\begin{proof}
   		\begin{align}
	   		\var{W} = \expect{(W - \expect{W})^2} \geq 0
   		\end{align}
   	\end{proof}
   	
   	\begin{theorem}
   		\begin{align}
   			E U^{k}-E(U-1)^{k}=n^{k-1} \quad k \in \mathbb{N}
   		\end{align}
   	\end{theorem}
   	
   	\begin{proof}
   	Let $U \sim unif\{1, \cdots, n\}$, it is evident that $U - 1 \sim unif\{0, \cdots, n-1\}$
   		\begin{align}
   			EU^k - E(U-1)^k &= \frac{\sum_{i=1}^n i^k - \sum_{i=0}^{n-1} i^k}{n} \\
   			&= \frac{\sum_{i=1}^n i^k - \sum_{i=1}^{n-1} i^k}{n} \\
   			&= \frac{n^k}{n} = n^{k-1}
   		\end{align}
   	\end{proof}
   	
   	
   	\subsection{Bernoulli Trials}
   	\begin{definition}
   		A random variable $Z$ is said to be a \textbf{Bernoulli trial} if 
   		\begin{align}
   			Z \sim\left(\begin{array}{ll}{0} & {1} \\ {q} & {p}\end{array}\right)
   		\end{align}
   		with $p \in [0, 1]$, denoted as $Z \sim bern(p)$.
   	\end{definition}
   	
   	\begin{remark}
   		Let $A \subseteq \Omega$, the \textbf{Bernoulli trial} associated with $A$ is defined to be the finite scheme distribution
   		\begin{align}
   			Z=I(A) \sim\left(\begin{array}{cc}{0} & {1} \\ {1-P(A)} & {P(A)}\end{array}\right)
   		\end{align}
   	\end{remark}
   	
   	\begin{proposition}[Invariance]
   		Given the outcome space of Bernoulli trials to be $\{0, 1\}$, $Z^s = Z$ for every $s \in \N$. \\
   		\emph{Remark: $Z^2$ and $Z$ are indeed equal, which is a stronger statement than equal in distribution.}
   	\end{proposition}
   	
   	\begin{proposition}[Negation]
   		$Z \sim bern(p) \quad \Leftrightarrow \quad 1-Z \sim bern(q)$ where $p + q = 1$.
   	\end{proposition}
   	
   	\subsection{Binomial Distribution}
   	\begin{definition}
   		Given $n \in \N$ and $0 \leq p \leq 1$, a random variable $X$ is said to follow \textbf{binomial distribution} with $n$ trails and chance $p$ if
   		\begin{align}
   			X &\eqd Z_1 + \cdots + Z_n\quad Z_i \iid  \begin{pmatrix}
   				0 & 1 \\ q & p
   			\end{pmatrix}
   		\end{align}
   		Denoted as $X \sim bin(n, p)$.
   	\end{definition}
   	
   	\begin{proposition}
   		$X^2 \eqd \left( \sum_{i=1}^n Z_i \right)^2$.
   	\end{proposition}
   	
   	\begin{proof}
   		The statement follows immediately from the definition of Binomial distribution.
   	\end{proof}
   	
   	\begin{corollary}
   		\begin{align}
   			\expe (X^2) &= \expe \left ( \sum_{i=1}^n Z_i \right)^2 \\
   			Var(X) &= Var \left( \sum_{i=1}^n Z_i \right)
   			= \sum_{i=1}^n Var(Z_i) \\
   			&= \sum_{i=1}^n pq
   			= npq
   		\end{align}
   		More generally, for any $f: \R \to \R$, $f(X) \eqd f \left( \sum_{i=1}^n Z_i \right)^2$.
   	\end{corollary}
   	
   	\begin{proposition}
   		Let $X \sim bin (m, p)$, $X \sim bin(n, p)$ and $X \perp Y$, then $X + Y \sim bin(m + n, p)$.
   	\end{proposition}
   	
   	\begin{proof}
   		Note that the joint distribution of $(X, Y)$ follows 
   		\begin{align}
   			\begin{pmatrix}
   				X \\ Y
   			\end{pmatrix}
   			\eqd
   			\begin{pmatrix}
   				Z_1 + \cdots + Z_m \\
   				Z_{m+1} + \cdots + Z_{m+n}
   			\end{pmatrix}
   		\end{align}
		then the result is immediate.
   	\end{proof}
   	
   	\begin{example}
   		Define $g(Z_1, \cdots, Z_n) := \sum_{i=1}^n a_i Z_i^i$, then 
   		\begin{align}
   			g(Z_1, \cdots, Z_n) &\eqd \sum_{i=1}^n a_i Z_i \\
   			\implies \expe g(Z_1, \cdots, Z_n) &= \sum_{i=1}^n a_i p \\
   			Var(g(Z_1, \cdots, Z_n)) &= \sum_{i=1}^n a_i pq
   		\end{align}
   	\end{example}
   	
   	\subsection{Probability Mass Function}
   	\begin{definition}
   		Let $X \sim bin(n, p)$, then for each $k \in \{0, \cdots, n\}$, the \textbf{probability mass function (pmf)} is defined as
   		\begin{align}
   			P(X = k) &:= P(\sum_{i=1}^n Z_i = k)
   		\end{align}
   		Conversely, for every $k \notin \{0, \cdots, n\}$, $P(X=k) := 0$.
   	\end{definition}
   	
   	\begin{theorem}[pmf for Binomial]
   		Note that $P(Z=z) = p^z q^{1-z}$ for $z \in \{0, 1\}$, then for every $(z_i)_{i=1}^n \in \{0, 1\}^n$, it is evident that
   		\begin{align}
   			P((Z_i)_{i=1}^n = (z_i)_{i=1}^n) &= P \left[\bigcap_{i=1}^n\{Z_i = z_i\}\right] \\
   			&= \prod_{i=1}^n P(Z_i = z_i) \\
   			&= \prod_{i=1}^n p^{z_i} q^{1-z_i} \\
   			&= p^{\sum_{z_i}} q^{n - \sum z_i}
   		\end{align}
   		Define
   		\begin{align}
   			C_k^n := \{(z_1, \cdots, z_n) \in \{0, 1\}^n: \sum_{i=1}^k z_i = k\}
   		\end{align}
   		Then 
   		\begin{align}
   			P(X = k) &= \sum_{(z_1, \cdots, z_n) \in C_k^n} P[(Z_1, \cdots, Z_n) = (z_1, \cdots, z_n)] \\
   			&= \sum_{(z_1, \cdots, z_n) \in C_k^n} p^k q^{n-k} \\
   			&= |C_k^n| p^k q^{n-k}
   		\end{align}
   		Define $\binom{n}{k} := |C_k^n|$.
   	\end{theorem}
   	
   	\begin{definition}
   		Given $k, n \in \N$ such that $k < n$, the \textbf{descending operator} $n^{(k)}$ is defined as
   		\begin{align}
   			n^{(k)} := \underbrace{n (n-1) (n-2) \cdots (n-(k-1))}_{k\tx{ terms in total}} = \frac{n!}{(n-k)!}
   		\end{align}
   	\end{definition}
   	
   	\begin{theorem}
   		Let $X \sim bin(n, p)$, then
   		\begin{align}
   			\red{\expe X^{(r)} = \begin{cases}
   				n^{(r)} p^r &0 \leq r \leq w \\
   				0 &0\geq n+1
   			\end{cases}}
   		\end{align}
   	\end{theorem}
   	
	\begin{proof}
		Let $X \sim bin(n, p)$ with $\Omega_X = \{0, \cdots, n\}$. \\
		By definition, $X^{(r)}$ is another random variable by construction:
		\begin{align}
			g(X) := X^{(r)} = X(X-1)(X-2)\cdots(X-(r-1))
		\end{align}
		Note that $g(X)$ is a polynomial with degree $r$ with sample space $\Omega_{g(X)}$. Then,
		\begin{align}
			\expe (X^{(r)}) &= \expe (g(X)) \\
			&= \sum_{k=0}^n k^{(r)} \binom{n}{k} p^k q^{n-k} \\
			&= \sum_{k=0}^n \frac{k!}{(k-r)!} \frac{n!}{k!(n-k)!} p^k q^{n-k} \\
			&= \sum_{k=0}^n \frac{1}{k!}\frac{k!}{(k-r)!} n^{(k)} p^k q^{n-k} \\
			&= \sum_{k=\red{r}}^n \frac{1}{k!}\frac{k!}{(k-r)!} n^{(k)} p^k q^{n-k} \\
			&= \sum_{k=r}^n \frac{1}{k!}\frac{k!}{(k-r)!} \frac{n! p^r}{((n-r) - (k-r))!} p^{k-r} q^{(n-r)-(k-r)}\\
			&= n^{(r)} p^r \sum_{k=r}^n \frac{1}{k!}\frac{k!}{(k-r)!} \frac{(n-r)!}{((n-r) - (k-r))!} p^{k-r} q^{(n-r)-(k-r)}\\
			&= n^{(r)} p^r \sum_{k=r}^n \frac{1}{(k-r)!} \frac{(n-r)!}{((n-r) - (k-r))!} p^{k-r} q^{(n-r)-(k-r)}\\
			&= n^{(r)} p^r \sum_{z=0}^{n-r} \frac{1}{z!} \frac{(n-r)!}{((n-r) - z)!} p^z q^{(n-r)-z}\\
			&= n^{(r)} p^r \sum_{z=0}^{n-r}\binom{n-r}{z} p^z q^{(n-r)-z}\\
			&= n^{(r)} p^r
		\end{align}
	\end{proof}
   	
   	\begin{example}
   		Find $\expe X^{(3)}$ given $\expe \sim bin(n, p)$ with $n \geq 3$.
   		\begin{align}
   			\expe X^{(3)} &= \expe[X(X-1)(X-2)] \\
   			&= \expe [X^3 - 3X^2 + 2X] \\
   			&= \expe X^3 - 3 \expe X^2 + 2 \expe X \\
   			&= \expe X^3 - 3 (npq + n^2 p^2) + 2 np \\
   			&= n(n-1)(n-2) p^3 \\
   			\implies \expe X^3 &= 3 (npq + n^2 p^2) - 2 np + n(n-1)(n-2) p^3
   		\end{align}
   	\end{example}
   	
   	\subsection{Standard Uniform Distribution}
   	
   	\begin{theorem}
   		Let $U \sim unif[0, 1]$, then
   		\begin{align}
   			E U^{k}=\frac{1}{k+1}, \quad k \geq 0
   		\end{align}
   	\end{theorem}
   	
   	\begin{proof}
   	Given the outcome space of standard uniform, $P(U \leq t) = F(t) = t$ for every $t \in [0, 1]$.
   		\begin{align}
   			EU^k &= \int_0^1 F'(t) t^k\ dt \\
   			&= \int_0^1 t^k\ dt\\
   			&= \left. \frac{1}{k+1} t^{k+1} \right|_0^1 \\
   			&= \frac{1}{k+1}
   		\end{align}
   	\end{proof}
   	
   	\subsection{Scaled Exponential Distribution}
   	
   	\subsection{Gamma Distribution}
   	
   	\section{Distribution Functions in General}
   	\subsection{Probability}
   	\begin{definition}
   		A \textbf{probability space} is a triple $(\Omega, \mc{F}, P)$ consisting of \textbf{sample space}, \textbf{event space}, and a \textbf{probability} satisfying the following properties: \\
   		\emph{Properties of $\mc{F}$}:
   		\begin{enumerate}[(i)]
   			\item $\Omega \in \mc{F}$;
   			\item Closed under complement;
   			\item Closed under countable union.
   		\end{enumerate}
   		\emph{Properties of $P: \mc{F} \to [0, 1]$}: 
   		\begin{enumerate}[(i)]
   			\item $\sigma$-additivity;
   			\item Non-negativity: $P(A) \geq 0$ for every $A \in \mc{F}$;
   			\item Normality: $P(\Omega) = 1$.
   		\end{enumerate}
   	\end{definition}
   	
   	\begin{definition}
   		A probability measure $P: \mc{F} \to [0, 1]$ is said to be \textbf{$\sigma$-additive} if for any countable mutually disjoint events $(A_n)$,
   		\begin{align}
   			P(\sum_{n \in \N} A_n) = \sum_{n \in \N} P(A_n)
   		\end{align}
   	\end{definition}
   	
   	\begin{definition}
   		Let $(A_n)$ be a sequence of subsets of $\Omega$, let $A \subseteq \Omega$. Then $(A_n) \to A$ if and only if $(I(A_n))$ point-wise converges to $I(A)$.
   	\end{definition}
   	
   	\begin{definition}
   		A probability measure $P$ is \textbf{continuous} if for any convergent sequence $(A_n) \to A$ in $\mc{F}$, $P(A_n) \to P(A)$. \\
   		\emph{Remark: the continuity of $P$ is only defined through the sequential definition, there is $\delta$-$\varepsilon$ definition for it.}
   	\end{definition}
   	
   	\begin{remark}
   		The \emph{constructor of indicator function} is a bijection between $\mc{P}(\Omega)$ and function space $\{0, 1\}^\Omega$.
   	\end{remark}
   	
   	\begin{proposition}[Set Theoretic Limits]
   		A sequence of sets $(A_n)$ converges if and only if that $I(A_n)$ converges, which is equivalent to $\lim_{n \to \infty} I(A_n) = \limsup I(A_n) = \liminf I(A_n)$:
   		\begin{align}
   			\lim_{n \to \infty} I(A_n) &= \lim_{n=1}^\infty \sup_{k\geq n} I(A_k) \\
   			&=\inf_{n=1}^\infty \sup_{k\geq n} I(A_k) \\
   			&=\inf_{n=1}^\infty I(\bigcup_{k \geq n} A_k) \\
   			&=I(\bigcap_{n=1}^\infty \bigcup_{k \geq n} A_k)
   		\end{align}
   		Similarly,
   		\begin{align}
   			\lim_{n \to \infty} I(A_n) &= \lim_{n=1}^\infty \inf_{k\geq n} I(A_k) \\
   			&= \sup_{n=1}^\infty \inf_{k\geq n} I(A_k) \\
   			&= I(\bigcup_{n=1}^\infty \bigcap_{k \geq n} A_k)
   		\end{align}
   		Therefore, 
   		\begin{align}
   			(A_n) \to A\ \iff\ A = \bigcup_{n=1}^\infty \bigcap_{k \geq n} A_k = \bigcap_{n=1}^\infty \bigcup_{k \geq n} A_k
   		\end{align}
   	\end{proposition}
   	
   	\begin{corollary}
   		Let $A_n \uparrow$ be an increasing sequence, that is, $A_n \subseteq A_{n+1}$. Then
   		\begin{align}
   			A_n \uparrow \bigcup_{n=1}^\infty A_n
   		\end{align}
   	\end{corollary}
   	
   	\begin{proof}
   		\todo{Prove this.}
   	\end{proof}
   	
   	\begin{corollary}
   		Let $A_n \downarrow$ be a decreasing sequence, that is, $A_{n} \supseteq A_{n+1}$. Then
   		\begin{align}
   			A_n \downarrow \bigcap_{n=1}^\infty A_n
   		\end{align}
   	\end{corollary}
   	
   	\begin{proof}
   		\todo{Prove this.}
   	\end{proof}
   	
   	\begin{proposition}
   		If $A_n \uparrow A$ or $A_n \downarrow A$, then $P(A_n) \to P(A)$.
   	\end{proposition}
   	
   	\begin{proof}
   		Let $A_0 = \varnothing$. Suppose $A_n \uparrow A$, then by previous corollary
   		\begin{align}
   			A = \bigcup_{n=1}^\infty A_n = \sum_{i=1}^\infty (A_i - A_{i-1})
   		\end{align}
   		Then 
   		\begin{align}
   			P(A) &= \sum_{i=1}^\infty P(A_i - A_{i-1}) \\
   			&= \lim_{n\to\infty}\sum_{i=1}^n P(A_i - A_{i-1}) \\
   			&= \lim_{n\to\infty}P(A_n) - P(A_0) \\
   			&= \lim_{n\to\infty}P(A_n)
   		\end{align}
   		\todo{Prove the other case.}
   	\end{proof}
   	
   	\begin{proposition}[Sequential Continuity]
   		\begin{align}
   			A_{n} \rightarrow A \implies P\left(A_{n}\right) \rightarrow P(A)
   		\end{align}
   	\end{proposition}
   	
   	\begin{theorem}
   		Any distribution function $F_X(x)$ is right-continuous. That is, $F(x+) = F(x)$.
   	\end{theorem}
   	
   	\begin{proof}
   		Let $x \in \R$, let $x_n \downarrow x$. Construct sequence of sets as $A_n := (-\infty, x_n]$.\\
   		By construction, $A_n \downarrow A$. \\
   		Therefore, $F_X(x_n) = P_X(A_n) \downarrow P_X(A) = F_X(x)$.
   	\end{proof}
   	
   	\begin{definition}
   		Let $X$ be a real-valued random variable, then the \textbf{probability mass function} of $X$ is given by
   		\begin{align}
   			p_X(x) := P(X=x)
   		\end{align}
   	\end{definition}
   	
   	\begin{proposition}
   		\begin{align}
   			p(x) = F(x) - F(x-)
   		\end{align}
   	\end{proposition}
   	\begin{proof}
   		Let $x_n \uparrow\uparrow x$, let $A_n = (-\infty, x_n]$ and $A = (-\infty, x)$. Clearly, $A_n \uparrow A$. Then,
   		\begin{align}
   			F(x-) &= \lim_{n \to \infty} P(A_n) \\
   			&= P(A) = P(X < x)
   		\end{align}
   		Hence $p(x) = P(X \leq x) - P(X < x) = F(x) - F(x-)$.
   	\end{proof}
   	
   	\begin{proposition}
   		It is evident that $p(x) = 0$ wherever $F(x)$ is continuous.
   	\end{proposition}
   	
   	\begin{definition}
   		The \textbf{points of continuity} of a distribution function $F$ is defined as
   		\begin{align}
   			C_F := p^{-1}(0)
   		\end{align}
   		And the \textbf{points of discontinuity} is simply $D_F := \R - C_F$.
   	\end{definition}
   	
   	\begin{theorem}
   		A distribution function $F$ can have at most countably many discontinuous point. That is, $|D_F| \leq \aleph_0$.
   	\end{theorem}
   	
   	\begin{proof}
   		Note that
   		\begin{align}
   			D_F = \bigcup_{n=1}^\infty \left\{x\in \R: p(x) > \frac{1}{n}\right\}
   		\end{align}
   		Note that $\abs{\left\{x\in \R: p(x) > \frac{1}{n}\right\}} < n$, therefore $D_F$ is a countable union of countable sets, so $|D_F| \leq \aleph_0$.
   	\end{proof}
   	
   	\subsection{Covariance as an Inner Product}
   	\paragraph{Motivation} Let $W$ be a random variable defined on $\Omega$, let $X := f(W)$ and $Y := g(W)$. Let $(x_1, \cdots, x_n)$ and $(y_1, \cdots, y_n)$ denote the sequence of outcomes. \\
   	Recall that the expectation of $X$ is defined as
   	\begin{align}
   		\expe X &:= \lim_{n \to \infty} \hat{\expe}_n X \\
   		&= \lim_{n \to \infty} \frac{\sum_{i=1}^n x_n}{n}
   	\end{align}
   	Such limit is well defined given $X$ satisfies the empirical law of large numbers.
   	
   	\begin{definition}
   		Define the \textbf{norm} (i.e. variance) of a random variable $X$ as 
   		\begin{align}
   			\norm{X} &:= \sqrt{\expe X^2} \\
   			&= \lim_{n \to \infty} \frac{\sqrt{\sum_{i=1}^n x_i^2}}{\sqrt{n}}
   		\end{align}
   	\end{definition}
   	\begin{definition}
   		Define the \textbf{inner product} between two random variables $X, Y$ as 
   		\begin{align}
   			\inner{X}{Y} &:= \lim_{n \to \infty} \frac{\sum_{i=1}^n x_i y_i}{n}
   		\end{align}
   	\end{definition}
   	
   	\begin{definition}
   		The \textbf{cosine similarity} between two random variable $X, Y$ can e written as
   		\begin{align}
   			\cos \angle (X, Y) &= \frac{\inner{X}{Y}}{\norm{X} \norm{Y}} \\
   			&= \lim_{n \to \infty} \frac{\frac{\sum_{i=1}^n x_i y_i}{n}}{\frac{\sqrt{\sum_{i=1}^n x_i^2}}{\sqrt{n}} \frac{\sqrt{\sum_{i=1}^n y_i^2}}{\sqrt{n}}} \\
   			&\overset{n \to \infty}{\to} \frac{\expe XY}{\sqrt{\expe X^2} \sqrt{\expe Y^2}}
   		\end{align}
   	\end{definition}
   	
   	\begin{definition}
   		The \textbf{covariance} between two random variables $X, Y$ is defined to be the \ul{decentralized} cosine similarity:
   		\begin{align}
   			Cov(X, Y) &:= \expe (X - \expe X)(Y - \expe Y)
   		\end{align}
   		In particular, for single random variable $X$, the \textbf{variance} is defined as
   		\begin{align}
   			Var(X) &:= Cov(X, X)
   		\end{align}
   		The \textbf{coefficient of correlation} is defined to be the \ul{normalized} covariance:
   		\begin{align}
   			\rho(X, Y) := \frac{Cov(X, Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}}
   		\end{align}
   	\end{definition}
   	
   	\begin{proposition}
   		$\expe X$ is the closest constant (in terms of norm defined on random variables) near $X$. That is, $\norm{X - \expe X} = \inf_{t \in \R} \norm{X - t}$.
   	\end{proposition}
   	
   	\begin{proof}
   		Define $g(t) := \expe (X - t)^2$, solving the first order condition gives $t^* = \expe X$.
   	\end{proof}

   	\begin{proposition}
   		Random variable space is a vector space.
   	\end{proposition}

   	\begin{definition}
   		The \textbf{centralization operator} for random variable is defined as
   		\begin{align}
   			\dot{X} := X - \expe X
   		\end{align}
   	\end{definition}
   	
   	\begin{proposition}
   		The centralization operator is linear, that is,
   		\begin{align}
   			\dot{(X + Y)} &= \dot{X} + \dot{Y} \\
   			\dot{(cX)} &= c \dot{X}
   		\end{align}
   	\end{proposition}
   	
   	\begin{proof}
   		The proof follows immediately from the linearity of expectation operator.
   	\end{proof}
   	
   	\begin{theorem}
   		Covariance operator is an inner product on the space of random variables.
   	\end{theorem}
   	
   	\begin{proof}
   		\emph{Symmetry:} obviously, $Cov(X, Y) = Cov(Y, X)$. \\
   		\emph{Bi-linearity:}
   		\begin{align}
   			Cov(X+Y, Z) &= \expe (\dot{X + Y} \dot{Z}) \\
   			&= \expe (\dot{X} + \dot{Y}) \dot{Z} \\
   			&= \expe \dot{X}\dot{Z} + \expe \dot{Y} \dot{Z} \\
   			&= Cov(X, Z) + Cov(Y, Z)
   		\end{align}
   		\emph{Non-negativity}
   		\begin{align}
   			Cov(X, X) &= \expe \dot{X} \dot{X} \\
   			&= \expe (\dot{X})^2 \geq 0
   		\end{align}
   		and the equality holds if and only if $X = \expe X$ with probability 1 (i.e. $X$ is deterministic).
   	\end{proof}
   	
   	\subsection{Markov Inequality}
   	\begin{theorem}[Markov Inequality]
   		Let $Z \geq 0$ be a random variable, let $t \in \R_+$ and $g: \R_+ \to \R_+$ be a non-decreasing function, then
   		\begin{align}
   			P(Z \geq t) &\leq \frac{\expe g(Z)}{g(t)}
   		\end{align}
   	\end{theorem}
   	
   	\begin{proof} Note that
   		\begin{align}
   			g(Z) &\geq g(t) \id{Z \geq t} \\
   			\implies \expe g(Z) &\geq g(t) P(Z \geq t) \\
   			\implies P(Z \geq t) &\leq \frac{\expe g(Z)}{g(t)}
   		\end{align}
   	\end{proof}
   	
   	\begin{corollary}[Chebyshev Inequality]
   		\begin{align}
   			P(|X - \expe X| \geq k) \leq \frac{\expe (X - \expe X)^2}{k^2}
   		\end{align}
   	\end{corollary}
   	
   	\begin{proof}
   		Let $Z = \abs{X - \expe X}$, $g(t) = t^2$, and apply the Markov inequality.
   	\end{proof}
   	
   	\begin{corollary}
   		\begin{align}
   			P(|X - \expe X| \geq k \sigma) \leq \frac{1}{k^2}
   		\end{align}
   		where $\sigma^2 = \expe (X - \expe X)^2$.
   	\end{corollary}
   	
   	\begin{proof}
   		Applying Chebyshev inequality gives
   		\begin{align}
   			P(|X - \expe X| \geq k \sigma) \leq \frac{\sigma^2}{(k \sigma)^2} = \frac{1}{k^2}
   		\end{align}
   	\end{proof}
   	
   	\begin{corollary}
   		Let $X$ be a random variable, $Var(X) = 0$ if and only if $X = \expe X$ with probability 1.
   	\end{corollary}
   	
   	\begin{proof}
   		($\implies$) Suppose $Var(X) = 0$, then for every $n \in \N$,
   		\begin{align}
   			0 \leq P(|X - \expe X| \geq \frac{1}{n}) \leq Var(X) n^2 = 0
   		\end{align}
   		Because the above inequality holds for arbitrarily large $n$, the only case is that $P(|X - \expe X| > 0) = 0$.\\
   		Given $|X - \expe X| \geq 0$, it must be $P(|X - \expe X| = 0) = 1$. \\
   		($\impliedby$) Suppose $P(\abs{X - \expe X} = 0) = 1$. \\
   		Let $Y := (X - \expe X)^2$, obviously $Var(X) = \expe Y$. \\
   		It's been given that $P(Y = 0) = 1$, which means $\expe Y = 0$. Therefore, $Var(X) = 0$.
   	\end{proof}
   	
   	\begin{theorem}[An Application]
   		\begin{align}
   			U \sim unif[0, 1] \iff [nU] \sim unif\{0, \cdots, n-1\}
   		\end{align}
   		where $[\ \cdot\ ]$ denotes the floor function.
   	\end{theorem}
   	
   	\begin{proof}
   		($\implies$) Note that for every $0 \leq k \leq n-1$,
   		\begin{align}
   			P([nU] = k) &= P(k \leq nU < k + 1) \\
   			&= P(\frac{k}{n} \leq U < \frac{k+1}{n}) \\
   			&= \frac{k+1}{n} - \frac{k}{n} = \frac{1}{n}
   		\end{align}
   		($\impliedby$)
   		Suppose $[nU] \sim unif\{0, \cdots, n-1\}$, we are going to show
   		\begin{align}
   			\forall \alpha \in [0, 1], P(U \leq \alpha) = \alpha
   		\end{align}
   		Considering $r \in [0, 1) \cap \Q$ such that $r = \frac{k}{n}$ for some $\Z \ni k < n$.\\
   		Then
   		\begin{align}
   			P(U < r) &= P(nU < k) \\
   			&= \sum_{i=0}^{n-1} P(nU < k \land [nU] = i) \\
   			&= \sum_{i=0}^{n-1} P(0 \leq nU < k \land i \leq nU < i+1) \\
   			&= \sum_{i=0}^{k-1} P(0 \leq nU < k \land i \leq nU < i+1) \\
   			&= \sum_{i=0}^{k-1}P(i \leq nU < i + 1) \\
   			&= \sum_{i=0}^{k-1}P([nu] = i) \\
   			&= \frac{k}{n} = r
   		\end{align}
   		Therefore, $P(U < r) = r$ for every rational $r \in [0, 1)$. Because $\Q$ is dense in $\R$, every $x \in [0, 1]$ can be approximated using a rational number $r$ arbitrarily precisely. Hence, the result generalizes to $[0, 1]$.
   	\end{proof}
   	
   	\section{Conditional Probability}
   	\begin{definition}
   		Given a random variable $W$ on $\Omega$ with sequence of realizations $(w_n)$, let $A, B \subseteq \Omega$. Then the \textbf{conditional empirical relative frequency} for $A$ conditioned on $B$ is defined to be
   		\begin{align}
   			\hat{P}_{n}(W \in A | W \in B) &=\frac{I_{A}\left(w_{1}\right) I_{B}\left(w_{1}\right)+\cdots+I_{A}\left(w_{n}\right) I_{B}\left(w_{n}\right)}{I_{B}\left(w_{1}\right)+\cdots+I_{B}\left(w_{n}\right)} \\
   			&= \frac{\hat{P}_n(W \in A \cap B)}{\hat{P}_n(W \in B)}
   		\end{align}
   		whenever $\hat{P}_n(W \in B) \neq 0$.
   	\end{definition}
   	
   	\begin{definition}
   		Given random variable $W$ defined on sample space $\Omega$ and $A, B \subseteq \Omega$, the \textbf{conditional probability} of $A$ given $B$ is defined as
   		\begin{align}
   			P(W \in A | W \in B) &:= \lim_{n \to \infty} \frac{\hat{P}_n(W \in A \cap B)}{\hat{P}_n(W \in B)} \\
   			&= \frac{P(W \in A \cap B)}{P(W \in B)}
   		\end{align}
   		provided that $P(W \in B) \neq 0$.
   	\end{definition}
\end{document}




















