\documentclass{article}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1truein]{geometry}
\usepackage{tkz-graph}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{setspace}
\linespread{1.15}
\usepackage[margin=1truein]{geometry}

\counterwithin{equation}{section}
\counterwithin{figure}{section}

\pagestyle{fancy}
\lhead{STA347: Probability}

\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}


\title{STA347: Probability}
\date{\today}
\author{Tianyu Du}
\begin{document}
    \maketitle
    \tableofcontents
    \newpage
   	\section{Preliminaries}
   	\begin{definition}
   		A \textbf{process}\footnote{This is just a process, not necessarily a random process.} $W$ is a mechanism generating \textbf{outcomes} $w$ from a sample space $\Omega$. Any realized trail of process $W$ can be denoted as a potentially infinite sequence in $\Omega$:
   		\begin{align}
   			W: w_1, w_2, \cdots, w_n, \cdots
   		\end{align}
   	\end{definition}
   	
   	\begin{definition}
   		A \textbf{random variable} (extended process), $X := g(W)$, can be constructed from a process $W$ and a real-valued function $g: \Omega \to \R$.
   	\end{definition}
   	
   	\begin{definition}
   		Given a random variable $X = g(W)$, the \textbf{sample mean} (i.e. empirical expectation) of the first $n$ trials from a sequence of realizations, $g(w_1), \cdots, g(w_n), \cdots$, is defined to be
   		\begin{align}
   			\hat{\expe}_n g(W) &:= \frac{\sum_{i=1}^n g(w_i)}{n}
   		\end{align}
   	\end{definition}
   	
   	\begin{definition}
   		A process $W$ is said to be a \textbf{random process} if it satisfies the \emph{empirical law of large numbers}, in that, $\forall g \in \R^\Omega$:
   		\begin{enumerate}[(i)]
   			\item \emph{stability}: $(\hat{\expe}_ng(W))_{n\in \N}$ converges;
   			\item \emph{Invariance}: $\forall\ (w_n)_{n \in \N} \subseteq \Omega$, the limits of $(\hat{\expe}_ng(W))_{n\in \N}$ are the same.
   		\end{enumerate}
   	\end{definition}
   	
   	\begin{definition}
   		Let $W$ be a random process and $g \in \R^\Omega$, the \textbf{expected value} of $g(W)$ is defined as
   		\begin{align}
   			\expe g(W) &:= \lim_{n \to \infty} \hat{\expe}_ng(W)
   		\end{align}
   		the limit is well-defined given ELLN.
   	\end{definition}
   	
   	\begin{definition}
   		Let $W$ be a random process. For every $A \subseteq \Omega$, take $g := I_A \in \R^\Omega$, the \textbf{empirical relative frequencies} (i.e. empirical probability) is defined as
   		\begin{align}
   			\hat{P}(W \in A) &:= \hat{\expe}_n I_A(W)
   		\end{align}
   		Given ELLN, the limit is well-defined, then the \textbf{probability} is defined to be the limit:
   		\begin{align}
   			P(W \in A) &:= \lim_{n \to \infty} \hat{P}(W \in A)
   		\end{align}
   	\end{definition}
   	
   	\begin{remark}
   		The notation of expected values and probabilities on $W$ is well-defined only when $W$ satisfies the empirical law of large numbers, that is, $W$ is a random process. \\
   		Given $W$ defined on $\Omega$ satisfies ELLN, the behaviour of $W$ can be fully characterized by its \textbf{probability distribution}.
   		\begin{align}
   			W \sim P_W \tx{ on } \Omega
   		\end{align}
   	\end{remark}
















   	\section{Distributions}

   	\begin{definition}
   		$Z \sim unif\{0, \cdots, p-1\}$ if and only if
   		\begin{align}
   			P(Z=i) = P(Z=j)\quad \forall i, j \in \{0, \cdots, p-1\}
   		\end{align}
   	\end{definition}

   	\begin{definition}
   		A \textbf{standard uniform} is defined to be $\mc{U} \sim unif[0, 1]$ if and only if 
   		\begin{align}
   			P(\mc{U} \leq u) = u\ \forall u \in [0, 1]	
   		\end{align}
   	\end{definition}
   	
   	\begin{theorem}
   		If $U = \sum_{n=1}^\infty Z_i p^{-i}$, then the following are equivalent:
   		\begin{enumerate}[(i)]
   			\item $U \sim unif[0, 1]$;
   			\item $Z_i \overset{i.i.d.}{\sim} Z \overset{d}{=} unif\{0, \cdots, p-1\}$.
   		\end{enumerate}
   	\end{theorem}
   	
   	\begin{definition}
   		Two random processes $X, Y$ on a common sample space $\mc{X}$ are \textbf{identically distributed}, $X \overset{d}{=} Y$ if and only if
   		\begin{align}
   			\expect{g(X)} = \expect{g(Y)}\quad \forall g: \mc{X} \to \R
   		\end{align}
   	\end{definition}
   	\begin{proposition}
   		Specifically, for $A \overset{d}{=} B$, take $g = I_A$ where $A \subset \mc{X}$. It is evident that for every such subset, the probability \textbf{probability} as 
	   	\begin{align}
	   		\prob{X \in A} = \expect{I_A(X)} = \expect{I_A(Y)} = \prob{Y \in A}
	   	\end{align}
   	\end{proposition}
   	
   	\begin{theorem}[Invariance]
   		If $X \overset{d}{=} Y$, then 
   		\begin{align}
   			\varphi(X) \overset{d}{=} \varphi(Y)\quad \forall \varphi: \mc{X} \to \mc{Y}
   		\end{align}
   	\end{theorem}
   	\begin{proof}
   		\begin{align}
   			\expect{h \circ \varphi(X)} = \expect{h \circ \varphi(Y)}\quad \forall h: \mc{Y} \to \R
   		\end{align}
   	\end{proof}
   	
   	\begin{definition}
   		The \textbf{expectation} operator
   		\begin{align}
   			\mathbb{E}: \mc{R} \to \R \cup \{\pm\infty\} \cup \{ \tx{DNE} \}
   		\end{align}
   		where $\mc{R}$ is the space of \emph{real-valued} random processes.
   	\end{definition}
   	
   	\begin{proposition}
   		Let $W \sim unif\{1, \cdots, n\}$, then
   		\begin{align}
   			n + 1 - W &\overset{d}{=} W \\
   			\implies (n + 1 - W)^2 &\overset{d}{=} W^2 \\
   			\implies (n+1)^2 - 2(n+1)W + W^2 &\overset{d}{=} W^2 \\
   			\implies \expect{(n+1)^2 - 2(n+1)W + W^2} &= \expect{W^2} \\
   			\implies \expect{W} &= \frac{n+1}{2}
   		\end{align}
   	\end{proposition}
   	
   	\begin{proposition}
   		\begin{align}
   			(n+1-W)^3 &\overset{d}{=} W^3 \\
   			\implies 2 \expect{W^3} &= (n+1)^3 - 3(n+1)^2 \expect{W} + 3(n+1) \expect{W^2} \\
   			\implies 2 \expect{W^3} &= (n+1)^3 - 3(n+1)^2 \frac{n+1}{2} + 3(n+1) \expect{W^2} \\
   			\implies 2 \expect{W^3} &= - \frac{(n+1)^2}{2} + 3(n+1) \expect{W^2} \\
   			\implies \expect{W^3} &= n (\expect{W})^2
   		\end{align}
   	\end{proposition}
   	
   	\begin{proposition}
   		$\expect{W^4}$. \hl{TODO}
   	\end{proposition}
   	
   	\begin{definition}
   		$W \sim unif\{1, \cdots, n\}$, then the \emph{distance between} $W^2$ and $\expect{W^2}$ is defined as
   		\begin{align}
   			d(W^2, \expect{W^2}) &:= \sqrt{\expect{W^2 - \expect{W})^2}^2} = \sqrt{\var{W^2}} = \sigma_{W^2}
   		\end{align}
   	\end{definition}
   	
   	\begin{corollary}[Corollary of Jensen's Inequality]
   		\begin{align}
   			\expect{W^2} \geq (\expect{W})^2
   		\end{align}
   		and equality holds \ul{if and only if}
   		\begin{align}
   			\expect{(W - \expect{W})^2} &= 0
   		\end{align}
   		which is equivalent to
   		\begin{align}
   			P(W = \expect{W}) = 1
   		\end{align}
   	\end{corollary}
   	\begin{proof}
   		\begin{align}
	   		\var{W} = \expect{(W - \expect{W})^2} \geq 0
   		\end{align}
   	\end{proof}
   	
   	\begin{lemma}
   		$u = \sum_{i=1}^\infty z_i p^{-i}$, and let $z = (z_i: i \in \N) \in \dot{p}^\infty$, then 
   		\begin{align}
   			z_1 = b_1
   		\end{align}
   	\end{lemma}
\end{document}




















