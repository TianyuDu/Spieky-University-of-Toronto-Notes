\documentclass{article}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1truein]{geometry}
\usepackage{tkz-graph}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{setspace}
\linespread{1.15}
\usepackage[margin=1truein]{geometry}

\title{A Short Note on Reinforcement Learning}

\author{Tianyu Du}
\date{\today}

\begin{document}
	\maketitle
	\section{Notations}
	\begin{itemize}
		\item $Y^X$: The space of all functions $f: X \to Y$.
		\item $f(\ \cdot\ )$: functions.
		\item $F[\ \cdot\ ]$: Functionals.
		\item $\Delta(X)$: The space of all probability distributions over $X$.
	\end{itemize}
	
	\section{Setup}
	\begin{definition}
		A \textbf{Markov decision process} is a tuple $(\mc{S}, \mc{A}, \mc{P}, \mc{R}, \gamma)$.
		\begin{itemize}
			\item Space of states $\mc{S}$;
			\item Space of actions $\mc{A}$, $\mc{A}$ is assumed to be finite;
			\item Transition probability: $\mc{P}: \mc{S} \times \mc{A} \to \Delta(\mc{S})$;
			\item Immediate reward distribution: $R_t(S_{t}, S_{t+1}, A) \sim \mc{R}: \mc{S} \times \mc{S} \times \mc{A} \to \Delta(\R)$
			\item Discount factor: $\gamma \in [0, 1]$.
		\end{itemize}
	\end{definition}
	
	\begin{definition}
		A \textbf{policy} $\pi$ is a mapping from $\mc{S}$ to $\mc{A}$ (pure strategies) or $\Delta(\mc{A})$ (mixed strategies). \\
		Let $\Pi \equiv \Delta(\mc{A})^\mc{S}$ denote the collection of all policies.
	\end{definition}
	
	\section{Value Functions}
	\begin{definition}
	The \textbf{value function} $V[\cdot]: \Pi \to \R^\mc{S}$ is defined as
		\begin{align}
			V[\pi](s) := \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k} | S_{t}=s\right]
		\end{align}
	\end{definition}
	\begin{assumption}
		Assume $\mc{R}$ is deterministic and depends on $(S_t, A_t)$ only:
		\begin{align}
			\prob{\mc{R}(s, S_{t+1}, a) = r(s,a)} = 1\ \forall S_{t+1} \in \mc{S}
		\end{align}
	\end{assumption}
	\begin{assumption}[Markov property]
		The future depends on the past only through the current state. That is,
		\begin{align}
			\mc{P}(S_t, A_t) \perp A_t
		\end{align}
	\end{assumption}
	\par Apply the law of total expectation (LTE),
	\begin{align}
		V[\pi](s) &:= \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k} \Big| S_{t}=s\right] \\
		&= \sum_{a \in \mc{A}}
		\pi(a|s) \mathbb{E}_\pi \left[
		\sum_{k=0}^{\infty} \gamma^{k} R_{t+k} \Big| S_{t}=s, A_t = a
		\right] \tx{ (LTE)} \\
		&= \sum_{a \in \mc{A}}
		\pi(a|s)
		\int_{\mc{S}}
		\mc{P}(s'|s, a)
		\mathbb{E}_\pi
		\left[ \sum_{k=0}^{\infty} \gamma^{k} R_{t+k} \Big| S_{t}=s, A_t = a, S_{t+1} = s' \right]
		ds' \tx{ (LTE)} \\
		&= \sum_{a \in \mc{A}}
		\pi(a|s)
		\int_{\mc{S}}
		\mc{P}(s'|s, a)
		\mathbb{E}_\pi
		\left[r(s, a) + \sum_{k=1}^{\infty} \gamma^{k} R_{t+k} \Big| S_{t}=s, A_t = a, S_{t+1} = s' \right]
		ds' \\
		&= \sum_{a \in \mc{A}}
		\pi(a|s)
		\int_{\mc{S}}
		\mc{P}(s'|s, a)
		\mathbb{E}_\pi
		\left[r(s, a) + \gamma \sum_{k=0}^{\infty} \gamma^{k} R_{t+1+k} \Big|S_{t+1} = s' \right]
		ds' \\
		&= \sum_{a \in \mc{A}}
		\pi(a|s)
		\int_{\mc{S}}
		\mc{P}(s'|s, a)
		\left \{ r(s, a) +
		\gamma \mathbb{E}_\pi
		\left[ \sum_{k=0}^{\infty} \gamma^{k} R_{t+1+k} \Big|S_{t+1} = s' \right] \right \}
		ds' \\
		&= \sum_{a \in \mc{A}}
		\pi(a|s) \left[
		r(s, a) + 
		\gamma  \int_{\mc{S}}
		\mc{P}(s'|s, a)
		V[\pi](s')
		ds' \right ]
	\end{align}
	
	Similarly, conditioning $V[\pi](s)$ on $A_t = a$ gives 
	\begin{align}
		Q[\pi](s, a) &:= \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k} \Big| S_{t}=s, A_t = a\right] \\
		&= \int_{\mc{S}}
			\mc{P}(s'|s, a)
			\mathbb{E}_\pi
			\left[ \sum_{k=0}^{\infty} \gamma^{k} R_{t+k} \Big| S_{t}=s, A_t = a, S_{t+1} = s' \right]
			ds' \tx{ (LTE)} \\
		&= \int_{\mc{S}}
			\mc{P}(s'|s, a)
			\mathbb{E}_\pi
			\left[ r(s,a) + \gamma \sum_{k=1}^{\infty} \gamma^{k} R_{t+1+k} \Big| S_{t}=s, A_t = a, S_{t+1} = s' \right] ds' \\
		&= \int_{\mc{S}}
			\mc{P}(s'|s, a) \left\{
			r(s,a) + \gamma \mathbb{E}_\pi
			\left[\sum_{k=1}^{\infty} \gamma^{k} R_{t+1+k} \Big| S_{t+1} = s', A_{t+1} = \pi(s')\right] \right \}ds' \\
		&= \int_{\mc{S}}
			\mc{P}(s'|s, a) \left\{
			r(s,a) + \gamma Q[\pi](s', \pi(s')) \right \}ds' \\
		&= r(s,a) + \gamma \int_{\mc{S}}
			\mc{P}(s'|s, a) Q[\pi](s', \pi(s')) ds'
	\end{align}
	
	\section{Bellman Operators}
	\begin{definition}
		Define the \textbf{Bellman operator} $T[Q, \theta]: \mc{S} \times \mc{A} \to \R$ where $Q \in \R^{\mc{S} \times \mc{A}}$ and $\theta \in \Pi \equiv \Delta(\mc{A})^\mc{S}$ as 
		\begin{align}
			T[Q, \theta] (s, a) &:= r(s, a) + \gamma \int_\mc{S} 
			\mc{P}(s'|s, a) Q(s', \theta(s')) ds'
		\end{align}
	\end{definition}
	
	\begin{definition}
		Define the \textbf{optimal (pure) policy} $\pi^*(\cdot)$ as 
		\begin{align}
			\forall s \in \mc{S},\ \pi^*(s) \in \argmax_{a \in \mc{A}} Q(s, a)
		\end{align}
	\end{definition}
	
	\begin{definition}
		Define the \textbf{Bellman optimality operator} $T^*[Q]$ as 
		\begin{align}
			T^*[Q] &:= T[Q, \pi^*] \\
			&= r(s, a) + \gamma \int_\mc{S} 
			\mc{P}(s'|s, a) Q(s', \pi^*(s')) ds' \\
			&= r(s, a) + \gamma \int_\mc{S} 
			\mc{P}(s'|s, a) \max_{a' \in \mc{A}} Q(s', a') ds'
		\end{align}
	\end{definition}
	
	\begin{proposition}
		For every $\pi \in \Delta(\mc{A})^\mc{S}$,
		\begin{align}
			T[Q[\pi](\cdot); \pi](s, a) = Q[\pi](s, a)\ \forall (s, a) \in \mc{S} \times \mc{A}
		\end{align}
		That is, given each $\pi$, $Q[\pi]$ is the fixed point for the corresponding Bellman operator $T[\ \cdot\ ; \pi]$.
	\end{proposition}
	
	\begin{proof}
		Follows the definition of Bellman operator.
	\end{proof}
	
	\begin{theorem}
		For every $\pi \in \Delta(\mc{A})^\mc{S}$, the fixed point of the corresponding Bellman operator is unique.
	\end{theorem}
	
	\begin{corollary}
		In particular, take $\pi = \pi^*$, then $Q[\pi^*]$ is the unique fixed point for Bellman optimality operator.
	\end{corollary}
	
	\par Consequently, finding the unique $Q[\pi^*]$ from $T^*[\ \cdot\ ]$ would provide sufficient evidence to identify the optimal policy $\pi^*$.
	
	\begin{theorem}
		For every $\pi$, the corresponding Bellman operator is a contraction mapping.
	\end{theorem}
	
	\section{Value Iteration}
	
	\section{Q-Learning: Exploration vs. Exploitation}
\end{document}








