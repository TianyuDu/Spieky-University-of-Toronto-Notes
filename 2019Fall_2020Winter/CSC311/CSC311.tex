\documentclass{article}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1truein]{geometry}
\usepackage{tkz-graph}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{setspace}
\linespread{1.15}
\usepackage[margin=1truein]{geometry}

\counterwithin{equation}{section}
\counterwithin{figure}{section}

\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}

\title{CSC311: Introduction to Machine Learning}
\date{\today}
\author{Tianyu Du}
\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	
	\section{Decision Trees}
	\subsection{Entropy: An Information Theoretical Metric}
	\begin{definition}
		\textbf{Accuracy gain} from splitting $R$ into $R_1$ and $R_2$ based on loss $L(R)$:
		\begin{align}
			L(R) - \underbrace{\frac{|R_1|L(R_1) + |R_2|L(R_2)}{|R_1| + |R_2|}}_{\tx{New Loss}}
		\end{align}
		Typically, $L(R) = 1 - \id{\tx{\% correct classified}}$.
	\end{definition}
	
	\begin{definition}
		Given a random variable $X \sim p$, the \textbf{entropy} measures the amount of randomness/uncertainty in an arbitrary realization of $X$. 
		\begin{align}
			H(X) &:= \mathbb{E}_{X \sim p}[- \log_2 p(X)]
		\end{align}
	\end{definition}
	
	\begin{proposition}
		Uniform random variable has the highest entropy.
	\end{proposition}
	
	\begin{remark}
		Entropy is defined using $\log_2$ instead of $\ln$ because of an information theory perspective. In actual implementation of optimization algorithms, $\ln$ is used to derive gradient ascent rules. These two definitions are equivalent as optimization is invariant under positive monotone transformation.
	\end{remark}
	
	\begin{definition}
		Given joint distribution $(X, Y) \sim p(X, Y)$, the \textbf{entropy of joint distribution} is defined as
		\begin{align}
			H(X, Y) &:= \mathbb{E}_{(X, Y) \sim p(X, Y)} [
			- \log_2 p(X, Y)
			] \\
			&= - \sum_{x \in Im(X)} \sum_{y \in Im(Y)} p(x, y) \log_2 p(x, y)
		\end{align}
	\end{definition}
	
	\begin{definition}
		Given two random variables $X$ and $Y$, the \textbf{conditional entropy of $Y$ conditioned on specific realization of $X$} is defined to be 
		\begin{align}
			H(Y|X=x) &:= \mathbb{E}_{y \sim p(y|X=x)}[
				-\log_2 p(y|X=x)
			] \\
			&= - \sum_{y \in Im(Y)} p(y|X=x) \log_2 p(y|X=x)
		\end{align}
		The \textbf{expected conditional entropy}\footnote{This is independent of specific realization of $X$} is defined as 
		\begin{align}
			H(Y|X) &= \mathbb{E}_{X \sim p(x)} [H(Y|X)] \\
			&= \mathbb{E}_{X \sim p(x)} [\mathbb{E}_{y \sim p(y|X=x)}[
				-\log_2 p(y|X=x)]
			] \\
			&= \sum_{x \in X} p(x) H(Y|X=x) \\
			&= - \sum_{x \in X} p(x) \sum_{y \in Im(Y)} p(y|X=x) \log_2 p(y|X=x) \\
			&= - \sum_{x \in \mc{X}} \sum_{y \in \mc{Y}} p(x, y) \log_2 p(y | X = x) \\
			&= - \mathbb{E}_{(X, Y) \sim p(x, y)} [\log_2 p(Y|X)]
		\end{align}
	\end{definition}
	
	\begin{proposition}
		For every $X \in \Delta(\mc{X})$, $H(X) \geq 0$.
		\begin{proof}
			Immediate.
		\end{proof}
	\end{proposition}
	
	\begin{proposition}[Chain Rule]
		\begin{align}
			H(X, Y)=H(X | Y)+H(Y)=H(Y | X)+H(X)
		\end{align}
	\end{proposition}
	
	\begin{proof}
		\begin{align}
			H(X, Y) &= - \sum_{x \in \mc{X}} \sum_{y \in \mc{Y}} p(x, y) \log_2 p(x, y) \\
			&= - \sum_{x \in \mc{X}} \sum_{y \in \mc{Y}} p(x, y) \log_2 (p(x|y) p(y)) \\
			&= - \sum_{x \in \mc{X}} \sum_{y \in \mc{Y}} p(x, y) [\log_2 p(x|y) + \log_2 p(y)] \\
			&= - \sum_{x \in \mc{X}} \sum_{y \in \mc{Y}} p(x, y) \log_2 p(x|y) - \sum_{x \in \mc{X}} \sum_{y \in \mc{Y}} p(x, y) \log_2 p(y) \\
			&= H(X|Y) - \sum_{y \in \mc{Y}}\sum_{x \in \mc{X}} p(x, y) \log_2 p(y) \\
			&= H(X|Y) - \sum_{y \in \mc{Y}}\left\{ \log_2 p(y) \sum_{x \in \mc{X}} p(x, y)\right\} \\
			&= H(X|Y) - \sum_{y \in \mc{Y}} (\log_2 p(y)) p(y) \\
			&= H(X|Y) + H(Y)
		\end{align}
	\end{proof}
	
	\begin{proposition}
		If $X \perp Y$, then knowing $X$ does not provide extra information (i.e. reduce entropy) of $Y$. That is $H(Y|X) = H(Y)$.
	\end{proposition}
	
	\begin{proof}
		\begin{align}
			H(Y|X) &= - \sum_{x \in \mc{X}} \sum_{y \in \mc{Y}} p(x, y) \log_2 p(y | X = x) \\
			&= - \sum_{x \in \mc{X}} \sum_{y \in \mc{Y}} p(x, y) \log_2 p(y) \\
			&= - \sum_{y \in \mc{Y}} \log_2 p(y) \sum_{X \in \mc{X}} p(x, y) \\
			&= - \sum_{y \in \mc{Y}} p(y) \log_2 p(y) \\
			&= H(Y)
		\end{align}
	\end{proof}
	
	\begin{proposition}
		$Y$ becomes deterministic by knowing $Y$, that is, $H(Y|Y) = 0$.
	\end{proposition}
	
	\begin{proof}
		\begin{align}
			H(Y|Y) &= - \sum_{y \in \mc{Y}} \sum_{y \in \mc{Y}} p(y, y) \log_2 p(y | Y = y) \\
			&= - \sum_{y \in \mc{Y}} \sum_{y \in \mc{Y}} p(y, y) \log_2 1 \\
			&= 0
		\end{align}
	\end{proof}
	
	\begin{proposition}
		By knowing $X$, the uncertainty about $Y$ is reduced: $H(Y|X) \leq H(Y)$.
	\end{proposition}
	
	\begin{proof}
		\todo{Proof this}
	\end{proof}
	
	\begin{definition}
		The \textbf{information gain} in $Y$ due to $X$, or \textbf{mutual information} of $X$ and $Y$ is defined to be
		\begin{align}
			IG(Y|X) &:= H(Y) - H(Y|X)
		\end{align}
		When $X$ is completely uninformative about $Y$: $H(Y|X) = H(Y)$, then $IG(Y|X) = 0$.\\
		When $X$ is completely information about $Y$: $H(Y|X) = 0$ (deterministic), then $IG(Y|X) = H(Y)$.
	\end{definition}
	
	\begin{proposition}[Symmetry of Information Gain]
		\begin{align}
			IG(Y|X) &:= H(Y) - H(Y|X) \\
			&= H(X, Y) - H(X|Y) - H(Y|X) \\
			&= H(Y|X) + H(X) - H(X|Y) - H(Y|X) \\
			&= H(X) - H(X|Y) \\
			&= IG(X|Y)
		\end{align}
	\end{proposition}
	
	\section{Decision Trees}
	\begin{remark}
		Greedy algorithms don't necessarily yield the global optimum.
	\end{remark}
	\begin{definition}
		An \textbf{ensemble} of predictors is a set of predictors whose individual decisions are combined in some way to predict new examples
	\end{definition}
	
	\section{Bias-Variance Decomposition}
	\par Let $(\vex\upi, y\upi) \in \mc{X} \times \R$ denote one training instance such that
	\begin{align}
		(\vex\upi, y\upi) \overset{i.i.d.}{\sim} p_{\tx{sample}}
	\end{align}
	where $p_{\tx{sample}} \in \Delta (\mc{X} \times \R)$. \\
	Fixing $N \in \N$, one can construct a new distribution $p_{\tx{dataset}} \in \Delta (\mc{X} \times \R)^N$ such that 
	\begin{align}
		(\vex\upi, y\upi)_{i=1}^N =: \mc{D} \sim p_{\tx{dataset}}
	\end{align}
	Given a (random) training set $\mc{D}$, a (random) classifier function $h_{\mc{D}} \in \mc{H}$ is generated. \\
	For every \emph{query point} $\vex \in \mc{X}$, the prediction $h_\mc{D}(\vex)$ is therefore random. \\
	Suppose $y$ is not deterministic in $x$, then the expected mean squared error when the model is applied on new instances sampled from $p_{\tx{sample}}$ is
	\begin{align}
		\mathbb{E}_{\vex, y, \mc{D}}[(h_\mc{D}(\vex) - y)^2] &= \mathbb{E}_{\mc{D}}[
		\mathbb{E}_{\vex, y} [(h_\mc{D}(\vex) - y)^2|\mc{D}]
		] \\
		&= \mathbb{E}_{\mc{D}}[
		\mathbb{E}_{\vex, y} [(h_\mc{D}(\vex) - \mathbb{E}_y[y|x] + \mathbb{E}_y[y|x] - y)^2|\mc{D}]
		] \\
		&= \mathbb{E}_{\mc{D}} \{
		\mathbb{E}_x[
		\mathbb{E}_y[
		(h_\mc{D}(\vex) - \mathbb{E}_y[y|x])^2
		]
		] \\
		&\quad+ 2 \mathbb{E}_{x,y}[(h_\mc{D}(\vex) - \mathbb{E}_y[y|x]) (\mathbb{E}_y[y|x] - y)] \\
		&\quad+ \mathbb{E}_{x, y} (\mathbb{E}_y[y|x] - y)^2
		\} \\
		&= \mathbb{E}_{\mc{D}} \{
		\mathbb{E}_x[
		(h_\mc{D}(\vex) - \mathbb{E}_y[y|x])^2
		]
		\\
		&\quad+ 2 \mathbb{E}_{x,y}[(h_\mc{D}(\vex) - \mathbb{E}_y[y|x]) (\mathbb{E}_y[y|x] - y)] \\
		&\quad+ \mathbb{E}_{x, y} [(\mathbb{E}_y[y|x] - y)^2]
		\} \\
	\end{align}
	By law of iterative expectation,
	\begin{align}
		\mathbb{E}_{x,y}[(h_\mc{D}(\vex) - \mathbb{E}_y[y|x]) (\mathbb{E}_y[y|x] - y)] &= \mathbb{E}_x [
		\mathbb{E}_y[
		(h_\mc{D}(\vex) - \mathbb{E}_y[y|x]) (\mathbb{E}_y[y|x] - y)
		]] \\
		&= \mathbb{E}_x[
		(h_\mc{D}(\vex) - \mathbb{E}_y[y|x]) (\mathbb{E}_y[y|x] - \mathbb{E}_y[y])
		] \\
		&= 0
	\end{align}
	By dropping irrelevant expectation operators, 
	\begin{align}
		\Delta &= \mathbb{E}_\mc{D}[\mathbb{E}_x[(h_\mc{D}(\vex) - \mathbb{E}_y[y|x])^2]] + \underbrace{\mathbb{E}_{x, y}[(\mathbb{E}_y[y|x] - y)^2]}_{\tx{Bayes Error } \varepsilon^2} \\
		&= \mathbb{E}_\mc{D}[\mathbb{E}_x[
		(h_\mc{D}(\vex) - \mathbb{E}_y[y|x])^2
		]] + \varepsilon^2 \\
		&= \mathbb{E}_\mc{D}[\mathbb{E}_x[
		(h_\mc{D}(\vex) - \mathbb{E}_\mc{D}[h_\mc{D}(x)|x] + \mathbb{E}_\mc{D}[h_\mc{D}(x)|x] -  \mathbb{E}_y[y|x])^2
		]] + \varepsilon^2
	\end{align}
	Note that
	\begin{align}
		\mathbb{E}_\mc{D}[
		\mathbb{E}_x[
		(h_\mc{D}(\vex) - \mathbb{E}_\mc{D}[h_\mc{D}(x)|x])
		(\mathbb{E}_\mc{D}[h_\mc{D}(x)|x] -  \mathbb{E}_y[y|x])
		]] &= 0
	\end{align}
	The first component reduced to zero after applying law of iterative expectation. \\
	Therefore,
	\begin{align}
		\Delta &= \mathbb{E}_\mc{D}[
		\mathbb{E}_x[
		(h_\mc{D}(\vex) - \mathbb{E}_\mc{D}(h_\mc{D}(x)|x))^2
		]] + 
		\mathbb{E}_\mc{D}[
		\mathbb{E}_x[
		(\mathbb{E}_\mc{D}[h_\mc{D}(x)|x] -  \mathbb{E}_y[y|x])^2
		]] + \varepsilon^2 \\
		&= \underbrace{\mathbb{E}_{x, \mc{D}}[
		(h_\mc{D}(\vex) - \mathbb{E}_\mc{D}(h_\mc{D}(x)|x))^2
		]}_{\tx{Variance}}
		+ 
		\underbrace{\mathbb{E}_x[
		(\mathbb{E}_\mc{D}[h_\mc{D}(x)|x] -  \mathbb{E}_y[y|x])^2
		]}_{\tx{Bias Squared}} + \varepsilon^2 \\
	\end{align}
	\section{Bagging}
	\section{Bayes Optimality}
	\begin{theorem}
		\begin{align}
			\argmin_{y} \expect{(y-t)^2|\vex} = \expect{t|\vex}
		\end{align}
		where $t \sim p(t|\vex)$.
	\end{theorem}
\end{document}






































