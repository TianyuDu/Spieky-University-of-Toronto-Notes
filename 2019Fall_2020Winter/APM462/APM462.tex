\documentclass{article}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1truein]{geometry}
\usepackage{tkz-graph}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{setspace}
\linespread{1.15}
\usepackage[margin=1truein]{geometry}

\counterwithin{equation}{section}
\counterwithin{figure}{section}

\pagestyle{fancy}
\lhead{APM462: Nonlinear Optimization}

\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}

\title{APM462: Nonlinear Optimization}
\date{\today}
\author{Tianyu Du}
\begin{document}
    \maketitle
    \tableofcontents
    \newpage
    
    \section{Preliminaries}
    \subsection{Mean Value Theorems and Taylor Approximations.}
    \begin{definition}
        Let $f: S \subset \R^n \to \R$, the \textbf{gradient} of $f$ at $x \in S$, if exists, is a vector $\nabla f(x) \in \R^n$ characterized by the property
        \begin{equation}
            \lim_{v \to 0} \frac{f(x+v) - f(x) - \nabla f(x) \cdot v}{\norm{v}} = 0
        \end{equation}
    \end{definition}
    
    \begin{theorem}[The First Order of Mean Value Theorem]
        Let $f$ be a $C^1$ real-valued function defined on $\R^n$, then for any $x, v \in \R^n$, there exists some $\theta \in (0, 1)$ such that
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x + \theta v) \cdot v
        \end{equation}
    \end{theorem}
    
    \begin{proof}
        Let $x, v \in \R^n$, define $g(t): \R \to \R := f(x + tv)$, which is $C^1$. By the mean value theorem on $\R^\R$, there exists $\theta \in (0, 1)$ such that $g(0+1) = g(0) + g'(\theta)(1-0)$, that is, $f(x+v) = f(x) + g'(\theta)$. Note that $g'(\theta) = \nabla(x + \theta v) \cdot v$, what desired is immediate.
    \end{proof}
    
    \begin{proposition}[The First Order Taylor Approximation]
        Let $f: \R^n \to \R$ be a $C^1$ function, then
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + o(\norm{v})
        \end{equation}
        that is
        \begin{equation}
            \lim_{\norm{v} \to 0} \frac{f(x + v) - f(x) - \nabla f(x) \cdot v}{\norm{v}} = 0
        \end{equation}
    \end{proposition}
    
    \begin{proof}
    	By the mean value theorem, $\exists \theta \in (0, 1)$ such that $f(x+v) - f(x) = \nabla f(x + \theta v) \cdot v$. The limit becomes $\lim_{\norm{v} \to 0} \frac{[\nabla f(x + \theta v) - \nabla f(x)] \cdot v}{\norm{v}} = \lim_{\norm{v} \to 0; x + \theta v \to x} \frac{[\nabla f(x + \theta v) - \nabla f(x)] \cdot v}{\norm{v}}$. Since $f \in C^1$, $\lim_{x + \theta v \to x} \nabla f(x + \theta v) = \nabla f(x)$. And $\frac{v}{\norm{v}}$ is a unit vector, and every component of it is bounded, as the result, the limit of inner product vanishes instead of explodes.
    \end{proof}
    
    \begin{theorem}[The Second Order Mean Value Theorem]
        Let $f: \R^n \to \R$ be a $C^2$ function, then for any $x, v \in \R^n$, there exists $\theta \in (0, 1)$ satisfying
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + \frac{1}{2} v' H_f(x + \theta v)\ v
        \end{equation}
        where $H_f$ is the Hessian matrix of $f$, may also be written as $\nabla^2 f$.
    \end{theorem}
    
    \begin{proposition}[The Second Order Taylor Approximation]
        Let $f: \R^n \to \R$ be a $C^2$ function, and $x, v \in \R^n$, then
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + \frac{1}{2} v' H_f(x)\ v + o(\red{\norm{v}^2})
        \end{equation}
        that is
        \begin{equation}
            \lim_{\norm{v} \to 0} \frac{
                f(x + v) - f(x) - \nabla f(x) \cdot v - \frac{1}{2} v' H_f(x)\ v
            }{\norm{v}^2} = 0
        \end{equation}
    \end{proposition}
    
    \begin{proof}
    	By the second mean value theorem, there exists $\theta \in (0, 1)$ such that the limit is equivalent to
    	\begin{align}
    		\lim_{\norm{v} \to 0} \frac{1}{2} \left(\frac{v}{\norm{v}}\right)' \left[H_f(x + \theta v) - H_f(x)\right] \frac{v}{\norm{v}}
    	\end{align}
    	Since $f \in C^2$, the limit of $\left[H_f(x + \theta v) - H_f(x)\right]$ is in fact $\textbf{0}_{n \times n}$. And every component of unit vector $\frac{v}{\norm{v}}$ is bounded, the quadratic form converges to zero as an immediate result.
    \end{proof}
    
    It is often noted that the gradient at a particular $x_0 \in dom(f) \subset \R^n$ gives the direction $f$ increases most rapidly.
        Let $x_0 \in dom(f)$, and $v$ be a \ul{unit vector} representing a \emph{feasible direction} of change. That is, there exists $\delta > 0$ such that $x_0 + t v \in dom(f)$ $\forall t \in [0, \delta)$. Then the rate of change of $f$ along feasible direction $v$ can be written as
        \begin{equation}
            \left.\frac{d}{dt}\right\vert_{t=0} f(x_0 + tv) = \nabla f(x_0) \cdot v = \norm{\nabla f(x_0)}\ \norm{v} \cos(\theta)
        \end{equation}
        where $\theta = \angle (v, \nabla f(x_0)$. And the derivative is maximized when $\theta=0$, that is, when $v$ and $\nabla f$ point the same direction.
    
    \subsection{Implicit Function Theorem}
    \begin{theorem}[Implicit Function Theorem]
        Let $f: \R^{n+1} \to \R$ be a $C^1$ function, let $(a, b) \in \R^n \times \R$ such that $f(a, b) = 0$. If $\nabla f(a, b) \neq 0$, then $\{(x, y) \in \R^n \times \R:f(x, y) = 0\}$ is locally a graph of a function $g: \R^n \to \R$.
    \end{theorem}
    
    \begin{remark}
        $\nabla f(x_0) \perp \tx{ level set of $f$ near $x_0$}$.
    \end{remark}
    
    \section{Convexity}
    \subsection{Terminologies}
    \begin{definition}
        Set $\Omega \subset \R^n$ is \textbf{convex} if and only if 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in [0, 1],\ \lambda x_1 + (1 - \lambda) x_2 \in \Omega
        \end{equation}
    \end{definition}
    
    \begin{definition}
        A function $f: \Omega \subset \R^n \to \R$ is \textbf{convex} if and only if $\Omega$ is convex, and 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in [0, 1],\ f\left(\lambda x_1 + (1- \lambda) x_2 \right) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
        \end{equation}
    \end{definition}
    
    \begin{definition}
        A function $f: \Omega \subset \R^n \to \R$ is \textbf{strictly convex} if and only if $\Omega$ is convex and 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in (0, 1),\ f\left(\lambda x_1 + (1- \lambda) x_2 \right) < \lambda f(x_1) + (1 - \lambda) f(x_2)
        \end{equation}
    \end{definition}
    
    \subsection{Basic Properties of Convex Functions}
    
    \begin{definition}
        A function $f: \Omega \to \R$ is \textbf{concave} if and only if $-f$ is \textbf{convex}.
    \end{definition}
    
    \begin{proposition}
        \begin{enumerate}[(i)]
            \item If $f_1, f_2$ are convex on $\Omega$, so is $f_1 + f_2$;
            \item If $f$ is convex on $\Omega$, then for any $a > 0$, $af$ is also convex on $\Omega$;
            \item Any \textbf{sub-level/lower contour set} of a convex function $f$ 
            \begin{align}
            	SL(c) := \{x \in \R^n: f(x) \leq c\}
            \end{align}
            is convex.
        \end{enumerate}
    \end{proposition}
    
    \begin{proof}[Proof of (iii).]
    	Let $c \in \R$, and $x_1 ,x_2 \in SL(c)$. Let $s \in [0, 1]$. Since $x_1, x_2 \in SL(c)$, and $f(\cdot)$ is convex, $f(s x_1 + (1-s) x_2) \leq s f(x_1) + (1-s) f(x_2) \leq s c + (1-s) c = c$. Which implies $s x_1 + (1-s) x_2 \in SL(c)$.
    \end{proof}
    
    \begin{example}
    	$f(x): \R^n \to \R := \norm{x}$ is convex.
    \end{example}
    
    \begin{proof}
    	Note that for any $u, v \in \R^n$, by triangle inequality, $\norm{u - (-v)} \leq \norm{u - 0} + \norm{0 - (-v)} = \norm{u} + \norm{v}$. Consequently, let $u, v \in \R^n$ and $s \in [0, 1]$, then $\norm{s u + (1-s) v} \leq \norm{su} + \norm{(1-s) v} = s \norm{u} + (1-s) \norm{v}$. Therefore, $\norm{\cdot}$ is convex.
    \end{proof}
	
	\subsection{Characterization of $C^1$ Convex Functions}
	
    \begin{theorem}
        Let $f \in C^1$, then $f$ is convex on a convex set $\Omega$ \ul{if and only if}
        \begin{equation}
            \forall x, y \in \Omega,\ f(y) \geq f(x) + \nabla f(x) \cdot (y - x)
        \end{equation}
        that is, \emph{the linear approximation is never an overestimation of value of $f$}.
    \end{theorem}
    \begin{proof}
        ($\implies$) Suppose $f$ is convex on a convex set $\Omega$. Then $f(sy + (1-s) x) \leq sf(y) + (1-s)f(x)$ for every $x, y \in \Omega$ and $s \in [0, 1]$, which implies, for every $s \in (0, 1]$:
        \begin{equation}
            \frac{f(sy + (1-s) x) - f(x)}{s} \leq f(y) - f(x)
        \end{equation}
        By taking the limit of $s \to 0$,
        \begin{align}
            \lim_{s \to 0} \frac{f(x + s(y-x)) - f(x)}{s} &\leq f(y) - f(x) \\
            \implies \left.\frac{d}{ds}\right\vert_{s=0} f(x + s(y-x)) &\leq f(y) - f(x) \\
            \implies \nabla f(x) \cdot (y-x) &\leq f(y) - f(x)
        \end{align}
        ($\impliedby$) Let $x_0, x_1 \in \Omega$, let $s \in [0, 1]$. Define $x^* := s x_0 + (1-s) x_1$, then 
        \begin{align}
        	f(x_0) &\geq f(x^*) + \nabla f(x^*) \cdot (x_0 - x^*) \\
        	\implies f(x_0) &\geq f(x^*) + \nabla f(x^*) \cdot [(1-s)(x_0 - x_1)]
        \end{align}
        Similarly,
        \begin{align}
        	f(x_1) &\geq f(x^*) + \nabla f(x^*) \cdot (x_1 - x^*) \\
        	\implies f(x_1) &\geq f(x^*) + \nabla f(x^*) \cdot [s(x_1 - x_0)]
        \end{align}
        Therefore, $sf(x_0) + (1-s)f(x_1) \geq f(x^*)$.
    \end{proof}
    
    \begin{theorem}
        $f \in C^2$ is a convex function on a convex set $\Omega \subset \R^n$ \ul{if and only if} $\nabla^2 f(x) \geq 0$ for all $x \in \Omega$.
    \end{theorem}
    
    \begin{proof}
        ($\impliedby$) Using the second-order Taylor's theorem and positive definiteness of $H_f$. \\
        ($\implies$) Prove by contradiction.
    \end{proof}
    
    \subsection{Minimum and Maximum of Convex Functions}
    \begin{theorem}
        Let $\Omega \subset \R^n$ be a convex set, and $f: \Omega \to \R$ is a convex function. Suppose set $\Gamma := \min_{x \in \Omega} f(x) \neq \varnothing$, then $\Gamma$ is convex, further, any local minimum of $f$ is the global minimum.
    \end{theorem}
    
    \begin{theorem}
        Let $\Omega \subset \R^n$ be a convex set, and $f: \Omega \to \R$ is a convex function. Then 
        \begin{equation}
            \max_{x \in \Omega} f(x) = \max_{x \in \partial \Omega} f(x)
        \end{equation}
    \end{theorem}
    
    \begin{proof}
        As we assumed, $\Omega$ is closed, therefore $\partial \Omega \subseteq \Omega$. Hence, $\max_{x \in \Omega} f \geq \max_{x \in \partial \Omega} f$. Suppose $\max_{x \in \Omega} f > \max_{x \in \partial \Omega} f$, let $x^* := \argmax_{x \in \Omega} f \in \Omega^{int}$. Then we can construct a string line through $x^*$ and intersects $\partial \Omega$ at two points, $y_1, y_2 \in \partial \Omega$, such that $x^* = s y_1 + (1-s) y_2$ for some $s \in (0, 1)$. Further, since $f$ is convex, $f(x^*) \leq s f(y_1) + (1-s) f(y_2) \leq s \max_{\partial \Omega} f + (1-s) \max_{\partial \Omega} f = \max_{\partial \Omega} f$, which leads to a contradiction.
    \end{proof}
    
    \begin{proposition}
        Let $f$ be continuous function on $\R^n$, then the level set and sub-level set of $f$ are closed. Further, since the intersection of arbitrary closed sets is closed, so the set defined by collection of continuous functions $\{h_i; g_j\}$
        \begin{equation}
            \{x \in \R^n: h_i(x) = 0 \land g_j(x) \leq 0\ \forall i, j\}
        \end{equation}
        is closed.
    \end{proposition}
    
    \begin{theorem}[Extreme Value Theorem]
        Let $\Omega \subset \R^n$ be a compact set, and $f: \Omega \to \R$ is continuous, then $f$ attains its maximum and minimum on $\Omega$.
    \end{theorem}
    
    \begin{corollary} 
        Let $f: \R^n \to \R$ be a continuous function, if there exists $a \in \R^n$ such that $f(x) \geq f(a)$ for every $x \notin \mc{B}(r, a)$, then $f$ attains its minimum in $\mc{B}(r, a)$.
    \end{corollary}
    
    \begin{proposition}
        When $\Omega = \R^n$, the unconstrained minimization has the following properties
        \begin{enumerate}[(i)]
            \item $\argmax f = \argmin (-f)$;
            \item $\max f = - \min (-f)$
        \end{enumerate}
    \end{proposition}
    
\end{document}