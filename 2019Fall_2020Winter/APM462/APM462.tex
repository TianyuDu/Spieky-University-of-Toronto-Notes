\documentclass{article}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1truein]{geometry}
\usepackage{tkz-graph}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{setspace}
\linespread{1.15}
\usepackage[margin=1truein]{geometry}

\counterwithin{equation}{section}
\counterwithin{figure}{section}

\pagestyle{fancy}
\lhead{APM462: Nonlinear Optimization}

\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}

\title{APM462: Nonlinear Optimization}
\date{\today}
\author{Tianyu Du}
\begin{document}
    \maketitle
    \tableofcontents
    \newpage
    
    \section{Preliminaries}
    \subsection{Mean Value Theorems and Taylor Approximations.}
    \begin{definition}
        Let $f: S \subset \R^n \to \R$, the \textbf{gradient} of $f$ at $x \in S$, if exists, is a vector $\nabla f(x) \in \R^n$ characterized by the property
        \begin{equation}
            \lim_{v \to 0} \frac{f(x+v) - f(x) - \nabla f(x) \cdot v}{\norm{v}} = 0
        \end{equation}
    \end{definition}
    
    \begin{theorem}[The First Order of Mean Value Theorem]
        Let $f$ be a $C^1$ real-valued function defined on $\R^n$, then for any $x, v \in \R^n$, there exists some $\theta \in (0, 1)$ such that
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x + \theta v) \cdot v
        \end{equation}
    \end{theorem}
    
    \begin{proof}
        Let $x, v \in \R^n$, define $g(t): \R \to \R := f(x + tv)$, which is $C^1$. By the mean value theorem on $\R^\R$, there exists $\theta \in (0, 1)$ such that $g(0+1) = g(0) + g'(\theta)(1-0)$, that is, $f(x+v) = f(x) + g'(\theta)$. Note that $g'(\theta) = \nabla(x + \theta v) \cdot v$, what desired is immediate.
    \end{proof}
    
    \begin{proposition}[The First Order Taylor Approximation]
        Let $f: \R^n \to \R$ be a $C^1$ function, then
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + o(\norm{v})
        \end{equation}
        that is
        \begin{equation}
            \lim_{\norm{v} \to 0} \frac{f(x + v) - f(x) - \nabla f(x) \cdot v}{\norm{v}} = 0
        \end{equation}
    \end{proposition}
    
    \begin{proof}
    	By the mean value theorem, $\exists \theta \in (0, 1)$ such that $f(x+v) - f(x) = \nabla f(x + \theta v) \cdot v$. The limit becomes $\lim_{\norm{v} \to 0} \frac{[\nabla f(x + \theta v) - \nabla f(x)] \cdot v}{\norm{v}} = \lim_{\norm{v} \to 0; x + \theta v \to x} \frac{[\nabla f(x + \theta v) - \nabla f(x)] \cdot v}{\norm{v}}$. Since $f \in C^1$, $\lim_{x + \theta v \to x} \nabla f(x + \theta v) = \nabla f(x)$. And $\frac{v}{\norm{v}}$ is a unit vector, and every component of it is bounded, as the result, the limit of inner product vanishes instead of explodes.
    \end{proof}
    
    \begin{theorem}[The Second Order Mean Value Theorem]
        Let $f: \R^n \to \R$ be a $C^2$ function, then for any $x, v \in \R^n$, there exists $\theta \in (0, 1)$ satisfying
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + \frac{1}{2} v' H_f(x + \theta v)\ v
        \end{equation}
        where $H_f$ is the Hessian matrix of $f$, may also be written as $\nabla^2 f$.
    \end{theorem}
    
    \begin{proposition}[The Second Order Taylor Approximation]
        Let $f: \R^n \to \R$ be a $C^2$ function, and $x, v \in \R^n$, then
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + \frac{1}{2} v' H_f(x)\ v + o(\red{\norm{v}^2})
        \end{equation}
        that is
        \begin{equation}
            \lim_{\norm{v} \to 0} \frac{
                f(x + v) - f(x) - \nabla f(x) \cdot v - \frac{1}{2} v' H_f(x)\ v
            }{\norm{v}^2} = 0
        \end{equation}
    \end{proposition}
    
    \begin{proof}
    	By the second mean value theorem, there exists $\theta \in (0, 1)$ such that the limit is equivalent to
    	\begin{align}
    		\lim_{\norm{v} \to 0} \frac{1}{2} \left(\frac{v}{\norm{v}}\right)' \left[H_f(x + \theta v) - H_f(x)\right] \frac{v}{\norm{v}}
    	\end{align}
    	Since $f \in C^2$, the limit of $\left[H_f(x + \theta v) - H_f(x)\right]$ is in fact $\textbf{0}_{n \times n}$. And every component of unit vector $\frac{v}{\norm{v}}$ is bounded, the quadratic form converges to zero as an immediate result.
    \end{proof}
    
    It is often noted that the gradient at a particular $x_0 \in dom(f) \subset \R^n$ gives the direction $f$ increases most rapidly.
        Let $x_0 \in dom(f)$, and $v$ be a \ul{unit vector} representing a \emph{feasible direction} of change. That is, there exists $\delta > 0$ such that $x_0 + t v \in dom(f)$ $\forall t \in [0, \delta)$. Then the rate of change of $f$ along feasible direction $v$ can be written as
        \begin{equation}
            \left.\frac{d}{dt}\right\vert_{t=0} f(x_0 + tv) = \nabla f(x_0) \cdot v = \norm{\nabla f(x_0)}\ \norm{v} \cos(\theta)
        \end{equation}
        where $\theta = \angle (v, \nabla f(x_0)$. And the derivative is maximized when $\theta=0$, that is, when $v$ and $\nabla f$ point the same direction.
    
    \subsection{Implicit Function Theorem}
    \begin{theorem}[Implicit Function Theorem]
        Let $f: \R^{n+1} \to \R$ be a $C^1$ function, let $(a, b) \in \R^n \times \R$ such that $f(a, b) = 0$. If $\nabla f(a, b) \neq 0$, then $\{(x, y) \in \R^n \times \R:f(x, y) = 0\}$ is locally a graph of a function $g: \R^n \to \R$.
    \end{theorem}
    
    \begin{remark}
        $\nabla f(x_0) \perp \tx{ level set of $f$ near $x_0$}$.
    \end{remark}
    
    \section{Convexity}
    \subsection{Terminologies}
    \begin{definition}
        Set $\Omega \subset \R^n$ is \textbf{convex} if and only if 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in [0, 1],\ \lambda x_1 + (1 - \lambda) x_2 \in \Omega
        \end{equation}
    \end{definition}
    
    \begin{definition}
        A function $f: \Omega \subset \R^n \to \R$ is \textbf{convex} if and only if $\Omega$ is convex, and 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in [0, 1],\ f\left(\lambda x_1 + (1- \lambda) x_2 \right) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
        \end{equation}
    \end{definition}
    
    \begin{definition}
        A function $f: \Omega \subset \R^n \to \R$ is \textbf{strictly convex} if and only if $\Omega$ is convex and 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in (0, 1),\ f\left(\lambda x_1 + (1- \lambda) x_2 \right) < \lambda f(x_1) + (1 - \lambda) f(x_2)
        \end{equation}
    \end{definition}
    
    \subsection{Basic Properties of Convex Functions}
    
    \begin{definition}
        A function $f: \Omega \to \R$ is \textbf{concave} if and only if $-f$ is \textbf{convex}.
    \end{definition}
    
    \begin{proposition}
        \begin{enumerate}[(i)]
            \item If $f_1, f_2$ are convex on $\Omega$, so is $f_1 + f_2$;
            \item If $f$ is convex on $\Omega$, then for any $a > 0$, $af$ is also convex on $\Omega$;
            \item Any \textbf{sub-level/lower contour set} of a convex function $f$ 
            \begin{align}
            	SL(c) := \{x \in \R^n: f(x) \leq c\}
            \end{align}
            is convex.
        \end{enumerate}
    \end{proposition}
    
    \begin{proof}[Proof of (iii).]
    	Let $c \in \R$, and $x_1 ,x_2 \in SL(c)$. Let $s \in [0, 1]$. Since $x_1, x_2 \in SL(c)$, and $f(\cdot)$ is convex, $f(s x_1 + (1-s) x_2) \leq s f(x_1) + (1-s) f(x_2) \leq s c + (1-s) c = c$. Which implies $s x_1 + (1-s) x_2 \in SL(c)$.
    \end{proof}
    
    \begin{example}
    	$f(x): \R^n \to \R := \norm{x}$ is convex.
    \end{example}
    
    \begin{proof}
    	Note that for any $u, v \in \R^n$, by triangle inequality, $\norm{u - (-v)} \leq \norm{u - 0} + \norm{0 - (-v)} = \norm{u} + \norm{v}$. Consequently, let $u, v \in \R^n$ and $s \in [0, 1]$, then $\norm{s u + (1-s) v} \leq \norm{su} + \norm{(1-s) v} = s \norm{u} + (1-s) \norm{v}$. Therefore, $\norm{\cdot}$ is convex.
    \end{proof}
	
	\subsection{Characteristics of $C^1$ Convex Functions}
	
    \begin{theorem}[$C^1$ criterions for convexity]
        Let $f \in C^1$, then $f$ is convex on a convex set $\Omega$ \ul{if and only if}
        \begin{equation}
            \forall x, y \in \Omega,\ f(y) \geq f(x) + \nabla f(x) \cdot (y - x)
        \end{equation}
        that is, \emph{the linear approximation is never an overestimation of value of $f$}.
    \end{theorem}
    \begin{proof}
        ($\implies$) Suppose $f$ is convex on a convex set $\Omega$. Then $f(sy + (1-s) x) \leq sf(y) + (1-s)f(x)$ for every $x, y \in \Omega$ and $s \in [0, 1]$, which implies, for every $s \in (0, 1]$:
        \begin{equation}
            \frac{f(sy + (1-s) x) - f(x)}{s} \leq f(y) - f(x)
        \end{equation}
        By taking the limit of $s \to 0$,
        \begin{align}
            \lim_{s \to 0} \frac{f(x + s(y-x)) - f(x)}{s} &\leq f(y) - f(x) \\
            \implies \left.\frac{d}{ds}\right\vert_{s=0} f(x + s(y-x)) &\leq f(y) - f(x) \\
            \implies \nabla f(x) \cdot (y-x) &\leq f(y) - f(x)
        \end{align}
        ($\impliedby$) Let $x_0, x_1 \in \Omega$, let $s \in [0, 1]$. Define $x^* := s x_0 + (1-s) x_1$, then 
        \begin{align}
        	f(x_0) &\geq f(x^*) + \nabla f(x^*) \cdot (x_0 - x^*) \\
        	\implies f(x_0) &\geq f(x^*) + \nabla f(x^*) \cdot [(1-s)(x_0 - x_1)]
        \end{align}
        Similarly,
        \begin{align}
        	f(x_1) &\geq f(x^*) + \nabla f(x^*) \cdot (x_1 - x^*) \\
        	\implies f(x_1) &\geq f(x^*) + \nabla f(x^*) \cdot [s(x_1 - x_0)]
        \end{align}
        Therefore, $sf(x_0) + (1-s)f(x_1) \geq f(x^*)$.
    \end{proof}
    
    \begin{theorem}[$C^2$ criterion for convexity]
        $f \in C^2$ is a convex function on a convex set $\Omega \subset \R^n$ \ul{if and only if} $\nabla^2 f(x) \succcurlyeq 0$ for all $x \in \Omega$.
    \end{theorem}
    
    \begin{remark}
    	When $f$ is defined on $\R$, the $C^2$ criterion becomes $f''(x) \geq 0$.
    \end{remark}
    
    \begin{proof}
        ($\impliedby$) Suppose $\nabla^2 f(x) \succcurlyeq 0$ for every $x \in \Omega$, let $x, y \in \Omega$. By the second order MVT,
        \begin{align}
        	f(y) &= f(x) + \nabla f(x) \cdot (y - x) + \frac{1}{2} (y-x)^T \nabla^2 f(x + s (y - x)) (y - x)\tx{ for some } s \in [0, 1] \\
        	&\implies f(y) \geq f(x) + \nabla f(x) \cdot (y - x)
        \end{align}
        So $f$ is convex by the $C^1$ criterion of convexity.\\
        ($\implies$) Let $v \in \R^n$. Suppose, for contradiction, that for some $x \in \Omega$, $\nabla^2 f(x) \centernot \succcurlyeq 0$. If such $x \in \partial \Omega$, note that $v^T \nabla^2 f(\cdot) v$ is continuous because $f \in C^2$, then there exists $\varepsilon > 0$ such that $\forall x' \in V_\varepsilon(x) \cap \Omega^{int},\ v^T \nabla^2 f(x') v < 0$. Hence, one may assume with loss of generality that such $x \in \Omega^{int}$. Because $x \in \Omega^{int}$, exists $\varepsilon' > 0$, such that $V_{\varepsilon'}(x) \subseteq \Omega^{int}$. Define $\hat{v} := \frac{v}{\sqrt{\varepsilon'}}$, then for every $s \in [0, 1]$, $\hat{v}^T \nabla^2 f(x + s\hat{v}) \hat{v} < 0$. Let $y = x + \hat{v}$, by the mean value theorem, $f(y) = f(x) + \nabla f(x) \cdot (y - x) + \frac{1}{2} (y - x)^T \nabla^2 f (x + s (y - x)) (y - x)$ for some $s \in [0, 1]$. This implies $f(y) < f(x) + \nabla f(x) \cdot (y - x)$, which contradicts the $C^1$ criterion for convexity.
    \end{proof}
    
    \subsection{Minimum and Maximum of Convex Functions}
    \begin{theorem}
        Let $\Omega \subset \R^n$ be a convex set, and $f: \Omega \to \R$ is a convex function. Let
        \begin{align}
        	\Gamma := \left\{x \in \Omega: f(x) = \min_{x \in \Omega} f(x) \right\} \equiv \argmin_{x \in \Omega} f(x)
        \end{align}
        If $\Gamma \neq \varnothing$, then 
        \begin{enumerate}[(i)]
        	\item $\Gamma$ is convex;
        	\item any local minimum of $f$ is the global minimum.
        \end{enumerate}
    \end{theorem}
    
    \begin{proof}[Proof (i).]
    	Let $x, y \in \Gamma$, $s \in [0, 1]$, then $sx+(1-s)y \in \Omega$ because $\Omega$ is convex. Since $f$ is convex, $f(sx+(1-s)y) \leq sf(x) + (1-s)f(y) = \min_{x \in \Omega} f(x)$. The inequality must be equality since it would contradicts the fact that $x, y \in \Gamma$. Therefore, $sx+(1-s)y \in \Gamma$.
    \end{proof}
    
    \begin{proof}[Proof (ii).]
    	Let $x \in \Omega$ be a local minimizer for $f$, but assume, for contradiction, it is not a global minimizer. That is, there exists some other $y$ such that $f(y) < f(x)$. Since $f$ is convex, \begin{align}
 				f(x + t(y-x)) = f((1-t)x + ty) \leq (1-t)f(x) + tf(y) < f(x)
			\end{align}
			for every $t \in (0, 1]$. Therefore, for every $\varepsilon > 0$, there exists $t^* \in (0, 1]$ such that $x + t^*(y-x) \in V_\varepsilon(x)$ and $f(x + t^*(y-x)) < f(x)$, this contradicts the fact that $x$ is a local minimum.
    \end{proof}
    
    \begin{theorem}
        Let $\Omega \subset \R^n$ be a convex and compact  set, and $f: \Omega \to \R$ is a convex function. Then 
        \begin{equation}
            \max_{x \in \Omega} f(x) = \max_{x \in \partial \Omega} f(x)
        \end{equation}
    \end{theorem}
    
    \begin{proof}
        As we assumed, $\Omega$ is closed, therefore $\partial \Omega \subseteq \Omega$. Hence, $\max_{x \in \Omega} f \geq \max_{x \in \partial \Omega} f$. Suppose $\max_{x \in \Omega} f > \max_{x \in \partial \Omega} f$, let $x^* := \argmax_{x \in \Omega} f \in \Omega^{int}$. Then we can construct a straight line through $x^*$ and intersects $\partial \Omega$ at two points, $y_1, y_2 \in \partial \Omega$, such that $x^* = s y_1 + (1-s) y_2$ for some $s \in (0, 1)$. Further, since $f$ is convex, $\max_{x \in \Omega}f(x) = f(x^*) \leq s f(y_1) + (1-s) f(y_2) \leq s \max_{\partial \Omega} f + (1-s) \max_{\partial \Omega} f = \max_{\partial \Omega} f$, which leads to a contradiction. Therefore, $\max_{x \in \Omega} f = \max_{x \in \partial \Omega} f$.
    \end{proof}
    
    \begin{proposition}
    	For $p, g > 1$ and $\frac{1}{p} + \frac{1}{g} = 1$,
    	\begin{align}
    		|ab| \leq \frac{1}{p} |a|^p + \frac{1}{g}|b|^g
    	\end{align}
    \end{proposition}
    
    \begin{proof}
    	\begin{align}
    		(-\log) |ab| &= (-\log) |a| + (-\log) |b| \\
    		&= \frac{1}{p} (-\log) |a|^p + \frac{1}{g} (-\log) |b|^p \\
    		(\because (-\log) \tx{ is convex})\ &\geq (-\log)\left( \frac{1}{p} |a|^p + \frac{1}{g} |b|^p \right)
    	\end{align}
    	And since $(-\log)$ is monotonically decreasing,
    	\begin{align}
    		|ab| \leq \frac{1}{p} |a|^p + \frac{1}{g} |b|^p
    	\end{align}
    \end{proof}
    
    \begin{corollary}
    	\begin{align}
    		|ab| \leq \frac{|a|^2 + |b|^2}{2}
    	\end{align}
    \end{corollary}
	
	\section{Finite Dimensional Optimization}
	\subsection{Unconstraint Optimization}
    \begin{theorem}[Extreme Value Theorem]
        Let $f: \R^n \to \R$ is \ul{continuous} and $K \subset \R^n$ be a \ul{compact} set, then the minimization problem $\min_{x \in K} f(x)$ has a solution.
    \end{theorem}
    
    \begin{remark}
    	$f: \Omega \to \R$ is convex does not imply $f$ is continuous.
    \end{remark}
    
    \begin{proposition}
    	A convex function $f$ defined on a \ul{convex open} set is continuous.
    \end{proposition}
    
    \begin{proof}
    	Let $f: \Omega \to \R$ be a convex function, where $\Omega \subset \R^n$ is open.
    	\hl{TODO}
    \end{proof}
    
    \begin{corollary}
    	A convex function $f$ defined on an \ul{open interval} in $\R$ is continuous.
    \end{corollary}
    
    \begin{proof}
    	See homework 1, using squeeze theorem.
    \end{proof}
    
    \begin{proof}[Proof of EVT.]
    	Let $f: K \to \R$ be a continuous function defined on a compact set $K$.\\
    	WLOG, we only prove the existence of $\min f$, since the existence of max can be easily proven by applying the exact same argument on $-f$. Because $K$ is compact, the continuity of $f$ implies $f(K)$ is compact. By the completeness axiom of $\R$, $m := \inf_{x \in K} f(x)$ is well-defined. There exists a sequence $(x_i) \subset K$, such that $(f(x_i)) \to m$. Because $K$ is compact, there exists a subsequence $(x_{ik})$ of $(x_i)$ converges to some limit $x^* \in K$. Because $f$ is continuous, $(f(x_{ik})) \to f(x^*)$, which is a subsequence of the convergent sequence $(f(x_i))$, and they must converge to the same limit. Hence, $f(x^*) = m$, and the infimum is attained at $x^* \in K$.
    \end{proof}
    
    \begin{theorem}[Heine–Borel]
    	Let $K \subset \R^n$, then $K$ is compact (every open cover of $K$ has a finite sub-cover) $\iff$ $K$ is closed and bounded.
    \end{theorem}
    
	\begin{proposition}
		Let $\{h_i\}$ and $\{g_i\}$ be sets of continuous functions on $\R^n$, the the set of all points in $\R^n$ that satisfy
		\begin{equation}
			\begin{cases}
				h_i(x) = 0\ \forall i\\
				g_j(x) \leq 0\ \forall j
			\end{cases}
		\end{equation}
		is a closed set (intersection of finitely many closed sets). Moreover, if the qualified set is also bounded, then it is compact.
	\end{proposition}
	
	\begin{proof}
		For every equality constraint $h_i$, it can be represented as the conjunction of two inequality constraint, namely $h_i^\alpha (x) := -h_i(x) \leq 0 \land h_i^\beta (x) := h_i(x) \leq 0$. Then the constraint collection is equivalent to
		\begin{align}
			\begin{cases}
				h_i^\alpha (x) \leq 0\ \forall i \\
				h_i^\beta (x) \leq 0\ \forall i \\
				g_j(x) \leq 0\ \forall j
			\end{cases}
		\end{align}
		The subset of $\R^n$ qualified by each individual constraint is closed by the property of continuous functions (i.e. the continuous function's pre-image of closed set is closed). And the intersection of arbitrarily many closed sets is closed.
	\end{proof}
	
	\begin{example}
		The set $\{(x, y) \in \R^2: x^2 - y^2 - 1 = 0\}$ is closed and bounded, therefore it is compact.
	\end{example}
    
    \begin{remark}
    	Computer algorithms for solving minimization problems try to construct a sequence of $(x_i)$ such that $f(x_i)$ decreases to $\min f$ rapidly.
    \end{remark}
    
%    \begin{corollary}
%        Let $f: \R^n \to \R$ be a continuous function, if there exists $a \in \R^n$ such that $f(x) \geq f(a)$ for every $x \notin \mc{B}(r, a)$, then $f$ attains its minimum in $\mc{B}(r, a)$.
%    \end{corollary}

    \par The optimization problems investigated in this section can be formulated as
    \begin{align}
    	\min_{x \in \Omega} f(x)
    \end{align}
    where $\Omega \subset \R^n$. Typically, for simplicity, $\Omega$ are often $\R^n$, an open subset of $R^n$, or the closure of some open subset of $\R^n$.
    \par Everything above minimization discussed in this section is applicable to maximization as well using the proposition below.

    \begin{proposition}
        When $\Omega = \R^n$, the unconstrained minimization has the following properties
        \begin{enumerate}[(i)]
            \item $\argmax f = \argmin (-f)$;
            \item $\max f = - \min (-f)$
        \end{enumerate}
    \end{proposition}
    
    \begin{proof}
    	\emph{Omitted.}
    \end{proof}
   	
   	\begin{definition}
   		A function $f: \Omega \to \R$ has \textbf{local minimum} at $x_0 \in \Omega$ if
   		\begin{align}
   			\exists \varepsilon > 0\ s.t.\ \forall x \in V_\varepsilon(x_0) \cap \Omega\ f(x_0) \leq f(x)
   		\end{align}
   		$f$ attains \textbf{strictly local minimum} at $x_0$ if
   		\begin{align}
   			\exists \varepsilon > 0\ s.t.\ \forall x \in V_\varepsilon(x_0) \cap \Omega \backslash \{x_0\}\ f(x_0) < f(x)
   		\end{align}
   		$f$ attains \textbf{global minimum} at $x_0$ if
   		\begin{align}
   			\forall x \in \Omega\ f(x_0) \leq f(x)
   		\end{align}
   		$f$ attains \textbf{strict global minimum} at $x_0$ if
   		\begin{align}
   			\forall x \in \Omega \backslash \{x_0\} \ f(x_0) < f(x)
   		\end{align}
   		\ul{Note that strict global minimum is always unique.}
   	\end{definition}
   	
   	\begin{theorem}[Necessary Condition for Local Minimum]
   		Let $C^1 \ni f: \Omega \to \R$, let $x_0 \in \Omega$ be a local minimum of $f$, then for every \emph{feasible direction} $v$ at $x_0$,
   		\begin{align}
   			\nabla f(x_0) \cdot v \geq 0
   		\end{align}
   	\end{theorem}
   	
   	\begin{definition}
   		For $x_0 \in \Omega \subset \R^n$,  $v \in \R^n$ is a \textbf{feasible direction}at $x_0$ if
   		\begin{align}
   			\exists \overline{s} > 0\ s.t.\ \forall s \in [0, \overline{s}], x_0 + s v \in \Omega
   		\end{align}
   	\end{definition}
   	
   	\begin{proof}[Proof of Necessary Condition]
   		Let $x_0 \in \Omega$ be a local minimum, and let $v$ be a 
   		Define auxiliary function $g(s) := f(x + sv)$. And since $g$ attains minimum at $s=0$, there exists some $\overline{s} > 0$ such that 
   		\begin{align}
   			g(s) - g(0) \geq 0\ \forall s \in [0, \overline{s}]
   		\end{align}
   		Therefore
   		\begin{align}
   			g'(0) := \lim_{s \to 0} \frac{g(s) - g(0)}{s - 0} \geq 0
   		\end{align}
   		The alternative form of derivative can be derived using chain rule as
   		\begin{align}
   			g'(0) = \nabla f(x + sv) \cdot v\ |_{s=0} = \nabla f(x) \cdot v
   		\end{align}
   		By combing the two identities above, $\nabla f(x) \cdot v \geq 0$.
   	\end{proof}
   	
   	\begin{proof}[Alternative Proof of Necessary Condition (not that rigorous)]
   		The prove is almost immediate, if there exists a feasible direction $v^*$ such that $\nabla f(x_0) \cdot v^* < 0$, for every $\varepsilon > 0$, one can construct $x' := x^* + s v^*$ with sufficiently small $s$ so that $x' \in V_\varepsilon(x^*) \cap  \Omega$ and $f(x') < f(x^*)$.
   	\end{proof}
   	
   	\begin{corollary}
   		When $\Omega$ is open, then $x_0$ is a local minimum $\implies \nabla f(x_0) = 0$.
   	\end{corollary}
   	\begin{proof}
   		Since $\Omega$ is open, any sufficiently small $v \neq 0$ such that both $v$ and $-v$ are feasible directions at $x_0$, applying the necessary condition on both $v$ and $-v$ provides the equality.
   	\end{proof}
   	
   	\begin{example}
   		Minimize $f(x, y)=x^{2}-x y+y^{2}-3 y$ over $\Omega = \R^2$.
   	\end{example}
   	
   	\begin{example}
   		Minimize $f(x, y)=x^{2}-x+y+x y$ over $\Omega = \max\{(x, y) \in \R^2: x, y \geq 0\}$.
   	\end{example}
   	
   	\begin{theorem}[Second Order Necessary Condition for Local Minimum]
   		Let $C^2 \ni f: \Omega \to \R$, let $x_0 \in \Omega$ be a local minimum of $f$, then for every non-zero feasible direction $v$ at $x_0$,
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) \cdot v \geq 0$;
   			\item $\nabla f(x_0) \cdot v = 0 \implies v^T \nabla^2 f(x_0) v \geq 0$.
   		\end{enumerate}
   	\end{theorem}
   	
   	\begin{proof}
   		Let $x_0$ be a local minimum and $v$ be a feasible direction at $\Omega$, and $s \in (0, \overline{s}]$. The first statement is the immediate result of the first order necessary condition. Now suppose $\nabla f(x_0) = 0$, by the Taylor's theorem,
   		\begin{align}
   			0 \leq f(x_0 + sv) - f(x_0) &= s \nabla f(x_0) \cdot v + \frac{s^2}{2} v^T \nabla^2 f(x_0) v + o(s^2) \\
   			&=\frac{s^2}{2} v^T \nabla^2 f(x_0) v + o(s^2)
   		\end{align}
   		Since $s^2 > 0$, divide both sides by $s^2$ and take limit,
   		\begin{align}
   			\lim_{s \to 0} \frac{f(x_0 + sv) - f(x_0)}{s^2} &= \lim_{s \to 0} 
   			\left \{\frac{1}{2} v^T \nabla^2 f(x_0) v + \frac{o(s^2)}{s^2} \right\}\\
   			&= \frac{1}{2} v^T \nabla^2 f(x_0) v + \lim_{s \to 0} \frac{o(s^2)}{s^2} \\
   			&= \frac{1}{2} v^T \nabla^2 f(x_0) v \geq 0
   		\end{align}
   	\end{proof}
   	
	\begin{example}
		$f(x, y) = x^2 - xy + y^2 - 3y: \Omega = \R^2 \to \R$. Then at $(x_0, y_0) = (1, 2)$, 
		\begin{align}
			\nabla f(x_0, y_0) &= (2x_0 - y, -x_0 + 2y_0 - 3) = (0, 0) \\
			\nabla^2 f(x_0, y_0) &= 
			\begin{pmatrix}
				2 & -1 \\ -1 & 2
			\end{pmatrix} \succcurlyeq 0 
		\end{align}
	\end{example}
	
	\begin{definition}
		Let $A \in \R^{n \times n}$, $A$ is 
		\begin{enumerate}[(i)]
			\item \textbf{Positive definite} ($A \succ 0$) if $x^T A x > 0\ \forall x \neq 0$, if and only if all eigenvalues $\lambda_i > 0$;
			\item \textbf{Positive Semi-definite} ($A \succcurlyeq 0$) if $x^T A x \geq\  \forall x \in \R^n$, if and only if all eigenvalues $\lambda_i \geq 0$.
		\end{enumerate}
	\end{definition}
   	
   	\begin{theorem}[Sylvester's Criterion]
   		Let $A \in \R^{n \times n}$ be a Hermitian matrix (i.e. $A = \overline{A^T}$), then
   		\begin{enumerate}
   			\item $A \succ 0$ $\iff$ all \emph{leading principal minors} have positive determinants;
   			\item $A \succcurlyeq 0$ $\iff$ all leading principal minors have non-negative determinants.
   		\end{enumerate}
   	\end{theorem}
   	
%   	\begin{example}
%   		Let $f(x, y) = x^2 - x + y + xy$ defined on $\Omega = \R^2_+$. Consider the  point $(x_0, y_0) = (1/2, 0)$ found, any feasible direction $(v, w)$ at $(x_0, y_0)$ satisfies $w \geq 0$. $\nabla f(1/2, 0) = (0 , 2/3)$.
%   		\begin{align}
%   			\nabla f(1/2, 0) &= (0, \frac{3}{2}).
%   		\end{align}
%   	\end{example}
   	
   	\begin{theorem}[Second Order Sufficient Condition for Interior Local Minima]
   		Let $C^2 \ni f: \Omega \to \R$, for some $x_0 \in \Omega$, if
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) = 0$,
   			\item \emph{(and)} $\nabla^2 f(x_0) \succcurlyeq 0$,
   		\end{enumerate}
   		then $x_0$ is a \ul{strictly local minimizer}.
   	\end{theorem}
   	
   	\begin{lemma}
   		Suppose $\nabla^2 f(x_0)$ is positive definite, then 
   		\begin{align}
   			\exists a > 0\ s.t.\ v^T \nabla^2 f(x_0) v \geq a \norm{v}^2\quad \forall v
   		\end{align}
   	\end{lemma}
   	
   	\begin{proof}[Proof of the Lemma]
   		Recall that a squared matrix $Q$ is called \textbf{orthogonal} when every column and row of it is an orthogonal unit vector. So that for every orthogonal matrix $Q$, $Q^T Q = I$, which implies $Q^T = Q^{-1}$. Further, note that 
   		\begin{align}
   			\norm{Qv}^2 &= (Qv)^T (Qv)
   			= v^T Q^T Q v 
   			= \norm{v}^2 \\
   			\implies \norm{Qv} &= \norm{v}\ \forall v \in \R^n
   		\end{align}
   		Let $v \in \R^n$, consider the eigenvector decomposition of $\nabla^2 f(x_0)$, let $w$ satisfy $v = Qw$:
   		\begin{align}
   			Q^T \nabla^2 f(x_0) Q &= \tx{diag}(\lambda_1, \cdots, \lambda_n) \\
   			\implies v^T \nabla^2 f(x_0) v &= (Qw)^T \nabla^2 f(x_0) (Qw) \\
   			&= w^T Q^T \nabla^2 f(x_0) Q w \\
   			&= w^T \tx{diag}(\lambda_1, \cdots, \lambda_n) w \\
   			&= \lambda_1 w_1^2 + \cdots + \lambda_n w_n^2
   		\end{align}
   		Let $a := \min\{\lambda_1, \cdots, \lambda_n\}$,
   		\begin{align}
   			... \geq a \norm{w}^2 = a\norm{Q^T v}^2 = a \norm{v}^2
   		\end{align}
   	\end{proof}
   	
   	\begin{proof}[Proof of the Theorem.]
   		Let $x \in \Omega$, suppose $\nabla f(x_0) = 0$ and $\nabla^2 f(x_0) \succcurlyeq 0$. By the second order Taylor approximation,
   		\begin{align}
   			f(x_0 + v) - f(x_0) &= \nabla f(x_0)^T v + \frac{1}{2} v^T \nabla^2 f(x_0) v + o(\norm{v}^2) \\
   			&= \frac{1}{2} v^T \nabla^2 f(x_0) v + o(\norm{v}^2) \\
   			&\geq \frac{a}{2} \norm{v}^2 + o(\norm{v}^2) \tx{ for some }a > 0 \\
   			&= \norm{v}^2 \left (
   			\frac{a}{2} + \frac{o(\norm{v}^2)}{\norm{v}} \right) \\
   			&>0\ \tx{ for sufficiently small } v
   		\end{align}
   		Therefore, $f(x_0) < f(x)\ \forall x \in V_\varepsilon(x_0)$.
   	\end{proof}
   	
   	\subsection{Equality Constraints: Lagrangian Multiplier}
   	\subsubsection{Tangent Space to a (Hyper) Surface at a Point}
   	\begin{definition}
   		A \textbf{surface} $\mc{M} \subset \R^n$ is defined as
   		\begin{align}
   			\mc{M} := \left \{
   			x \in \R^n : h_i (x) = 0\ \forall i
   			\right \}
   		\end{align}
   		where $h_i$ are all $C^1$ functions.
   	\end{definition}
   	
   	\begin{definition}
   		A \textbf{differentiable curve} on a surface $\mc{M}$ is a $C^1$ function mapping from $(-\varepsilon, \varepsilon)$ to $\mc{M}$. \\
   		\emph{Remark: in previous calculus courses, differentiable curves are often referred to as parameterizations.}
   	\end{definition}
   	
   	Let $x(s)$ be a differentiable curve on $\mc{M}$ passes through $x_0 \in \mc{M}$, WLOG, $x(0) = x_0$. Then vector
   	\begin{align}
   		v := \frac{d}{ds} \bigg \vert_{s=0} x(s)
   	\end{align}
   	touches $\mc{M}$ \emph{tangentially}.
   	
   	\begin{definition}
   		Any vector $v$ generated by some differentiable curve on $\mc{M}$ and takes above form is a $\textbf{tangent vector}$ on $\mc{M}$ through $x_0$.
   	\end{definition}
   	
   	\begin{definition}
   		The set of all tangent vectors is defined to be the \textbf{tangent space} to $\mc{M}$ at $x_0$:
   		\begin{align}
   			T_{x_0} \mc{M} := \left \{
   			v \in \R^n
   			:
   			v := \frac{d}{ds} \bigg \vert_{s=0} x(s) \tx{ for some } x(\cdot) \in \mc{M}^{(-\varepsilon, \varepsilon)}\ s.t.\ x(0) = x_0
   			\right \}
   		\end{align}
   	\end{definition}
   	
   	\begin{example}
   		Define 
   		\begin{align}
   			\mc{M} := \left\{x \in \R^2: \norm{x}_2 = 1 \right\}
   		\end{align}
   		By defining $C^1$ functions $g(x) := \norm{x}^2_2  - 1$, $\mc{M}$ is a surface. The tangent space of $\mc{M}$ at $x_0$ is
   		\begin{align}
   			T_{x_0} \mc{M} = \left \{
   			v \in \R^n:
   			\inner{v}{x_0} = 0
   			\right \}
   		\end{align}
   	\end{example}
   	
   	\begin{definition}
   		Let $\mc{M}$ be a surface defined using $C^1$ functions, a point $x_0 \in \mc{M}$ is a \textbf{regular point} of the constraints if 
   		\begin{align}
   			\{\nabla h_1(x_0), \cdots, \nabla h_k(x_0)\}
   		\end{align}
   		are linearly independent.
   	\end{definition}
   	
   	\begin{notation}
   		Define
   		\begin{align}
   			T_{x_0} := \{x \in \R^n: \inner{x_0}{\nabla h_i(x_0)}\ \forall i \in [k]\}
   		\end{align}
   	\end{notation}
   	
   	\begin{example}[Counter example]
   		Define
   		\begin{align}
   			\mc{M} := \left \{
   			(x, y) \in \R^2:
   			h(x,y) = xy = 0
   			\right \}
   		\end{align}
   		Then it is easy to verify that $(0,0)$ is not a regular point. And
   		\begin{align}
   			T_{0,0} &= \{(x, y) \in \R^2: (x, y) \cdot (0,0) = 0\} = \R^2 \\
   			\neq T_{0,0}\mc{M} &= \{(x, y) \in \R^2: x = 0 \lor y = 0\}
   		\end{align}
   	\end{example}
   	
   	\begin{theorem}
   		Suppose $x_0$ is a \emph{regular point} of $\mc{M} := \{h_i(x) = 0, i=1,\cdots,k\}$, then $T_{x_0} = T_{x_0} \mc{M}$.
   	\end{theorem}
   	
   	\begin{proof}
   		\emph{Show} $T_{x_0} \mc{M} \subset T_{x_0}$.\\
   		Suppose $x_0$ is a regular point of $\mc{M}$.
   		Let $v \in T_{x_0} \mc{M}$, then there exists some differentiable curve $x(\cdot):V_\varepsilon(0) \to \mc{M}$ such that $x(0) = x_0$, such that
   		\begin{align}
   			v &= \frac{d}{ds} \bigg \vert_{s=0} x(s)
   		\end{align}
   		Note that $h_i(x(s)) = 0$ is constant for every $i \in [k]$, therefore
   		\begin{align}
   			\frac{d}{ds} \bigg \vert_{s=0} h_i(x(s))
   		\end{align}
   		By the chain rule, 
   		\begin{align}
   			\nabla h_i(x_0)\cdot v = 0\ \forall i
   		\end{align}
   		Therefore $v \in T_{x_0}$. \\
   		\emph{Show} $T_{x_0} \subset T_{x_0} \mc{M}$.
   		\begin{enumerate}[(i)]
   			\item $x_0$ is regular $\implies T_{x_0} \mc{M}$ is a vector space;
   			\item $T_{x_0} = \tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\}^\perp$.
   		\end{enumerate}
   		\emph{Show} $T_{x_0} \subset \tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\}^\perp$: \\
   		Let $v \in T_{x_0}$, then $v \perp \nabla h_i(x_0)$ for every $i$. Therefore $v$ is orthogonal to every linear combination of $\nabla h_i(x_0)$, and therefore orthogonal to the span.\\
   		\emph{Show} $\tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\}^\perp \subset T_{x_0}$: \\
   		Let $v$ in the perp of the span, then $v$ is orthogonal to every basis of the span, so $v \in T_{x_0}$.
   	\end{proof}
   	
   	\begin{lemma}
   		Let $f, h_1, \cdots, h_k \in C^1$ defined on \ul{open} subset $\Omega \subset \R^n$. Define $\mc{M} := \{x \in \R^n: h_i(x) = 0\ \forall i \}$. Suppose $x_0 \in \mc{M}$ is a local minimum of $f$ on $\mc{M}$, then 
   		\begin{align}
   			\nabla f(x_0) \perp T_{x_0} \mc{M}
   		\end{align}
   	\end{lemma}
   	
   	\begin{proof}
   		WLOG $\Omega = \R^n$, take $v \in T_{x_0} \mc{M}$. Then there exists some differentiable curve $x$ on $\mc{M}$ satisfying $v = x'(0)$. Because $x_0$ is a local minimum of $f$ on $\Omega$, $s=0$ is a local minimum of $f(x(s))$, moreover, it is an interior minimum. By chain rule and the necessary condition of local minimum,
   		\begin{align}
   			D f(x(0)) &= \nabla f (x(0)) \cdot x'(0) = 0 \\
   			\implies \nabla f(x_0) \cdot v &= 0
   		\end{align}
   		Therefore $\nabla f(x_0) \perp T_{x_0} \mc{M}$.
   	\end{proof}
   	
   	\begin{theorem}[Lagrange Multipliers: First Order Necessary Condition]
   		Let $f, h_1, \cdots, h_k \in C^1$ defined on open subset $\Omega \subset \R^n$. Let $x_0$ be a regular point of the constraint set $\mc{M} := \bigcap_{i=1}^k h^{-1}_i(0)$. Suppose $x_0$ is a local minimum of $\mc{M}$, then there exists $\lambda_1, \cdots, \lambda_k \in \R$ such that
   		\begin{align}
   			\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) = 0
   		\end{align}
   		\emph{Remark: if we define Lagrangian $\mc{L}(x, \lambda_i) := f(x) + \sum_{i=1}^k h_i(x)$, then the theorem says the local minimum is a critical point of $\mc{L}$.}
   	\end{theorem}
   	
   	\begin{proof}
   		Because $x_0$ is a regular point, then by previous lemma, $\nabla f(x_0) \perp T_{x_0} \mc{M}$. Moreover,
   		\begin{align}
   			T_{x_0} \mc{M} = T_{x_0} = \left (\tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\} \right )^\perp
   		\end{align}
   		Also, because $x_0$ is a local minimum, 
   		\begin{align}
   			\nabla f(x_0) \perp T_{x_0} \mc{M}
   		\end{align}
   		Therefore, $\nabla f(x_0) \in (T_{x_0} \mc{M})^\perp = \left (\tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\} \right )^{\perp \perp} = \tx{span}\{\nabla h_1 (x_0), \cdots, \nabla h_k(x_0)\}$, where the last equality holds in finite dimensional cases. Hence, it is obvious that we can write $\nabla f(x_0)$ as a linear combination of $\{\nabla h_i(x_0)\}$.
   	\end{proof}
   	
   	\begin{theorem}[Second Order Necessary Condition]
   		Let $f, h_i \in C^2$, if $x_0$ is a local minimum on previously defined surface $\mc{M}$, then there exists Lagrangian multipliers $\{\lambda_i\}$ such that
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) = 0$ ($\nabla_x \mc{L} = 0$);
   			\item And $\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0) \succcurlyeq 0$ \red{on $T_{x_0} \mc{M}$} ($\nabla_x^2 \mc{L} \succcurlyeq 0$).
   		\end{enumerate}
   		\emph{Remark: whenever $x_0$ is a local minimum, it must be a critical point of $\mc{L}$, and $\mc{L}$ is positive semidefinite on the tangent space at $x_0$.}
   	\end{theorem}
   	
   	\begin{proof}
   		The first result is exactly the same as the first order condition proven above. \\
   		To show the second result, let $x(s) \in \mc{M}$ be an arbitrary differentiable curve on $\mc{M}$ such that $x(0) = x_0$. Then,
   		\begin{align}
   			\frac{d}{ds} f(x(s)) &= \nabla f(x(s)) \cdot x'(s) \\
   			\frac{d^2}{ds^2} f(x(s)) &= x'(s)^T \nabla^2 f(x(s)) x'(s) + \nabla f(x(s)) x''(s)
   		\end{align}
   		By the second order Taylor theorem, for every $s$ such that $x(s) \in \mc{M}$, 
   		\begin{align}
   			f(x(s)) - f(x_0) = s \nabla f(x_0) \cdot x'(0) + \frac{s^2}{2} \left[
   			x'(0)^T \nabla^2 f(x(0)) x'(s) + \nabla f(x(0)) x''(0)
   			\right] + o(s^2)
   		\end{align}
   		Note that by definition, $x'(0)$ is in the tangent space at $x_0$. Also, we've shown previously that $\nabla f(x_0)$ is orthogonal to the tangent space at $x_0$, therefore,
   		\begin{align}
   			f(x(s)) - f(x_0) = \frac{s^2}{2} \left[
   			x'(0)^T \nabla^2 f(x(0)) x'(s) + \nabla f(x(0)) x''(0)
   			\right] + o(s^2)
   		\end{align}
   		Also, by the definition of $\mc{M}$, all constraints hold with equality:
   		\begin{align}
   			f(x_0) &= f(x_0) + \sum_{i=1}^k \lambda_i h_i(x_0)
   		\end{align}
   		where $\lambda_i$'s are from the first result. Hence,
   		\begin{align}
   			f(x(s)) - f(x_0) &= \frac{s^2}{2} \left [
   			x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0)
   			+ \left(
   			\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0)
   			\right)x''(0)
   			\right ] + o(s^2) \\
   			&= \frac{s^2}{2} x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0) + o(s^2)
   		\end{align}
   		And above expression is greater or equal to zero because $x_0$ is a local minimum,
   		\begin{align}
   			\frac{s^2}{2} x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0) + o(s^2) &\geq 0 \\
   			\implies x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0) + \frac{o(s^2)}{s^2} &\geq 0 \\
   			\overset{s\to 0}{\implies}x'(0)^T \left(\nabla^2 f(x_0) + \sum_{i=1}^k \lambda_i \nabla^2 h_i(x_0)\right) x'(0) &\geq 0
   		\end{align}
   		Where $x'(0)$ is a vector in the tangent space at $x_0$ by definition. Moreover, the curve $x(s)$ was chosen arbitrarily, so the argument works for every curve and therefore every tangent vector, and what's desired is shown.
   	\end{proof}
   	
   	\begin{example}
   		\begin{align}
   			\min f(x, y) &= x^2 - y^2 \\
   			s.t.\ h(x, y) &= y = 0
   		\end{align}
   		First order condition suggests $(x_0, y_0) = (0, 0)$ Note that the tangent space at $(x_0, y_0)$ is $\tx{span}\{\nabla h_i\}^\perp$:
   		\begin{align}
   			T_{x_0}\mc{M} = \{ (u, 0): u \in \R\}
   		\end{align}
   		and 
   		\begin{align}
   			\nabla_x^2 \mc{L} = \begin{pmatrix}
   				2 & 0 \\ 0 & -2
   			\end{pmatrix}
   		\end{align}
   		is obviously positive semidefinite (actually positive definition) on the tangent space.
   	\end{example}
   	
   	\begin{theorem}[Second Order Sufficient Conditions]
   		Let $f, h_i \in C^2$ on open $\Omega \subset \R^n$, and $x_0 \in \mc{M}$ is a regular point, if there exists $\lambda_i \in \R$ such that
   		\begin{enumerate}[(i)]
   			\item $\nabla_x \mc{L}(x_0, \lambda_i) = 0$;
   			\item $\nabla_x^2 \mc{L}(x_0, \lambda_i) \succ 0$ \red{on $T_{x_0} \mc{M}$},
   		\end{enumerate}
   		then $x_0$ is a \emph{strict} local minimum.
   	\end{theorem}
   	\begin{proof}
   		Recall that $\nabla^2 f(x_0) + \sum \lambda_i \nabla^2 h_i(x_0)$ positive definite on $T_{x_0} \mc{M}$ implies there exists $a > 0$ ($a$ is taken to be equal to the least eigenvalue of $\nabla^2_x \mc{L}$) such that 
   		\begin{align}
   			v^T [\nabla^2 f(x_0) + \sum \lambda_i \nabla^2 h_i(x_0)] v \geq a \norm{v}^2\quad \forall v \in T_{x_0} \mc{M}
   		\end{align}
   		Let $x(s) \in \mc{M}$ be a curve such that $x(0) = x_0$ and $v = x'(0)$. WLOG, $\norm{x'(0)} = 1$. By the second order Taylor expansion,
   		\begin{align}
   			f(x(s)) - f(x(0)) &= s \left. \frac{d}{ds} \right\vert_{s=0} f(x(s)) + \frac{s^2}{2} \left. \frac{d^2}{ds^2} \right\vert_{s=0} f(x(s)) + o(s^2) \\
   			&= s \left. \frac{d}{ds} \right\vert_{s=0} \left[f(x(s)) + \sum \lambda_i h_i(x(s))\right] + \frac{s^2}{2} \left. \frac{d^2}{ds^2} \right\vert_{s=0} \left[f(x(s)) + \sum \lambda_i h_i(x(s))\right] + o(s^2) \\
   			&= s \nabla_x \mc{L}(x_0, \lambda_i) \cdot x'(0)
   			+ \frac{s^2}{2}
   			\left[x'(0)^T \nabla_x^2 \mc{L}(x_0, \lambda_i) x'(0) + \nabla_x \mc{L}(x_0, \lambda_i) x''(0) \right]
   			+ o(s^2) \\
   			&= \frac{s^2}{2} x'(0)^T \nabla_x^2 \mc{L}(x_0, \lambda_i) x'(0) + o(s^2) \\
   			&\geq \frac{s^2}{2} a \norm{x'(0)}^2 + o(s^2)\quad \tx{where } a > 0 \\
   			&= s^2 \left(\frac{a}{2} + \frac{o(s^2)}{s^2}\right) \\
   			&\overset{s \to 0}{>} 0
   		\end{align}
   		Therefore, for sufficiently small $s$, $f(x(s)) - f(x(0)) > 0$. And this is true for every curve $x$ on $\mc{M}$. So $x(0)$ is a strict local minimum.
   	\end{proof}
   	
   	\subsection{Remark on the Connection Between Constrained and Unconstrained Optimizations}
   	\begin{example}
   		Consider
   		\begin{align}
   			&\min f(x,y,z) \\
   			&s.t. g(x,y,z) = z - h(x,y) = 0
   		\end{align}
   		where $\mc{M}$ is the graph of $h$. Using Lagrangian multiplier provides necessary condition: $\nabla f + \lambda \nabla g = 0$,
   		\begin{align}
   			\begin{pmatrix}
   				f_x \\ f_y \\ f_z
   			\end{pmatrix}
   			+ \lambda
   			\begin{pmatrix}
   				-h_x \\ -h_y \\ 1
   			\end{pmatrix} = 0
   		\end{align}
   		Convert the constrained optimization into an unconstrained optimization as 
   		\begin{align}
   			\min_{(x,y) \in \R^2} F(x, y) = f(x, y, h(x, y))
   		\end{align}
   		The necessary condition for unconstrained optimization is
   		\begin{align}
   			\nabla F(x, y) &= \begin{pmatrix}
   				f_x + f_z h_x \\
   				f_y + f_z h_y
   			\end{pmatrix} \\
   			&= \begin{pmatrix}
   				f_x \\ f_y
   			\end{pmatrix} - f_z \begin{pmatrix}
   				-h_x \\ -h_y
   			\end{pmatrix} = \begin{pmatrix}
   				0 \\ 0
   			\end{pmatrix}
   		\end{align}
   		Define $\lambda := -f_z$.
   		\begin{align}
   			\nabla F(x, y) &= \begin{pmatrix}
   				f_x \\ f_y \\ f_z
   			\end{pmatrix} + \lambda
   			\begin{pmatrix}
   				-h_x \\ -h_y \\ 1
   			\end{pmatrix} = \begin{pmatrix}
   				0 \\ 0
   			\end{pmatrix}
   		\end{align}
   	\end{example}
   	\subsection{Inequality Constraints}
   	\begin{definition}
   		Let $x_0$ satisfy the set of constraints
   		\begin{align}
   			h_i(x) &= 0\quad i \in \{1, \cdots, k\} \\
   			g_j(x) &\leq 0\quad j \in \{1, \cdots, \ell\}
   		\end{align}
   		we say that the constraint $g_i$ is \textbf{active} at $x_0$ if $g_i(x_0) = 0$, and is \textbf{inactive} at $x_0$ if $g_i(x_0) < 0$.
   	\end{definition}
   	
   	\begin{definition}
   		Split the collection of inequality constraints into active and inactive constraints, let $\Theta(x_0)$ denote the collection of active indices, that's:
   		\begin{align}
   			g_j (x_0) = 0\ \forall j \in \Theta(x_0) \\
   			g_j (x_0) < 0\ \forall j \notin \Theta(x_0)
   		\end{align}
   		Then $x_0$ is said to be a \textbf{regular point} of the constraint if 
   		\begin{align}
   			\{\nabla h_i(x_0)\ \forall i \in \{1, \cdots, k\}; \underbrace{\nabla g_j(x_0)\ \forall j \in \Theta(x_0)}_{\tx{Active Constraints}} \}
   		\end{align}
   		is linearly independent. 
   	\end{definition}
   	
   	\begin{theorem}[The First Order Necessary Condition for Local Minimum: Kuhn-Tucker Conditions]
   		Let $\Omega$ be an open subset of $\R^n$ with constraints $h_i$ and $g_i$ to be $C^1$ on $\Omega$. Suppose $x_0 \in \Omega$ is a regular point with respect to constraints, further suppose $x_0$ is a local minimum, then there exists some $\lambda_i \in \R$ and $\mu_j \in \red{\R_+}$ such that 
   		\begin{enumerate}[(i)]
   			\item $\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0$;
   			\item $\mu_j g_j(x_0) = 0$ (\emph{Complementary slackness}).
   		\end{enumerate}
   		\emph{Remark: by complementary slackness, all $\mu_j$ corresponding to inactive inequality constraints are zero.}
   	\end{theorem}
   	
   	\begin{proof}
   		Let $x_0$ be a local minimum for $f$ satisfying constraints, equivalently, it is a local minimum for equality constraints and active inequality constraints. \\
   		By the first order necessary condition for local minimum with equality constraints, there exists $\lambda_i, \mu_j \in \R$ such that
   		\begin{align}
   			\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j \in \Theta(x_0)} \mu_j \nabla g_j(x_0) = 0
   		\end{align}
   		Then by setting $\mu_j = 0$ for all $j \notin \Theta(x_0)$ one have
   		\begin{align}
   			\nabla f(x_0) + \sum_{i=1}^k \lambda_i \nabla h_i(x_0) + \sum_{j=1}^\ell \mu_j \nabla g_j(x_0) = 0
   		\end{align}
   		By construction, the complementary slackness is satisfied. At this stage, we have construct $\lambda_i \in \R$ and $\mu_j \in \R$ satisfying both conditions, we still need to argue that $\mu_j \geq 0$ for every $j$.
   	\end{proof}
\end{document}













