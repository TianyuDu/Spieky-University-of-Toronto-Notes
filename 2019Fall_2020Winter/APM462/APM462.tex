\documentclass{article}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1truein]{geometry}
\usepackage{tkz-graph}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{setspace}
\linespread{1.15}
\usepackage[margin=1truein]{geometry}

\counterwithin{equation}{section}
\counterwithin{figure}{section}

\pagestyle{fancy}
\lhead{APM462: Nonlinear Optimization}

\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}

\title{APM462: Nonlinear Optimization}
\date{\today}
\author{Tianyu Du}
\begin{document}
    \maketitle
    \tableofcontents
    \newpage
    
    \section{Preliminaries}
    \subsection{Mean Value Theorems and Taylor Approximations.}
    \begin{definition}
        Let $f: S \subset \R^n \to \R$, the \textbf{gradient} of $f$ at $x \in S$, if exists, is a vector $\nabla f(x) \in \R^n$ characterized by the property
        \begin{equation}
            \lim_{v \to 0} \frac{f(x+v) - f(x) - \nabla f(x) \cdot v}{\norm{v}} = 0
        \end{equation}
    \end{definition}
    
    \begin{theorem}[The First Order of Mean Value Theorem]
        Let $f$ be a $C^1$ real-valued function defined on $\R^n$, then for any $x, v \in \R^n$, there exists some $\theta \in (0, 1)$ such that
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x + \theta v) \cdot v
        \end{equation}
    \end{theorem}
    
    \begin{proof}
        Let $x, v \in \R^n$, define $g(t): \R \to \R := f(x + tv)$, which is $C^1$. By the mean value theorem on $\R^\R$, there exists $\theta \in (0, 1)$ such that $g(0+1) = g(0) + g'(\theta)(1-0)$, that is, $f(x+v) = f(x) + g'(\theta)$. Note that $g'(\theta) = \nabla(x + \theta v) \cdot v$, what desired is immediate.
    \end{proof}
    
    \begin{proposition}[The First Order Taylor Approximation]
        Let $f: \R^n \to \R$ be a $C^1$ function, then
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + o(\norm{v})
        \end{equation}
        that is
        \begin{equation}
            \lim_{\norm{v} \to 0} \frac{f(x + v) - f(x) - \nabla f(x) \cdot v}{\norm{v}} = 0
        \end{equation}
    \end{proposition}
    
    \begin{proof}
    	By the mean value theorem, $\exists \theta \in (0, 1)$ such that $f(x+v) - f(x) = \nabla f(x + \theta v) \cdot v$. The limit becomes $\lim_{\norm{v} \to 0} \frac{[\nabla f(x + \theta v) - \nabla f(x)] \cdot v}{\norm{v}} = \lim_{\norm{v} \to 0; x + \theta v \to x} \frac{[\nabla f(x + \theta v) - \nabla f(x)] \cdot v}{\norm{v}}$. Since $f \in C^1$, $\lim_{x + \theta v \to x} \nabla f(x + \theta v) = \nabla f(x)$. And $\frac{v}{\norm{v}}$ is a unit vector, and every component of it is bounded, as the result, the limit of inner product vanishes instead of explodes.
    \end{proof}
    
    \begin{theorem}[The Second Order Mean Value Theorem]
        Let $f: \R^n \to \R$ be a $C^2$ function, then for any $x, v \in \R^n$, there exists $\theta \in (0, 1)$ satisfying
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + \frac{1}{2} v' H_f(x + \theta v)\ v
        \end{equation}
        where $H_f$ is the Hessian matrix of $f$, may also be written as $\nabla^2 f$.
    \end{theorem}
    
    \begin{proposition}[The Second Order Taylor Approximation]
        Let $f: \R^n \to \R$ be a $C^2$ function, and $x, v \in \R^n$, then
        \begin{equation}
            f(x + v) = f(x) + \nabla f(x) \cdot v + \frac{1}{2} v' H_f(x)\ v + o(\red{\norm{v}^2})
        \end{equation}
        that is
        \begin{equation}
            \lim_{\norm{v} \to 0} \frac{
                f(x + v) - f(x) - \nabla f(x) \cdot v - \frac{1}{2} v' H_f(x)\ v
            }{\norm{v}^2} = 0
        \end{equation}
    \end{proposition}
    
    \begin{proof}
    	By the second mean value theorem, there exists $\theta \in (0, 1)$ such that the limit is equivalent to
    	\begin{align}
    		\lim_{\norm{v} \to 0} \frac{1}{2} \left(\frac{v}{\norm{v}}\right)' \left[H_f(x + \theta v) - H_f(x)\right] \frac{v}{\norm{v}}
    	\end{align}
    	Since $f \in C^2$, the limit of $\left[H_f(x + \theta v) - H_f(x)\right]$ is in fact $\textbf{0}_{n \times n}$. And every component of unit vector $\frac{v}{\norm{v}}$ is bounded, the quadratic form converges to zero as an immediate result.
    \end{proof}
    
    It is often noted that the gradient at a particular $x_0 \in dom(f) \subset \R^n$ gives the direction $f$ increases most rapidly.
        Let $x_0 \in dom(f)$, and $v$ be a \ul{unit vector} representing a \emph{feasible direction} of change. That is, there exists $\delta > 0$ such that $x_0 + t v \in dom(f)$ $\forall t \in [0, \delta)$. Then the rate of change of $f$ along feasible direction $v$ can be written as
        \begin{equation}
            \left.\frac{d}{dt}\right\vert_{t=0} f(x_0 + tv) = \nabla f(x_0) \cdot v = \norm{\nabla f(x_0)}\ \norm{v} \cos(\theta)
        \end{equation}
        where $\theta = \angle (v, \nabla f(x_0)$. And the derivative is maximized when $\theta=0$, that is, when $v$ and $\nabla f$ point the same direction.
    
    \subsection{Implicit Function Theorem}
    \begin{theorem}[Implicit Function Theorem]
        Let $f: \R^{n+1} \to \R$ be a $C^1$ function, let $(a, b) \in \R^n \times \R$ such that $f(a, b) = 0$. If $\nabla f(a, b) \neq 0$, then $\{(x, y) \in \R^n \times \R:f(x, y) = 0\}$ is locally a graph of a function $g: \R^n \to \R$.
    \end{theorem}
    
    \begin{remark}
        $\nabla f(x_0) \perp \tx{ level set of $f$ near $x_0$}$.
    \end{remark}
    
    \section{Convexity}
    \subsection{Terminologies}
    \begin{definition}
        Set $\Omega \subset \R^n$ is \textbf{convex} if and only if 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in [0, 1],\ \lambda x_1 + (1 - \lambda) x_2 \in \Omega
        \end{equation}
    \end{definition}
    
    \begin{definition}
        A function $f: \Omega \subset \R^n \to \R$ is \textbf{convex} if and only if $\Omega$ is convex, and 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in [0, 1],\ f\left(\lambda x_1 + (1- \lambda) x_2 \right) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
        \end{equation}
    \end{definition}
    
    \begin{definition}
        A function $f: \Omega \subset \R^n \to \R$ is \textbf{strictly convex} if and only if $\Omega$ is convex and 
        \begin{equation}
            \forall x_1, x_2 \in \Omega,\ \lambda \in (0, 1),\ f\left(\lambda x_1 + (1- \lambda) x_2 \right) < \lambda f(x_1) + (1 - \lambda) f(x_2)
        \end{equation}
    \end{definition}
    
    \subsection{Basic Properties of Convex Functions}
    
    \begin{definition}
        A function $f: \Omega \to \R$ is \textbf{concave} if and only if $-f$ is \textbf{convex}.
    \end{definition}
    
    \begin{proposition}
        \begin{enumerate}[(i)]
            \item If $f_1, f_2$ are convex on $\Omega$, so is $f_1 + f_2$;
            \item If $f$ is convex on $\Omega$, then for any $a > 0$, $af$ is also convex on $\Omega$;
            \item Any \textbf{sub-level/lower contour set} of a convex function $f$ 
            \begin{align}
            	SL(c) := \{x \in \R^n: f(x) \leq c\}
            \end{align}
            is convex.
        \end{enumerate}
    \end{proposition}
    
    \begin{proof}[Proof of (iii).]
    	Let $c \in \R$, and $x_1 ,x_2 \in SL(c)$. Let $s \in [0, 1]$. Since $x_1, x_2 \in SL(c)$, and $f(\cdot)$ is convex, $f(s x_1 + (1-s) x_2) \leq s f(x_1) + (1-s) f(x_2) \leq s c + (1-s) c = c$. Which implies $s x_1 + (1-s) x_2 \in SL(c)$.
    \end{proof}
    
    \begin{example}
    	$f(x): \R^n \to \R := \norm{x}$ is convex.
    \end{example}
    
    \begin{proof}
    	Note that for any $u, v \in \R^n$, by triangle inequality, $\norm{u - (-v)} \leq \norm{u - 0} + \norm{0 - (-v)} = \norm{u} + \norm{v}$. Consequently, let $u, v \in \R^n$ and $s \in [0, 1]$, then $\norm{s u + (1-s) v} \leq \norm{su} + \norm{(1-s) v} = s \norm{u} + (1-s) \norm{v}$. Therefore, $\norm{\cdot}$ is convex.
    \end{proof}
	
	\subsection{Characteristics of $C^1$ Convex Functions}
	
    \begin{theorem}[$C^1$ criterions for convexity]
        Let $f \in C^1$, then $f$ is convex on a convex set $\Omega$ \ul{if and only if}
        \begin{equation}
            \forall x, y \in \Omega,\ f(y) \geq f(x) + \nabla f(x) \cdot (y - x)
        \end{equation}
        that is, \emph{the linear approximation is never an overestimation of value of $f$}.
    \end{theorem}
    \begin{proof}
        ($\implies$) Suppose $f$ is convex on a convex set $\Omega$. Then $f(sy + (1-s) x) \leq sf(y) + (1-s)f(x)$ for every $x, y \in \Omega$ and $s \in [0, 1]$, which implies, for every $s \in (0, 1]$:
        \begin{equation}
            \frac{f(sy + (1-s) x) - f(x)}{s} \leq f(y) - f(x)
        \end{equation}
        By taking the limit of $s \to 0$,
        \begin{align}
            \lim_{s \to 0} \frac{f(x + s(y-x)) - f(x)}{s} &\leq f(y) - f(x) \\
            \implies \left.\frac{d}{ds}\right\vert_{s=0} f(x + s(y-x)) &\leq f(y) - f(x) \\
            \implies \nabla f(x) \cdot (y-x) &\leq f(y) - f(x)
        \end{align}
        ($\impliedby$) Let $x_0, x_1 \in \Omega$, let $s \in [0, 1]$. Define $x^* := s x_0 + (1-s) x_1$, then 
        \begin{align}
        	f(x_0) &\geq f(x^*) + \nabla f(x^*) \cdot (x_0 - x^*) \\
        	\implies f(x_0) &\geq f(x^*) + \nabla f(x^*) \cdot [(1-s)(x_0 - x_1)]
        \end{align}
        Similarly,
        \begin{align}
        	f(x_1) &\geq f(x^*) + \nabla f(x^*) \cdot (x_1 - x^*) \\
        	\implies f(x_1) &\geq f(x^*) + \nabla f(x^*) \cdot [s(x_1 - x_0)]
        \end{align}
        Therefore, $sf(x_0) + (1-s)f(x_1) \geq f(x^*)$.
    \end{proof}
    
    \begin{theorem}[$C^2$ criterion for convexity]
        $f \in C^2$ is a convex function on a convex set $\Omega \subset \R^n$ \ul{if and only if} $\nabla^2 f(x) \succcurlyeq 0$ for all $x \in \Omega$.
    \end{theorem}
    
    \begin{remark}
    	When $f$ is defined on $\R$, the $C^2$ criterion becomes $f''(x) \geq 0$.
    \end{remark}
    
    \begin{proof}
        ($\impliedby$) Suppose $\nabla^2 f(x) \succcurlyeq 0$ for every $x \in \Omega$, let $x, y \in \Omega$. By the second order MVT,
        \begin{align}
        	f(y) &= f(x) + \nabla f(x) \cdot (y - x) + \frac{1}{2} (y-x)^T \nabla^2 f(x + s (y - x)) (y - x)\tx{ for some } s \in [0, 1] \\
        	&\implies f(y) \geq f(x) + \nabla f(x) \cdot (y - x)
        \end{align}
        So $f$ is convex by the $C^1$ criterion of convexity.\\
        ($\implies$) Let $v \in \R^n$. Suppose, for contradiction, that for some $x \in \Omega$, $\nabla^2 f(x) \centernot \succcurlyeq 0$. If such $x \in \partial \Omega$, note that $v^T \nabla^2 f(\cdot) v$ is continuous because $f \in C^2$, then there exists $\varepsilon > 0$ such that $\forall x' \in V_\varepsilon(x) \cap \Omega^{int},\ v^T \nabla^2 f(x') v < 0$. Hence, one may assume with loss of generality that such $x \in \Omega^{int}$. Because $x \in \Omega^{int}$, exists $\varepsilon' > 0$, such that $V_{\varepsilon'}(x) \subseteq \Omega^{int}$. Define $\hat{v} := \frac{v}{\sqrt{\varepsilon'}}$, then for every $s \in [0, 1]$, $\hat{v}^T \nabla^2 f(x + s\hat{v}) \hat{v} < 0$. Let $y = x + \hat{v}$, by the mean value theorem, $f(y) = f(x) + \nabla f(x) \cdot (y - x) + \frac{1}{2} (y - x)^T \nabla^2 f (x + s (y - x)) (y - x)$ for some $s \in [0, 1]$. This implies $f(y) < f(x) + \nabla f(x) \cdot (y - x)$, which contradicts the $C^1$ criterion for convexity.
    \end{proof}
    
    \subsection{Minimum and Maximum of Convex Functions}
    \begin{theorem}
        Let $\Omega \subset \R^n$ be a convex set, and $f: \Omega \to \R$ is a convex function. Let
        \begin{align}
        	\Gamma := \left\{x \in \Omega: f(x) = \min_{x \in \Omega} f(x) \right\} \equiv \argmin_{x \in \Omega} f(x)
        \end{align}
        If $\Gamma \neq \varnothing$, then 
        \begin{enumerate}[(i)]
        	\item $\Gamma$ is convex;
        	\item any local minimum of $f$ is the global minimum.
        \end{enumerate}
    \end{theorem}
    
    \begin{proof}[Proof (i).]
    	Let $x, y \in \Gamma$, $s \in [0, 1]$, then $sx+(1-s)y \in \Omega$ because $\Omega$ is convex. Since $f$ is convex, $f(sx+(1-s)y) \leq sf(x) + (1-s)f(y) = \min_{x \in \Omega} f(x)$. The inequality must be equality since it would contradicts the fact that $x, y \in \Gamma$. Therefore, $sx+(1-s)y \in \Gamma$.
    \end{proof}
    
    \begin{proof}[Proof (ii).]
    	Let $x \in \Omega$ be a local minimizer for $f$, but assume, for contradiction, it is not a global minimizer. That is, there exists some other $y$ such that $f(y) < f(x)$. Since $f$ is convex, \begin{align}
 				f(x + t(y-x)) = f((1-t)x + ty) \leq (1-t)f(x) + tf(y) < f(x)
			\end{align}
			for every $t \in (0, 1]$. Therefore, for every $\varepsilon > 0$, there exists $t^* \in (0, 1]$ such that $x + t^*(y-x) \in V_\varepsilon(x)$ and $f(x + t^*(y-x)) < f(x)$, this contradicts the fact that $x$ is a local minimum.
    \end{proof}
    
    \begin{theorem}
        Let $\Omega \subset \R^n$ be a convex set, and $f: \Omega \to \R$ is a convex function. Then 
        \begin{equation}
            \max_{x \in \Omega} f(x) = \max_{x \in \partial \Omega} f(x)
        \end{equation}
    \end{theorem}
    
    \begin{proof}
        As we assumed, $\Omega$ is closed, therefore $\partial \Omega \subseteq \Omega$. Hence, $\max_{x \in \Omega} f \geq \max_{x \in \partial \Omega} f$. Suppose $\max_{x \in \Omega} f > \max_{x \in \partial \Omega} f$, let $x^* := \argmax_{x \in \Omega} f \in \Omega^{int}$. Then we can construct a straight line through $x^*$ and intersects $\partial \Omega$ at two points, $y_1, y_2 \in \partial \Omega$, such that $x^* = s y_1 + (1-s) y_2$ for some $s \in (0, 1)$. Further, since $f$ is convex, $\max_{x \in \Omega}f(x) = f(x^*) \leq s f(y_1) + (1-s) f(y_2) \leq s \max_{\partial \Omega} f + (1-s) \max_{\partial \Omega} f = \max_{\partial \Omega} f$, which leads to a contradiction. Therefore, $\max_{x \in \Omega} f = \max_{x \in \partial \Omega} f$.
    \end{proof}
    
    \begin{proposition}
    	For p, g > 1 and $\frac{1}{p} + \frac{1}{g} = 1$,
    	\begin{align}
    		|ab| \leq \frac{1}{p} |a|^p + \frac{1}{g}|b|^g
    	\end{align}
    \end{proposition}
    
    \begin{proof}
    	\begin{align}
    		(-\log) |ab| &= (-\log) |a| + (-\log) |b| \\
    		&= \frac{1}{p} (-\log) |a|^p + \frac{1}{g} (-\log) |b|^p \\
    		(\because (-\log) \tx{ is convex})\ &\geq (-\log)\left( \frac{1}{p} |a|^p + \frac{1}{g} |b|^p \right)
    	\end{align}
    	And since $(-\log)$ is monotonically decreasing,
    	\begin{align}
    		|ab| \leq \frac{1}{p} |a|^p + \frac{1}{g} |b|^p
    	\end{align}
    \end{proof}
    
    \begin{corollary}
    	\begin{align}
    		|ab| \leq \frac{|a|^2 + |b|^2}{2}
    	\end{align}
    \end{corollary}

	\section{Basics of Unconstrained Optimization}
    \begin{theorem}[Extreme Value Theorem]
        Let $f: \R^n \to \R$ is continuous and $K \subset \R^n$ be a compact set, then the minimization problem $\min_{x \in K} f(x)$ has a solution.
    \end{theorem}
    
    \begin{proposition}
    	Let $K \subset \R^n$, then $K$ is compact if and only if $K$ is closed and bounded.
    \end{proposition}
    
	\begin{proposition}
		Let $\{h_i\}$ and $\{g_i\}$ be sets of continuous functions on $\R^n$, the the set of all points in $\R^n$ that satisfy
		\begin{equation}
			\begin{cases}
				h_i(x) = 0\ \forall i\\
				g_j(x) \leq 0\ \forall j
			\end{cases}
		\end{equation}
		is a closed set. Moreover, if the qualified set is also bounded, then it is compact.
	\end{proposition}
	
	\begin{example}
		The set $\{(x, y) \in \R^2: x^2 - y^2 - 1 = 0\}$ is closed and bounded, therefore it is compact.
	\end{example}
    
    \begin{corollary}
        Let $f: \R^n \to \R$ be a continuous function, if there exists $a \in \R^n$ such that $f(x) \geq f(a)$ for every $x \notin \mc{B}(r, a)$, then $f$ attains its minimum in $\mc{B}(r, a)$.
    \end{corollary}
    
    \begin{proposition}
        When $\Omega = \R^n$, the unconstrained minimization has the following properties
        \begin{enumerate}[(i)]
            \item $\argmax f = \argmin (-f)$;
            \item $\max f = - \min (-f)$
        \end{enumerate}
    \end{proposition}
    
\end{document}













