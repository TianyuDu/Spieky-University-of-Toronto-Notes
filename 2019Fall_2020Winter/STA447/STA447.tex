\documentclass{article}
\usepackage{spikey}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{centernot}
\usepackage[shortlabels]{enumitem}
\usepackage[margin=1truein]{geometry}
\usepackage{tkz-graph}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{setspace}
\linespread{1.15}
\usepackage[margin=1truein]{geometry}

\counterwithin{equation}{section}
\counterwithin{figure}{section}

\newcommand{\upn}[0]{^{(n)}}
\newcommand{\upk}[0]{^{(k)}}

\pagestyle{fancy}
\lhead{STA447: Stochastic Processes}

\usepackage[
    type={CC},
    modifier={by-nc},
    version={4.0},
]{doclicense}

\title{STA447: Stochastic Processes}
\date{\today}
\author{Tianyu Du}
\begin{document}
    \maketitle
    \tableofcontents
    \newpage
    \section{Markov Chain Probabilities}
    
    \begin{definition}
    	A \textbf{discrete-time, discrete-space, and time-homogenous Markov chain} is a triple of $\mc{S} = (S, v, P)$ in which
    	\begin{enumerate}[(i)]
    		\item $S$ represents the \emph{state space}, which is nonempty and countable;
    		\item \emph{initial probability} $v$, which is a distribution on $S$;
    		\item and \emph{transition probability} $(p_{ij})$ satisfying
    		\begin{align}
    			\sum_{j \in S} p_{ij} = 1\quad \forall i \in S
    		\end{align}
    	\end{enumerate}
    \end{definition}
    
    \begin{definition}
    	A Markov chain satisfies the \textbf{time-homogenous property} if
    	\begin{align}
    		P(X_{n+1}=j|X_n=i) = P(X_1=j|X_0=i) = p_{ij}\quad \forall n \in \N
    	\end{align}
    \end{definition}
    
    \begin{definition}
    	A Markov chain satisfies the \textbf{Markov property} if
    	\begin{align}
    		P(X_{n+1}=j|X_n=i_n, X_{n-1}=i_{n-1}, \cdots, X_0=i_0) = P(X_{n+1}=j|X_n=i_n)
    	\end{align}
    	That is, the chain is \emph{memoryless}.
    \end{definition}
    
    \begin{proposition}
    	As an immediate result from the Markov property, the joint probability
    	\begin{align}
    		\red{P(X_0=i_0, X_1=i_1, X_2=i_2, \cdots, X_n=i_n)}
    		&= P(X_0=i_0) P(X_1=i_1, X_2=i_2, \cdots, X_n=i_n|X_0=i_0) \\
    		&= v_{i_0} P(X_1=i_1|X_0=i_0) P(X_2=i_2, \cdots, X_n=i_n|X_0=i_0, X_1=i_1) \\
    		&= v_{i_0} P(X_1=i_1|X_0=i_0) P(X_2=i_2, \cdots, X_n=i_n|X_1=i_1) \tx{ (Markov property)}\\
    		&\red{= v_{i_0} p_{i_0 i_1} \cdots p_{i_{n-1}i_n}}
    	\end{align}
    \end{proposition}
    
    \begin{definition}[$n$-step Arrival Probability]
    	Let $m = |S|$ and $\mu_i^{(n)} := P(X_n=i)$ denote \ul{the probability that the state ends up at $i$ after $n$ step (starting point follows $v$)}.
    \end{definition}	
    
    \begin{proposition}
    	\begin{align}
    		\mu^{(n)} = v P^n
    	\end{align}
    \end{proposition}
	\begin{proof}
    	By the law of total expectation,
    	\begin{align}
    		P(X_n=i) &= \sum_{j \in S} P(X_n=i, X_{n-1}=j) \\
    		&= \sum_{j \in S} P(X_n=i | X_{n-1}=j) P(X_{n-1}=j) \\
    		&= \sum_{j \in S} P(X_{n-1}=j) p_{ij} \\
    		&= \sum_{j \in S} \mu_{j}^{(n-1)} p_{ij}
    	\end{align}
    	Let $\mu^{(n)} := \left[\mu_1^{(n)}, \mu_2^{(n)}, \cdots, \mu_m^{(n)} \right] \in \R^{1 \times m}$ and $P = [p_{ij}] \in \R^{m \times m}$. The recurrence relation can be expressed in matrix notation as:
    	\begin{align}
    		\mu^{(n)} = \mu^{(n-1)} P
    	\end{align}
    	where $\mu^{(0)} = v = [v_1, v_2, \cdots, v_m]$ by construction. Define $P^0$ to be the identity matrix $I_m$, then
    	\begin{align}
    		\mu^{(0)} &= v = v P^0 \\
    		\mu^{(1)} &= \mu^{(0)} P = v P^1 \\
    		&\vdots \\
    		\mu^{(n)} &= v P^n
    	\end{align}
    \end{proof}
    
    \begin{definition}[$n$-step Transition Probability]
    	Define 
    	\begin{align}
    		p_{ij}^{(n)} := P(X_{m+n}=j|X_{m}=i)
    	\end{align}
    	to be the probability of arriving state $j$ after $n$ steps, \ul{starting from state $i$}\footnote{In the definition of $\mu^{(n)}_j$, the starting state is random following distribution $v$. While defining $p^{(n)}_{ij}$ the initial state is fixed to be $i$.}. By the time-homogenous property,
    	\begin{align}
    		p_{ij}^{(n)} = P(X_{m+n}=j|X_{m}=i)\quad \forall m \in \N
    	\end{align}
    \end{definition}
    
    \begin{proposition}
    Let $P^{(n)} := [p_{ij}^{(n)}] \in \R^{m \times m}$, then
    	\begin{align}
    		P^{(n)} = P^n
    	\end{align}
    \end{proposition}
    \begin{proof}
    	\ul{Initial Step}: for $n = 1$, $P^{(1)} = P$ by definition. \\
    	\ul{Inductive Step}: for $n \in \N$,
    	\begin{align}
    		p_{ij}^{(n+1)} &= P(X_{n+1}=j|X_0=i) \\
    		&= \sum_{k \in S} P(X_{n+1}=j|X_n = k, X_0=i) P(X_n=k|X_0=i) \\
    		&= \sum_{k \in S} P(X_{n+1}=j|X_n = k) p_{ik}^{(n)} \\
    		&= \sum_{k \in S} p_{ik}^{(n)} p_{kj} \\
    		&= [P^{(n)} P]_{ij}
    	\end{align}
    	Therefore,
    	\begin{align}
    		P^{(n+1)} = P^{(n)} P
    	\end{align}
    	and 
    	\begin{align}
    		P^{(n)} = P^n
    	\end{align}
    \end{proof}

    \begin{theorem}[Chapman-Kolmogorov Equation]
    	For every $k \in S$,
    	\begin{align}
    		p_{ij}^{(m+n)} = p_{ik}^{(m)} p_{kj}^{(n)}
    	\end{align}
    	For $k,\ell \in S$,
    	\begin{align}
    		p_{ij}^{(m+s+n)} = p_{ik}^{(m)} p_{k\ell}^{(s)} p_{\ell j}^{(n)}
    	\end{align}
    \end{theorem}

    \begin{theorem}[Chapman-Kolmogorov Equations (Generalization)]
    	Let $n = (n_1, n_2, \cdots, n_k)$ be a multi-set of non-negative integers, then
    	\begin{align}
    		P^{(\sum_{i=1}^k n_i)} = \prod_{i=1}^k P^{(n_i)}\quad (\dagger)
    	\end{align}
    \end{theorem}
    
    \begin{proof}
    	Prove by induction on the size of multi-set: \\
    	\ul{Base case} is trivial for $k=1$. \\
    	\ul{Inductive step} for $k > 1$, suppose $(\dagger)$ holds for every set of length $k$, consider another multi-set with length $k+1$: $n' = (n_1, n_2, \cdots, n_k, n_{k+1})$. Let $\delta := \sum_{i=1}^k n_i$.
    	\begin{align}
    		P^{(\delta + n_{k+1})}_{ij}
    		&= P(X_{\delta + n_{k+1}}=j|X_0=i) \\
    		&= \sum_{k \in S} P(X_{\delta + n_{k+1}}=j|X_\delta=k, X_0=i) P(X_\delta | X_0=i) \\
    		&= \sum_{k \in S} P(X_{\delta + n_{k+1}}=j|X_\delta=k) P(X_\delta | X_0=i) \\
    		&= \sum_{k \in S} P(X_{n_{k+1}}=j|X_0=k) P(X_\delta=k | X_0=i) \\
    		&= \sum_{k \in S} p^{n_{k+1}}_{kj} p^{(\delta)}_{ik} \\
    		&= [P^{(\delta)} P^{(n_{k+1})}]_{ij} \\
    		\implies P^{(\delta + n_{k+1})} &= P^{(\delta)} P^{(n_{k+1})}
    	\end{align}
    \end{proof}
    
    \begin{corollary}[Chapman-Kolmogorov Inequality]
    	For every $k \in S$,
    	\begin{align}
    		p_{ij}^{(m+n)} \geq p_{ik}^{(m)} p_{kj}^{(n)}
    	\end{align}
    	For $k,\ell \in S$,
    	\begin{align}
    		p_{ij}^{(m+s+n)} \geq p_{ik}^{(m)} p_{k\ell}^{(s)} p_{\ell j}^{(n)}
    	\end{align}
    \end{corollary}

	\begin{proof}[Informal Proof.]
		Note that $p_{ik}^{(m)} p_{kj}^{(n)}$ is exactly the probability of arriving $j$ from $i$ in $m+n$ steps (say, event $E$), \ul{conditioned on passing state $k$ at $m$ steps}. And $p_{ij}^{(m+n)}$ is the unconditional probability of event $E$, which is no less than the 
	\end{proof}

	\subsection{Recurrent and Transience}
	\begin{notation}
		For an arbitrary event $E$,
		\begin{align}
			P_i(E) &:= P(E|X_0=i) \\
			\expe_i(E) &:= \expect{E|X_0=i}
		\end{align}
	\end{notation}
    
    \begin{notation}
    	Let $N(i) := |\{n \geq 1: X_n = i\}|$ denote the number of times the Markov chain arrives state $i$. Note that $N(i)$ does not count the initial state.
    \end{notation}
    
    \begin{definition}
    	Define the \textbf{return probability} from state $i$ to $j$, $f_{ij}$, as the probability of arriving state $j$ starting from state $i$. That is,
    	\begin{align}
    		f_{ij} &= P(\exists n \geq 1\ s.t.\ X_n = j | X_0 = i) \\
    		&= P_i(N(j) \geq 1)
    	\end{align}
    \end{definition}
    
    \begin{proposition}
    	The probability of \ul{firstly arriving $j$, then arriving $k$} (denoted as event $E$) starting from $i$ equals
    	\begin{align}
    		P_i(E) = f_{ij} f_{jk}
    	\end{align}
    \end{proposition}
    
    \begin{proof}
    	\begin{align}
    		P_i(E) &= P(\exists 1 \leq m \leq n \ s.t.\ X_m=j,\ X_n = k) \\
    		&= P_i(\exists 1 \leq m \leq n \ s.t.\ X_n = k|\exists m \geq 1\ s.t.\ X_m=j) P_i(\exists m \geq 1\ s.t.\ X_m=j) \\
    		&= P_{\red{i}}(\exists 1 \leq m \leq n \ s.t.\ X_n = k|\exists m \geq 1\ s.t.\ X_m=j) f_{ij} \\
    		&= P(\exists 1 \leq m \leq n \ s.t.\ X_n = k|X_m=j) f_{ij} \tx{ (Markov property)} \\
    		&= P(\exists 1 \leq n \ s.t.\ X_n = k|X_0=j) f_{ij} \tx{ (time homogenous property)} \\
    		&= f_{ij}f_{jk}
    	\end{align}
    \end{proof}
    
    \begin{corollary}
    	\begin{align}
    		\red{P_i(N(i) \geq k)} &\red{= (f_{ii})^k} \\
    		\red{P_i(N(j) \geq k)} &\red{= f_{ij}(f_{jj})^{k-1}}
    	\end{align}
    \end{corollary}

	\begin{corollary}
		\begin{align}
			\red{f_{ij} \geq f_{ik}f_{kj}}
		\end{align}
	\end{corollary}
    
    \begin{proposition}
    	$1-f_{i j}$ captures the probability that the Markov chain does not return to $j$ from $i$.
    	\begin{align}
    		1-f_{i j}=P_{i}\left(X_{n} \neq j \text { for all } n \geq 1\right)
    	\end{align}
    \end{proposition}
    
    \begin{definition}
    	A state $i$ in a Markov chain is \textbf{recurrent} if $f_{ii} = 1$. That is, starting from state $i$, the chain returns state $i$ for sure.
    	Otherwise, state $i$ is \textbf{transient}.
    \end{definition}
    
    \begin{theorem}[Recurrent State Theorem]
    	The following statements are equivalent:
    	\begin{enumerate}[(i)]
    		\item State $i$ is recurrent (i.e., $f_{ii}=1$);
    		\item $P_i(N(i) = \infty) = 1$, that is, starting from state $i$, state $i$ will be visited infinitely often;
    		\item $\sum_{n=1}^\infty p_{ii}^{(n)} = \infty$.
    	\end{enumerate}
    \end{theorem}

    \begin{proof}
    	$(i) \iff (ii)$:
    	\begin{align}
    		P(N(i) = \infty|X_0=i) &= P(\lim_{k \to \infty} N(i) \geq k| X_0=i) \\
    		&= \lim_{k \to \infty} P(N(i) \geq k| X_0=i) \\
    		&= \lim_{k \to \infty} (f_{ii})^k = 1 \tx{ if and only if } f_{ii} = 1
    	\end{align}
    	$(i) \iff (iii)$:
    	\begin{align}
    		\sum_{n=1}^\infty p_{ii}^{(n)}
    		&= \sum_{n=1}^\infty P(X_n = i | X_0 = i) \\
    		&= \sum_{n=1}^\infty \expe (\id{X_n=i} | X_0 = i) \\
    		&= \expe \left(
    		\sum_{n=1}^\infty \id{X_n=i} \Big| X_0 = i
    		\right ) \\
    		&= \expe (N(i) | X_0=i) \\
    		&= \sum_{k=1}^\infty k P(N(i)=k | X_0=i) \\
    		&= \sum_{k=1}^\infty P(N(i) \geq k | X_0=i) \\
    		&= \sum_{k=1}^\infty (f_{ii})^k \\
    		&= \infty \tx{ if and only if } f_{ii} = 1
    	\end{align}
    \end{proof}
 
	\begin{theorem}[Transient State Theorem]
		The following statements are equivalent:
    	\begin{enumerate}[(i)]
    		\item State $i$ is transient;
    		\item $P_i(N(i) = \infty) = 0$, that is, state $i$ will only be visited finitely many times;
    		\item $\sum_{n=1}^\infty p_{ii}^{(n)} < \infty$.
    	\end{enumerate}
	\end{theorem}
 
 	\begin{proof}
 		Take negation of the recurrent state theorem.
 	\end{proof}
 
    \begin{lemma}[Stirling's Approximation]
    	\begin{align}
    		n! \approx (n/e)^n \sqrt{2\pi n}
    	\end{align}
    \end{lemma}

    \begin{proposition}
    	For simple random walk, if $p=1/2$, then $f_{ii} = 1\ \forall i \in S$. Otherwise, all states are transient.
    	\begin{align}
    		\forall i \in S,\ f_{ii} = 1 \iff p = \frac{1}{2}
    	\end{align}
    \end{proposition}
    
    \begin{proof}
    	For simplicity, consider state $0$ and the series $\sum_{n=1}^\infty p_{00}\upn$. \\
    	Note that for odd $n$'s, $p_{00}\upn = 0$. \\
    	For all even $n$'s such that $n = 2k$,
    	\begin{align}
    		\sum_{n=1}^\infty p_{00}\upn &= \sum_{k=1}^\infty p_{00}^{(2k)} \\
    		&= \sum_{k=1}^\infty \binom{2k}{k}p^k (1-p)^k \\
    		&= \sum_{k=1}^\infty \frac{2k!}{(k!)^2} p^k (1-p)^k \\
    		&\approx \sum_{k=1}^\infty
    		\frac{(2k/e)^{2k}\sqrt{4 \pi k}}{(k^k e^{-k} \sqrt{2 \pi k})^2}
    		p^k (1-p)^k \\
    		&= \sum_{k=1}^\infty
    		\frac{2^{2k} k^{2k} e^{-2k} 2 \sqrt{\pi k}}
    		{k^{2k} e^{-2k} 2 \pi k}
    		p^k (1-p)^k \\
    		&= \sum_{k=1}^\infty
    		\frac{2^{2k}}
    		{\sqrt{\pi k}}
    		p^k (1-p)^k \\
    		&= \sum_{k=1}^\infty
    		\frac{4^k}
    		{\sqrt{\pi k}}
    		p^k (1-p)^k \\
    		&= \sum_{k=1}^\infty
    		\frac{1}
    		{\sqrt{\pi k}}
    		[4p(1-p)]^k
    	\end{align}
    	When $p = \frac{1}{2}$, 
    	\begin{align}
    		\sum_{n=1}^\infty p_{00}\upn &= \frac{1}{\sqrt{\pi}} \sum_{k=1}^\infty k^{-1/2} \\
    		&= \infty
    	\end{align}
    	When $p \neq \frac{1}{2}$,
    	\begin{align}
    		\sum_{n=1}^\infty p_{00}\upn &< \frac{1}{\sqrt{\pi}} \sum_{k=1}^\infty [4 \pi (1 - p)]^k \\
    		&< \infty
    	\end{align}
    	By the recurrent state theorem, $f_{ii} = 1 \iff p = 1/2$. \\
    	For other $i \neq 0$, the prove is similar.
    \end{proof}
    
    \begin{theorem}[f-Expansion]
    	\begin{align}
    		f_{ij} = p_{ij} + \sum_{k \in S \backslash \{j\}} p_{ik} f_{kj}
    	\end{align}
    \end{theorem}
    
    \begin{proof}
    	\begin{align}
	    	f_{ij} &= P(\exists n \in \Z_{++} \ s.t.\ X_n = j|X_0 = i) \\
	    	&= \sum_{k \in S} P(\exists n \in \Z_{++} \ s.t.\ X_n = j|X_0 = i, X_1 = k) P(X_1=k|X_0=i) \\
	    	&= \sum_{k \in S} P(\exists n \in \Z_{++} \ s.t.\ X_n = j| X_1 = k) P(X_1=k|X_0=i) \tx{ (Markov Property)} \\
	    	&= \underbrace{P(\exists n \in \Z_{++} \ s.t.\ X_n = j| X_1 = j)}_{=1} P(X_1=j|X_0=i)
	    	+ \sum_{k \neq j} f_{kj} P(X_1=k|X_0=i) \\
	    	&= p_{ij} + \sum_{k \neq j} f_{kj} p_{ik}
    	\end{align}
    \end{proof}

	\subsection{Communicating States}

    \begin{definition}
    	State $i$ is said to \textbf{communicate} with state $j$, denoted as $i \to j$, if $f_{ij} > 0$. That is, it is possible to get from state $i$ to state $j$ given arbitrarily long period of time.
    \end{definition}
    
    \begin{proposition}[Equivalent Defintion]
    	The following statements are equivalent:
    	\begin{enumerate}[(i)]
    		\item $i \to j$;
    		\item $\exists m \geq 1,\ s.t.\ p_{ij}^{(m)} > 0$.
    	\end{enumerate}
    \end{proposition}
    
    \begin{proof}
    	(Proving the negation) If $p^{(m)}_{ij} = 0$ for every $m \geq 1$, then it's impossible to get state $j$ from state $i$, that's, $f_{ij} = 0$.
    \end{proof}
    
    \begin{definition}
    	A Markov chain s \textbf{irreducible} if $i \to j\ \forall i,j \in S$.
    \end{definition}
    
    
    \subsection{Recurrence and Transience Equivalence Theorem}
    \begin{lemma}[Sum Lemma]
    	If
    	\begin{enumerate}[(i)]
    		\item $i \to k$;
    		\item $\ell \to j$;
    		\item $\sum_{n=1}^\infty p_{k \ell}\upn = \infty$.
    	\end{enumerate}
    	Then, $\sum_{n=1}^\infty p_{ij}\upn = \infty$.
    \end{lemma}
    
    \begin{proof}
    	Suppose $i \to k$ and $\ell \to j$, then there exists $m$ and $r$ such that $p_{ik}^{(m)} > 0$ and $p_{\ell j}^{(r)} > 0$. \\
    	By the Chapman-Kolmogorov inequality, $p_{ij}^{(m+n+r)} \geq p_{ik}^{(m)} p_{k \ell}^{(n)} p_{\ell j}^{(r)}$. \\
    	Then,
    	\begin{align}
    		\sum_{n=1}^\infty p_{ij}\upn
    		&\geq \sum_{n=m+r+1}^\infty p_{ij}\upn \\
    		&= \sum_{s=1}^\infty p_{ij}^{(m+s+r)} \\
    		&\geq \sum_{s=1}^\infty p_{ik}^{(m)} p_{k \ell}^{(s)} p_{\ell j}^{(r)} \\
    		&= p_{ik}^{(m)} p_{\ell j}^{(r)} \sum_{s=1}^\infty p_{k \ell}^{(s)} = \infty
    	\end{align}
    \end{proof}

	\begin{remark}
		Note that sum lemma is still applicable when $k = \ell$ or $i = j$.		
	\end{remark}

    \begin{corollary}[Sum Corollary]
    	If $i \leftrightarrow k$, then
    	\begin{align}
    		f_{ii} = 1 \iff f_{kk} = 1
    	\end{align}
    \end{corollary}
    
    \begin{proof}
    	Provided $i \leftrightarrow k$, there exists $m, r \in \N$ such that 
    	\begin{align}
    		p_{ik}^{(m)} > 0 \\
    		p_{kj}^{(r)} > 0
    	\end{align}
    	Suppose $f_{ii} = 1$, 
    	\begin{align}
    		\sum_{i=1}^\infty p_{kk}\upn 
    		&\geq \sum_{i=1}^\infty p_{ik}^{(m)} p_{ii}^{(s)} p_{kj}^{(r)} \\
    		&\geq \sum_{s=1}^\infty p_{ik}^{(m)} p_{ii}^{(s)} p_{kj}^{(r)} \\
    		&= p_{ik}^{(m)} p_{kj}^{(r)} \sum_{s=1}^\infty p_{ii}^{(s)} \\
    		&= \infty \\
    		\iff f_{kk} &= 1
    	\end{align}
    \end{proof}
    
    \begin{theorem}[Case Theorem]
    	For an \ul{irreducible} Markov chain, it is either
    	\begin{enumerate}[(a)]
    		\item a \textbf{recurrent} Markov chain: $\forall i \in S,\ f_{ii} = 1$ and  $\sum_{n=1}^\infty p_{ij}\upn = \infty\ \forall i, j \in S$;
    		\item or a \textbf{transient} Markov chain: $\forall i \in S,\ f_{ii} < 1$ and $\sum_{n=1}^\infty p_{ij}\upn < \infty\ \forall i, j \in S$.
    	\end{enumerate}
    \end{theorem}

    \begin{proof}
    	Let $\mc{M}$ be an irreducible Markov chain, if there exists $i \neq j \in S$ such that $f_{ii} = 1$ but $f_{jj} < 1$, this leads to a contradiction to the sum corollary because irreducibility of $\mc{M}$ implies $i \leftrightarrow j$. Also, if there exists. some $i, j \in S$ such that $\sum_{n=1}^\infty p_{ij}\upn = \infty$. Then for every other $k, \ell \in S$, $k \leftrightarrow i$ and $j \leftrightarrow \ell$ by the irreducibility of $\mc{M}$. Then $\sum_{n=1}^\infty p_{k \ell}\upn = \infty$ by sum lemma.
    \end{proof}
    
    \begin{theorem}[Finite Space Theorem]
    	An \ul{irreducible} Markov chain on a \ul{finite} state space is always recurrent.
    \end{theorem}
    
    \begin{proof}
    	Let $i \in S$ (u.i.),
    	\begin{align}
    		\sum_{j \in S} \sum_{n=1}^\infty p_{ij}\upn
    		&= \sum_{n=1}^\infty \sum_{j \in S} p_{ij}\upn \\
    		&= \sum_{n=1}^\infty 1 = \infty
    	\end{align}
    	Because $S$ is finite, $\exists k \in S$ such that $\sum_{n=1}^\infty p_{ik}\upn = \infty$. Therefore, all states are recurrent.
    \end{proof}
    
    \begin{theorem}[Hit-Lemma]
    	Define $H_{ij}$ as the event in which the chain starts from $j$ and visits $i$ without firstly returning to $j$ (\emph{direct path from $j$ to $i$}) \footnote{Notation abuse: $H_{ij}$ describes the event starting from $j$ and ending at $i$, instead of the other way round.}:
    	\begin{align}
    		H_{ij} := \{\exists n \in \N\ s.t.\ X_n = i \land X_m \neq j\ \forall m < n\}
    	\end{align}
    	If $j \to i$ with $j \neq i$, then $P(H_{ij}|X_0=j) > 0$.
    \end{theorem}
    
    \begin{theorem}[$f$-Lemma]
    	\red{For all $i, j \in S$, if $j \to i$ and $f_{jj} = 1$, then $f_{ij} = 1$.}
    \end{theorem}
    
    \begin{proof}
    	For $i = j$, trivial. \\
    	Suppose $i \neq j$, since $j \to i$, then $P(H_{ij}|X_0=j)> 0$. \\
    	Further,
    	\begin{align}
    		P(X_n \neq j\ \forall n \in \Z_{++} | X_0=j)
    		&\geq P(H_{ij}|X_0=j) P(X_n \neq j\ \forall n \in \Z_{++} | X_0=i)\\
    		\implies 0 = 1 - f_{jj} &\geq P(H_{ij}|X_0=j) (1 - f_{ij}) \\
    		\implies f_{ij} &= 1
    	\end{align}
    \end{proof}
    
    \begin{lemma}[Infinite Returns Lemma]
    	For an \ul{irreducible} Markov chain,
    	\begin{enumerate}[(i)]
    		\item if this chain is recurrent, then $P(N(j)=\infty|X_0=i)=1\ \forall i, j \in S$;
    		\item if this chain is transient, then $P(N(j)=\infty|X_0=i)=0\ \forall i, j \in S$.
    	\end{enumerate}
    \end{lemma}
    
    \begin{proof}
    	Let $i, j \in S$. \\
    	Suppose the chain is irreducible and recurrent, if $i = j$, then $f_{ii} = f_{jj} = 1$. \\
    	Otherwise, $i \neq j$. Since $j \to i$, by the f-Lemma, $f_{jj} = f_{ii} = f_{ij} = f_{ji} = 1$.
    	\begin{align}
    		P(N(j) = \infty | X_0 = i)
    		&= \lim_{k \to \infty} P(N(j) \geq k | X_0 = i) \\
    		&= \lim_{k \to \infty} f_{ij} f_{jj}^{k-1} \\
    		&= 1
    	\end{align}
    	When the chain is transient, $f_{jj} < 1$, and $\lim_{k \to \infty} f_{ij} f_{jj}^{k-1} = 0$.
    \end{proof}
    
    \begin{theorem}[Recurrent Equivalences Theorem]
    	For a \ul{irreducible} Markov chain (so that $i \to j$ for all $i, j \in S$), the following statements are equivalent:
    	\begin{enumerate}[(1)]
    		\item $\exists k, \ell \in S$ such that $\sum_{n=1}^\infty p_{k \ell}\upn = \infty$;
    		\item $\forall i,j \in S,\ \sum_{n=1}^\infty p_{ij}\upn = \infty$;
    		\item $\exists k \in S\ s.t.\ f_{\red{kk}} = 1$ (need two nodes to be the same to form a strong condition);
    		\item $\forall j \in S,\ f_{jj} = 1$;
    		\item $\forall i,j \in S,\ f_{ij} = 1$;
    		\item $\exists k, \ell \in S$ such that $P_k(N(\ell) = \infty) = 1$;
    		\item $\forall i, j \in S,\ P_i(N(j) = \infty) = 1$.
    	\end{enumerate}
    \end{theorem}

	\begin{proof}
		(1)$\implies$(2) by sum lemma; \\
		(2)$\implies$(3) take the special case when $i = j$, use recurrent state theorem; \\
		(3)$\implies$(4) by sum corollary; \\
		(4)$\implies$(5) by $f$-lemma; \\
		(5)$\implies$(6) by infinite returns lemma; \\
		(6)$\implies$(7)  \\
		(7)$\implies$(1)
	\end{proof}

    \begin{theorem}[Transience Equivalences Theorem]
    	For a \ul{irreducible} Markov chain (so that $i \to j$ for all $i, j \in S$), the following statements are equivalent:
    	\begin{enumerate}[(1)]
    		\item $\forall k, \ell \in S \sum_{n=1}^\infty p_{k \ell}\upn < \infty$;
    		\item $\exists i,j \in S,\ s.t.\ \sum_{n=1}^\infty p_{ij}\upn < \infty$;
    		\item $\forall k \in S\ f_{kk} < 1$;
    		\item $\exists j \in S,\ s.t.\ f_{jj} < 1$;
    		\item $\exists i,j \in S,\ s.t.\ f_{ij} < 1$;
    		\item $\forall k, \ell \in S,\ P_k(N(\ell) = \infty) = 0$;
    		\item $\exists i, j \in S,\ s.t.\ P_i(N(j) = \infty) = 0$.
    	\end{enumerate}
    \end{theorem}


    \subsection{Closed Subset of a Markov Chain}
    
    \begin{definition}
    	For a Markov chain with state space $S$, then any $C \subseteq S$ satisfies
    	\begin{align}
    		p_{ij} = 0\quad \forall i \in C,\ j \notin C
    	\end{align}
    	is a \textbf{closed subset} of the original Markov chain. That is, the chain will stay in the closed subset once enters it.
    \end{definition}

	\begin{remark}
		All theorems hold on the closed subset as well.
	\end{remark}
    
    \begin{proposition}
    	For a simple random walk, if $p \geq \frac{1}{2}$, then $f_{ij} = 1$ for every $j > i$.
    \end{proposition}
    
    \section{Markov Chain Convergence}
    \subsection{Stationary Distributions}
    \begin{definition}
    	Let $\pi \in \Delta(S)$, $\pi$ is \textbf{stationary} for a Markov chain if
    	\begin{align}
    		\pi_j = \sum_{i \in S} \pi_i p_{ij}\quad \forall j \in S
    	\end{align}
    	In matrix notation
    	\begin{align}
    		\pi = \pi P
    	\end{align}
    \end{definition}
    
    \begin{proposition}
    	Let $\pi$ be a stationary distribution of $\mc{M}$, then 
    	\begin{align}
    		\pi_j = \sum_{i \in S} \pi_i p_{ij}^{\red{(n)}}
    	\end{align}
    	In matrix notation,
    	\begin{align}
    		\pi = \pi P^n
    	\end{align}
    \end{proposition}
    
    \begin{proof}
    	Using the matrix notation, it can be shown that $\pi = \pi P^n$ for every $n \in \N$. Therefore,
    	\begin{align}
    		\pi_j &= \sum_{i \in S} \pi_i [P^n]_{ij} \\
    		&= \pi_j = \sum_{i \in S} \pi_i p_{ij}\upn \tx{ since } P\upn = P^n
    	\end{align}
    \end{proof}
    
    \begin{definition}
    	A chain is \textbf{doubly stochastic} if
    	\begin{align}
    		\forall j \in S\ \sum_{i \in S} p_{ij} = 1
    	\end{align}
    	That is, for every state $j$, the arrival probability is one.
    \end{definition}

	\begin{proposition}
		Uniform distribution is stationary for all finite state doubly stochastic Markov chains.
	\end{proposition}

	\begin{proof}
		Let $\pi_i = \frac{1}{|S|}$ for all $i \in S$, then
		\begin{align}
			\sum_{i \in S} \pi_i p_{ij} &= \sum_{i \in S} \frac{1}{|S|} p_{ij} \\
			&= \frac{1}{|S|} \sum_{i \in S} p_{ij} \\
			&= \frac{1}{|S|} \tx{ (doubly stochastic)} \\
			&= \pi_j
		\end{align}
	\end{proof}
    
    \subsection{Searching for Stationarity}
    \begin{definition}
    	A Markov chain is \textbf{reversible} with respect to a distribution $\pi$ if 
	    \begin{align}
	    	\pi_i p_{ij} = \pi_j p_{ji}\quad \forall i, j \in S
	    \end{align}
	\end{definition}

	\begin{theorem}
		If a chain is reversible with respect to $\pi$, then $\pi$ is a stationary distribution.
	\end{theorem}
	
	\begin{proof}
		\begin{align}
			\sum_{i \in S} \pi_i p_{ij} &= \sum_{i \in S} \pi_j p_{ji} \\
			&= \pi_j \sum_{i \in S} p_{ji} \tx{ (reverse the chain)}\\
			&= \pi_j
		\end{align}
	\end{proof}
	
	\begin{proposition}[Vanishing Probability Proposition]
		For a Markov chain $\mc{M}$, if
		\begin{align}
			\forall i, j \in S,\ \lim_{n\to\infty} p_{ij}\upn = 0
		\end{align}
		that is, the chain moves chaotically, 
		then $\mc{M}$ cannot have a stationary distribution.
	\end{proposition}
	
	\begin{proof}
		Suppose, for contradiction, there is a stationary distribution $\pi$. Then,
		\begin{align}
			\pi_j &= \lim_{n \to \infty} \pi_j \\
			&= \lim_{n \to \infty} \sum_{i \in S} \pi_i p_{ij}\upn \\
			&= \sum_{i \in S} \lim_{n \to \infty} \pi_i p_{ij}\upn \\
			&= \sum_{i \in S} \pi_i \lim_{n \to \infty} p_{ij}\upn \\
			&= 0 \neq 1
		\end{align}
		$\rightarrow\leftarrow$
	\end{proof}
	
	\begin{lemma}[Vanishing Lemma]
		If $\mc{M}$ has some $k, \ell$ such that $\lim_{n \to \infty} p_{k\ell}\upn = 0$, then for all $i, j \in S$
		such that $k \to i$ and $j \to \ell$,
		$\lim_{n \to \infty} p_{ij}\upn = 0$.
	\end{lemma}
	
	\begin{proof}
		Because $k \to i$ and $j \to \ell$, there exists $r, s \in \N$ such that
		\begin{align}
			p_{ki}^{(r)} > 0,\ 
			p_{j\ell}^{(s)} > 0
		\end{align}
		Note that for arbitrary $n \in \N$,
		\begin{align}
			p_{k \ell}^{(r+n+s)}
			&\geq p_{ki}^{(r)} p_{ij}^{(n)} p_{j\ell}^{(s)} \\
			\implies p_{ij}^{(n)}
			&\leq \frac{p_{k \ell}^{(r+n+s)}}{p_{ki}^{(r)}p_{j\ell}^{(s)}}
		\end{align}
		Therefore, 
		\begin{align}
			0 \geq \lim_{n \to \infty} p_{ij}\upn &\leq \lim_{n \to \infty} \frac{p_{k \ell}^{(r+n+s)}}{p_{ki}^{(r)}p_{j\ell}^{(s)}} \\
			&= \frac{1}{{p_{ki}^{(r)}p_{j\ell}^{(s)}}} \lim_{n \to \infty} {p_{k \ell}^{(r+n+s)}} \\
			&= \frac{1}{{p_{ki}^{(r)}p_{j\ell}^{(s)}}} 0 = 0 
		\end{align}
		Therefore,
		\begin{align}
			\lim_{n \to \infty} p_{ij}\upn &= 0
		\end{align}
	\end{proof}
	
	\begin{corollary}[Vanishing Together Corollary]
		For an \ul{irreducible} Markov chain, either
		\begin{enumerate}[(i)]
			\item $\lim _{n \rightarrow \infty} p_{i j}^{(n)}=0$ for all $i, j \in S$;
			\item $\lim _{n \rightarrow \infty} p_{i j}^{(n)}\neq 0$ for all $i, j \in S$.
		\end{enumerate}
	\end{corollary}
	
	\begin{proof}
		Immediate result from vanishing lemma.
	\end{proof}
	
	\begin{corollary}[Vanishing Probabilities Corollary]
		If \ul{there exists} $i, j \in S$, $\lim_{n \to \infty} p_{ij}\upn = 0$, then $\mc{M}$ cannot have a stationary distribution.
	\end{corollary}
	
	\begin{proof}
		Omitted.
	\end{proof}
	
	\begin{corollary}[Transient Not Stationary Corollary]
		A Markov chain which is \ul{irreducible} and \ul{transient} cannot have a stationary distribution.
	\end{corollary}
	
	\begin{proof}
		\begin{align}
			\forall i, j \in S\ \sum_{n=1}^\infty f_{ij}\upn < \infty \implies \lim_{n\to\infty}p_{ij}\upn = 0
		\end{align}
	\end{proof}
	
	\begin{definition}
		The \textbf{period} of a state $i$ is the greatest common divisor of the set
		\begin{align}
			\Phi_i = \{n \geq 1: p_{ii}\upn > 0\}
		\end{align}
		Note that if $f_{ii} = 0$, then $\Phi = \varnothing$, and period is not well-defined.
	\end{definition}
	
	\begin{definition}
		If all states in $\mc{M}$ has period of 1, then $\mc{M}$ is said to be \textbf{aperiodic}.
	\end{definition}
	
	\begin{lemma}[Equal Period Lemma]
		If $i \leftrightarrow j$, then the periods of $i$ and $j$ are equal.
	\end{lemma}
	
	\begin{proof}
		Let $t_i$ and $t_j$ be the periods of $i$ and $j$. \\
		Because $i \leftrightarrow j$, there exists $r, s \in \N$ such that $p_{ij}^{(r)}, p_{ji}^{(s)} > 0$. \\
		For any $n \in \N$ such that $p_{jj}\upn > 0$ (i.e., $n \in \Phi_j$), it must be the case that
		\begin{align}
			p^{(r+n+s)}_{ii} &\geq p_{ij}^{(r)} p_{jj}\upn p_{ji}^{(s)} > 0 \\
			p^{(r+s)}_{ii} &\geq p_{ij}^{(r)} p_{ji}^{(s)} > 0
		\end{align}
		Therefore, $r+n+s$ and $r + s \in \Phi_i$, and $t_i | r + n + s$ and $t_i | r + s$. \\
		Hence $t_i | n$. \\
		Because $n$ is chosen to be an arbitrary element in $\Phi_j$, therefore, $t_i \leq t_j$. \\
		Proving $t_i \geq t_j$ is similar. 
	\end{proof}
	
	\begin{corollary}
		If $\mc{M}$ is \ul{irreducible}, then all states have the same period.
	\end{corollary}
	\begin{proof}
		Follows the equal period lemma directly.
	\end{proof}
	
	\begin{corollary}
		If $\mc{M}$ is \ul{irreducible}, and $p_{ii} > 0$ for some $i \in S$ (so that state $i$ has period 1), then the whole chain $\mc{M}$ is aperiodic.
	\end{corollary}

	\begin{proof}
		Follows the equal period corollary directly.
	\end{proof}
	
	\subsection{Convergence Theorem}
	
	\begin{theorem}[Markov Chain Convergence Theorem]
		If a Markov chain $\mc{M}$ is 
		\begin{enumerate}[(i)]
			\item irreducible;
			\item aperiodic;
			\item with a stationary distribution $\pi$
		\end{enumerate}
		(i. conditioned on initial state) then
		\begin{align}
			\lim_{n \to \infty} p_{ij}\upn = \pi_j\quad\forall i, j \in S
		\end{align}
		In fact, the limiting probability does not depend on initial state $i$. \\
		(ii. unconditional) and for any initial probability $v$,
		\begin{align}
			\lim_{n \to \infty} P(X_n = j) 
			= \lim_{n \to \infty} \mu_j\upn
			= \pi_j
		\end{align}
	\end{theorem}
	
	\begin{theorem}[Stationary Recurrence Theorem]
		For an \ul{irreducible} chain $\mc{M}$ with a stationary distribution, $\mc{M}$ is always recurrent.
	\end{theorem}
	
	\begin{proof}
		Suppose not, this contradicts the previous result \emph{irreducible transient Markov chain cannot have stationary distribution}.
	\end{proof}
	
	\begin{proposition}
		If a state $i$ has $f_{ii} > 0$ and is \ul{aperiodic}, then there is $n_0(i) \in \N$ such that
		\begin{align}
			p_{ii}\upn > 0\quad \forall n \geq n_0(i)
		\end{align}
	\end{proposition}
	
	\begin{proof}
		Because $f_{ii} > 0$, $\Phi_i := \{n \geq 1: p_{ii}\upn > 0\} \neq \varnothing$. \\
		Let $m, n \in \Phi_i$, then $p_{ii}^{(m+n)} \geq p_{ii}^{(m)} p_{jj}^{(n)} > 0$, so that $m + n \in \Phi_i$. \\
		Therefore, $\Phi_i$ satisfies additivity property. \\
		Also, $\gcd(\Phi_i) = 1$. \\
		\ul{Lemma} show that $n \in \Phi_i$ implies $n' \in \Phi_i\ \forall n' \geq n$. \\
		Let $n(i) \in \Phi_i$, then for all $n' \geq n(i)$, $n' \in \Phi_i$.
	\end{proof}

	\begin{corollary}
		If a chain is \ul{irreducible} and \ul{aperiodic}, then for any states $i, j \in S$, there is $n_0(i, j) \in \N$ such that
		\begin{align}
			p_{ij}\upn > 0\quad \forall n \geq n_0(i, j)
		\end{align}
	\end{corollary}

	\begin{proof}
		Let $n_0(i) \in \N$ such that for all $n' \geq n_0(i)$, $n' \in \Phi_i$. \\
		Provided $i \rightarrow j$, there exists $m \in \N$ such that $p_{ij}^{(m)} >0$. \\
		Let $n_0(i, j) = n_0(i) + m$. \\
		For every $n \geq n_0(i, j)$, $n$ can be written as $n = n' + m$ for some $n' \geq n_0(i)$, 
		\begin{align}
			n' \geq n_0(i) \implies p_{ii}^{(n')} > 0
		\end{align}
		Then
		\begin{align}
			p_{ij}\upn &= p_{ij}^{(n' + m)} \\
			&\geq p_{ii}^{(n')} p_{ij}^{(m)} > 0
		\end{align}
	\end{proof}
	
	\begin{lemma}[Markov Forgetting Lemma]
		If a Markov chain $\mc{M}$ is
		\begin{enumerate}[(i)]
			\item irreducible;
			\item aperiodic;
			\item with a stationary distribution $\pi$
		\end{enumerate}
		then for all $i, j, k \in S$, then
		\begin{align}
			\lim_{n \to \infty} \abs{
				p_{ik}\upn - p_{jk}\upn
			} = 0
		\end{align}
	\end{lemma}
	
	\begin{proof}
		Omiited
	\end{proof}
	
	\begin{corollary}
		If $\mc{M}$ is \ul{irreducible} and \ul{aperiodic} then it has at most one stationary distribution.
	\end{corollary}

	\begin{proof}
		Suppose $\mc{M}$ has a stationary distribution, then by the Markov chain convergence theorem, $\pi_j$ is the limit of 
		\begin{align}
			\lim_{n \to \infty} P(X_n = j)
		\end{align}
		and such limit must be unique if it exists.
	\end{proof}

	\begin{corollary}[Generalized Version]
		If $\mc{M}$ is \ul{irreducible} then it has at most one stationary distribution.
	\end{corollary}
	\subsection{Periodic Convergence}
	\begin{theorem}[Periodic Convergence Theorem]
		Suppose a Markov chain is \ul{irreducible}, with period $b \geq 2$, and has stationary distribution $\pi$, then for all $i, j \in S$,
		\begin{align}
			\lim _{n \rightarrow \infty} \frac{1}{b}\left[p_{i j}^{(n)}+\ldots+p_{i j}^{(n+b-1)}\right]=\pi_{j}
		\end{align}
		and
		\begin{align}
			\lim _{n \rightarrow \infty} \frac{1}{b}\left(\prob{X_{n}=j}+\prob{X_{n+1}=j}+\ldots+\prob{X_{n+b-1}=j}\right)=\pi_{j}
		\end{align}
		Moreover,
		\begin{align}
			\lim_{n \to \infty} \prob{X_{n}=j \tx{ or } X_{n+1}=j \tx{ or } \cdots \tx{ or }X_{n+b-1}=j}
		\end{align}
	\end{theorem}
	
	\begin{proof}
		For the last equality, note that since the period is $b$, it must take at least $b$ steps for the chain to return to $j$ from $j$. Therefore, all events in the last equality are disjoint.
	\end{proof}

	\begin{lemma}[Cesaro Sum]
		If $\lim_{n \to \infty} x_n = r$, then $\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n x_i = r$.
	\end{lemma}

	\begin{theorem}[Average Probability Convergence]
		If a Markov chain is \ul{irreducible} with stationary distribution $\pi$, then for all $i, j \in S$,
		\begin{align}
			\lim_{n \to \infty} \frac{1}{n} \sum_{\ell=1}^n p_{ij}^{(\ell)} = \pi_j
		\end{align}
	\end{theorem}
	
	\begin{proof}
		If the chain is aperiodic, apply the Markov chain convergence theorem and Cesaro sum. Otherwise, suppose the chain has period $b \geq 2$, 
		then by periodic convergence theorem,
		\begin{align}
			x_n := \frac{1}{b}\left[p_{i j}^{(n)}+\ldots+p_{i j}^{(n+b-1)}\right] \to \pi_j
		\end{align}
		and Cesaro sum,
		\begin{align}
			\lim_{n\to\infty} \frac{1}{n} \sum_{\ell=1}^n x_\ell = \pi_j
		\end{align}
	\end{proof}
	
	\section{Random Walk on Graphs}
	\begin{definition}
		A \textbf{graph} consists of a non-empty finite or countable set $V$ of \ul{vertices} and a \ul{symmetric weight function} $w: V \times V \to \R_+$.
	\end{definition}
	
	\begin{definition}
		A graph is \textbf{unweighted} if for every $w, v \in V$,
		\begin{enumerate}[(i)]
			\item $d(w, v) = 1$ if and only if there is an edge between $w$ and $v$;
			\item and $d(w, v) = 0$ if and only if there is no edge in between.
		\end{enumerate}
	\end{definition}
	
	\begin{definition}
		For a vertex $u \in V$, the \textbf{degree} of vertex $u$ is defined as
		\begin{align}
			d(u) := \sum_{v \in V} w(u, v)
		\end{align}
	\end{definition}
	
	\begin{definition}
		Given a vertex set $V$ with symmetric weights $w$ the \textbf{(simple) random walk on the (undirected) graph} $(V, w)$ is the Markov chain with state space $S=V$ and transition probabilities $p_{u v}=\frac{w(u, v)}{d(u)}$ for all $u, v \in V$.
	\end{definition}
	
	\begin{definition}
		Consider a random walk on a graph $V$ with degree $d(u)$. Assume 
		\begin{align}
			Z = \sum_{u \in V}d(u) = \sum_{u, v \in V} w(u, v)	
		\end{align}
		is finite, then
		\begin{align}
			\pi_u = \frac{d(u)}{Z}
		\end{align}
		is a stationary distribution for this walk.
	\end{definition}
	\begin{proof}
		We are going to show this random walk is reversible with respect to $\pi$. Let $u, v \in V$,
		\begin{align}
			\pi_u p_{uv} = \frac{d(u)}{Z} \frac{w(u, v)}{d(u)} = \frac{w(u,v)}{Z} \\
			\pi_v p_{vu} = \frac{d(v)}{Z} \frac{w(v, u)}{d(v)} = \frac{w(v,u)}{Z}
		\end{align}
		These two products are in fact equal because weight is symmetric.
	\end{proof}
	
	\begin{proposition}
		A random walk on graph is irreducible if and only if the graph is connected.
	\end{proposition}
	
	\begin{proposition}
		The period of a random walk on graph is either 1 or 2, since $p^{(2)}_{uu} > 0$.
	\end{proposition}
	
	\begin{proposition}
		A random walk on graph is aperiodic if and only if it's non-bipartite.
	\end{proposition}
	
	\begin{proposition}
		Any cycle with odd number of vertices is non-bipartite, therefore, aperiodic.
	\end{proposition}
	
	\begin{theorem}[Graph Convergence Theorem]
		For a random walk on a \ul{connected non-bipartite} graph, if $Z<\infty,$ then 
		\begin{align}
			\lim _{n \rightarrow \infty} p_{u v}^{(n)}=\frac{d(v)}{Z} \equiv \pi_v	
		\end{align}
		for all $u, v \in V,$ and 
		\begin{align}
			\lim _{n \rightarrow \infty} \mathbf{P}\left[X_{n}=v\right]=\frac{d(v)}{Z}
		\end{align}
		for any initial probabilities.
	\end{theorem}
	\begin{proof}
		Since the graph is irreducible and aperiodic, further it possesses a stationary distribution. By Markov chain convergence theorem, it converges to its stationary distribution.
	\end{proof}
	
	\begin{theorem}[Graph Average Convergence]
		For a random walk on any \ul{connected graph} with $Z<\infty$ (whether bipartite or not), for all $u, v \in V$,
		\begin{align}
			\lim _{n \rightarrow \infty} \frac{1}{2}\left[p_{u v}^{(n)}+p_{u v}^{(n+1)}\right]=\frac{d(v)}{Z}	
		\end{align}
		and
		\begin{align}
			\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{\ell=1}^{n} p_{u v}^{(\ell)}=\frac{d(v)}{Z}
		\end{align}
	\end{theorem}
	
	\begin{proof}
		By periodic convergence theorem.
	\end{proof}
	
	\section{Mean Recurrence Times}
	\begin{definition}
		Given a Markov chain with states $S$, the \textbf{mean recurrence time} of a state $i \in S$ is the expected value of the time of returning state $i$ from state $i$. That is,
		\begin{align}
			m_i = \expe_i[\inf\{n \geq 1: X_n = i\}] = \expect{\inf\{n \geq 1: X_n = i | X_0 = i\}}
		\end{align}
		Let $\tau_i := \inf\{n \geq 1: X_n = i\}$.
	\end{definition}
	
	\begin{remark}
		Even if state $i$ is recurrent, it is still possible that $m_i = \infty$.
	\end{remark}
	
	\begin{definition}
		A state is \textbf{positive recurrent} if $m_i < \infty$. It is \textbf{null recurrent} if $m_i = \infty$.
	\end{definition}
	
	\begin{theorem}[Recurrent Time Theorem]
		For an irreducible Markov chain, either
		\begin{enumerate}[(i)]
			\item $m_i < \infty$ for all $i \in S$, and there is a unique stationary distribution $\pi_i = \frac{1}{m_i}$;
			\item or $m_i = \infty$ for all $i \in S$, and there is no stationary distribution.
		\end{enumerate}
	\end{theorem}
	
	\begin{proposition}
		An \ul{irreducible} Markov chain on a \ul{finite} state space $S$ always have $m_i < \infty$ for all $i \in S$, and this chain has stationary distribution $\pi_i = \frac{1}{m_i}$.
	\end{proposition}
	
	\begin{remark}
		The converse to above proposition is false. There are Markov chains with stationary distribution, but has infinite state space.
	\end{remark}
	
	\begin{proposition}
		For a symmetric random walk starting from state $i$,
		\begin{align}
			\infty = m_i = \expe_i[\tau_i] &= 1 + p \expe_{i+1}[\tau_i] + (1-p) \expe_{i-1} [\tau_i] \\
			&= 1 + p \expe_{i}[\tau_{i-1}] + (1-p) \expe_{i} [\tau_{i+1}] \\
			&\implies \expe_{i}[\tau_{i-1}] = \expe_{i} [\tau_{i+1}] = \infty
		\end{align}
		Therefore, on average, it takes infinite steps for the simple symmetric walk to progress one step.
	\end{proposition}

	\section{Sequence Waiting Times}
	\paragraph{Motivation} Converting the waiting time for a particular pattern in stochastic process $\{X_n\}$ to a Markov chain.
	\todo{fill this part}
	
	\section{Martingales}
\end{document}



























